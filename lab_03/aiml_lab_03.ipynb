{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"hsg_logo.png\">\n",
    "\n",
    "###  Lab 03 - \"Supervised Machine Learning\"\n",
    "\n",
    "Introduction to AI and ML, University of St. Gallen, Autumn Term 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lab, you learned about Python programming elements such as conditions, loops as well as how to implement functions etc. In this third lab, we will build our first supervised machine learning \"pipeline\" using:\n",
    "\n",
    "\n",
    ">- (1) the **Gaussian Naive-Bayes (GNB)** classifier, and; \n",
    ">- (2) the **k Nearest-Neighbours (kNN)** classifier \n",
    "\n",
    "you learned about in the lecture.\n",
    "\n",
    "The **Naive-Bayes (NB)** classifier belongs to the family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with a strong (naive) independence assumptions between the features. Naive Bayes has been studied extensively since the 1950s and remains an accessible (baseline) method for text categorization as well as other domains.\n",
    "\n",
    "\n",
    "The **k Nearest-Neighbours (kNN)** is a simple, easy to understand, versatile, but powerful machine learning algorithm. Until recently (prior to the advent of deep learning approaches) it was used in a variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. In Credit ratings, financial institutes will predict the credit rating of customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After today's lab you should be able to:\n",
    "\n",
    "> 1. Understand how a Gaussian **Naive-Bayes (NB)** classifier can be trained and evaluated.\n",
    "> 2. Understand how a **k Nearest-Neighbor (kNN)** classifier can be trained and evaluated.\n",
    "> 3. Know how to Python's sklearn library to **train** and **evaluate** arbitrary classifiers.\n",
    "> 4. Understand how to **evaluate** and **interpret** the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start let's watch a motivational video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1024\"\n",
       "            height=\"576\"\n",
       "            src=\"https://www.youtube.com/embed/x4O8pojMF0w\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10e3c1690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# OpenAI: \"Solving Rubik's Cube with a Robot Hand\"\n",
    "YouTubeVideo('x4O8pojMF0w', width=1024, height=576)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup of the Analysis Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. In this lab will use the pandas, numpy, sklearn, matplotlib and seaborn library. Let's import the libraries by the execution of the statements below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the numpy, scipy and pandas data science library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# import sklearn data and data pre-processing libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import sklearn naive.bayes and k-nearest neighbor classifier library\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# import sklearn classification evaluation library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# import matplotlib data visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable inline Jupyter notebook plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed of all our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Gaussian \"Naive-Bayes\" (NB) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.0: Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Iris Dataset** is a classic and straightforward dataset often used as a \"Hello World\" example in multi-class classification. This data set consists of measurements taken from three different types of iris flowers (referred to as **Classes**),  namely the Iris Setosa, the Iris Versicolour, and, the Iris Virginica) and their respective measured petal and sepal length (referred to as **Features**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px; height: auto\" src=\"iris_dataset.png\">\n",
    "\n",
    "(Source: http://www.lac.inpe.br/~rafael.santos/Docs/R/CAP394/WholeStory-Iris.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, the dataset consists of **150 samples** (50 samples taken per class) as well as their corresponding **4 different measurements** taken for each sample. Please, find below the list of the individual measurements:\n",
    "\n",
    ">- `Sepal length (cm)`\n",
    ">- `Sepal width (cm)`\n",
    ">- `Petal length (cm)`\n",
    ">- `Petal width (cm)`\n",
    "\n",
    "Further details of the dataset can be obtained from the following puplication: *Fisher, R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).\"*\n",
    "\n",
    "Let's load the dataset and conduct a preliminary data assessment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect the names of the four features contained in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and print the feature dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and print the class label dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect the names of the three classes contained in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly envision how the feature information of the dataset is collected and presented in the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px; height: auto\" src=\"feature_collection.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the top five feature rows of the Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.data, columns=iris.feature_names).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also inspect the top five class labels of the Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.target, columns=[\"class\"]).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now conduct a more in depth data assessment. Therefore, we plot the feature distributions of the Iris dataset according to their respective class memberships as well as the features pairwise relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pls. note that we use Python's **Seaborn** library to create such a plot referred to as **Pairplot**. The Seaborn library is a powerful data visualization library based on the Matplotlib. It provides a great interface for drawing informative statstical graphics (https://seaborn.pydata.org). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed from the created Pairplot, that most of the feature measurements that correspond to flower class \"setosa\" exhibit a nice **linear seperability** from the feature measurements of the remaining flower classes. In addition, the flower classes \"versicolor\" and \"virginica\" exhibit a commingled and **non-linear seperability** across all the measured feature distributions of the Iris Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1. Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand and evaluate the performance of any trained supervised machine learning model, it is good practice, to divide the dataset into a **training set** (the fraction of data records solely used for training purposes) and a **evaluation set** (the fraction of data records solely used for evaluation purposes). Pls. note, the **evaluation set** will never be shown to the model as part of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"train_eval_dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the fraction of evaluation records to **30%** of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fraction = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly split the dataset into training set and evaluation set using sklearns `train_test_split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% training and 30% evaluation\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(iris.data, iris.target, test_size=eval_fraction, random_state=random_seed, stratify=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the training set dimensionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the evaluation set dimensionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval.shape, y_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2. Gaussian Naive-Bayes (NB) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular (and remarkably simple) algorithm is the **Naive Bayes Classifier**. Note that one natural way to adress a given classification task is via the probabilistic question: **What is the most likely class $\\hat{c}$ given a set of observations $x$?** Formally, we wish to output a prediction for $c$ by calculating its posterior probabilities $P(c|x)$ given the expression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{c} = \\arg \\max_{c} P(c|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In probability theory, the posterior probability referrs to the probability of observing a particular class $c$ after taking into consideration all the available information $x$. In our case this would unfortunately require that we estimate $P(c | \\mathbf{x})$ for every value of $\\mathbf{x} = x_1, x_2, ..., x_n$. \n",
    "\n",
    "Imagine that each feature could take one of just 2 values. For example, the feature $x_1 = 1$ might signify that the word pumpkin appears in a given document and $x_1 = 0$ would signify that it does not. If we had 30 such binary features, that would mean that we need to be prepared to calculate the probability $P(c | \\mathbf{x})$ of any of $2^{30}$ (over 1 billion) possible values of the input vector $\\mathbf{x}$. Moreover, where is the learning? If we need to see every single possible example in order to predict the corresponding label then we're not really learning a pattern but just memorizing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution to this challenge is the so-called **Bayes' theorem** (alternatively Bayes' law or Bayes' rule) that you learned about in the lecture. It provides an elegant way of calculating class posterior probabilities $P(c|x)$ without the need of seeing every single possible example. Let's briefly revisit the formula of the Bayes' theorem below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"bayes_theorem.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the formula of the **Bayes' theorem** above,\n",
    "\n",
    ">- $P(c|x)$ denotes the **posterior** probability of class $c$ given a set of features $x$ denoted by $x_1, x_2, ..., x_n$.\n",
    ">- $P(c)$ denotes the **prior** probability of observing class $c$.\n",
    ">- $P(x|c)$ denotes the **likelihood** which is the probability of a feature $x$ given class $c$.\n",
    ">- $P(x)$ denotes the **evidence** which is the general probability of observing feature $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the **chain rule** of probability, we can express the likelihood term $P( \\mathbf{x} | c)$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P( \\mathbf{x} | c) = P(x_1 | c) \\cdot P(x_2 | x_1, c) \\cdot ... \\cdot P( x_n | x_1, ..., x_{n-1}, c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By itself, this expression doesn't get us any further. We still need (in a worst case) estimate roughly $2^d$ parameters. The trick of the (naive) Bayes' theorem however is to assume that the distinct features $x_1, x_2, ..., x_n$ are conditionally independent from each other given a particular class $c$. \n",
    "\n",
    "Using this assumption we're in much better shape, as the likelihood term $p( \\mathbf{x} | c)$ term simplifies to: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P( \\mathbf{x} | c) = P(x_1 | c) \\cdot P(x_2 | c) \\cdot ... \\cdot P( x_n | c) = \\prod^n_i P(x_i | c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating each term in $\\prod^n_i p(x_i | c)$ amounts to estimating just one feature distribution. So our assumption of conditional independence has taken the complexity of our model (in terms of the number of parameters) from an exponential dependence on the number of features to a linear dependence. Hence, we call it the **\"naive\"** Bayes' theorem, since it makes the naive assumption about feature independence, so we don't have to care about dependencies among them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2.1. Calculation of the prior probabilities $P(c)$ of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an intuition of the Bayes' theorem by first calculating the prior probability $P(c)$ of each class iris flower contained in the dataset. Therefore, we first obtain the number of occurance of each class in the extracted training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine counts of unique class labels\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# concatenate counts and class labels in a python dictionary\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "# print obtained dictionary\n",
    "print(class_counts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the obtained counts into probabilites. Therefore, we divide the class counts by the overall number of observations contained in the extracted training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide counts by the number of observations available in the training data\n",
    "prior_probabilities = counts / np.sum(counts)\n",
    "\n",
    "# print obtained probabilites\n",
    "print(prior_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2.2. Calculation of the evidence $P(x)$ of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the general probability of observing feature $𝑥$ which denotes the evidence $P(x)$ of a feature.\n",
    "\n",
    "During the lecture you learned that evidence distribution can be approximated by a Gaussian (Normal) probability distribution $\\mathcal{N}(\\mu, \\sigma)$. This simplification is justified by the application of the \"law of large numbers\" or \"Central Limit Theorem\" (you may want to have a look at further details of the theorem under: https://en.wikipedia.org/wiki/Central_limit_theorem). In general, the probability density of a Gaussian \"Normal\" distribution, as defined by the formula below. It is parametrized its mean $\\mu$ and corresponding standard deviation $\\sigma$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 600px; height: auto\" src=\"evidence_calculation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the \"law of large numbers\" we will approximate the probability density $P(x) \\approx \\mathcal{N}(x | \\mu, \\sigma)$ of each of each feature by a Gaussian. But how can this be achieved? \n",
    "\n",
    "Let's start by inspecting the true probability density of the **sepal length** feature (the first feature) of the Iris Dataset. The following line of code determines a histogram of the true **sepal length** feature value distribution and plots it accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine a histogram of the \"sepal length\" feature value distribution\n",
    "hist_probabilities, hist_edges = np.histogram(x_train[:, 0], bins=10, range=(0,10), density=True)\n",
    "\n",
    "# print the histogram feature value probabilites\n",
    "print(hist_probabilities)\n",
    "\n",
    "# print the histogram edges\n",
    "print(hist_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the probability density accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations\n",
    "ax.hist(x_train[:, 0], bins=10, range=(0, 10), density=True, color='g')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x$\", fontsize=10)\n",
    "ax.set_ylabel(\"$P(x)$\", fontsize=10)\n",
    "ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Distribution of the \"Sepal Length\" feature', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we approximate the true probability density of the **sepal length** feature using a Gaussian distribution? Well, all we need to do is to calculate it's mean $\\mu$ and standard deviation $\\sigma$. Let's start by calculating the mean $\\mu$ of the **sepal length** feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean of the sepal length observations\n",
    "mean_sepal_length = np.mean(x_train[:, 0])\n",
    "\n",
    "# print the obtained mean\n",
    "print(mean_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue by calculating the standard devition $\\sigma$ of the **sepal length** feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the standard deviation of the sepal length observations\n",
    "std_sepal_length = np.std(x_train[:, 0])\n",
    "\n",
    "# print the obtained standard deviation\n",
    "print(std_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now determine the approximate Gaussian (Normal) probability density distribution $\\mathcal{N}(\\mu, \\sigma)$ of the **sepal length** feature using the $\\mu$ and $\\sigma$ obtained above. Thereby, we will utilize the `pdf.norm` function available in the `scipy.stats` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability density function of the Gaussian distribution\n",
    "hist_gauss_sepal_length = norm.pdf(np.arange(0, 10, 0.1), mean_sepal_length, std_sepal_length)\n",
    "\n",
    "# print obtained probabilities\n",
    "print(hist_gauss_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the approximate Gaussian (Normal) probability density distribution $P(x) \\approx \\mathcal{N}(\\mu, \\sigma)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), hist_gauss_sepal_length, color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations\n",
    "ax.hist(x_train[:, 0], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x$\", fontsize=10)\n",
    "ax.set_ylabel(\"$P(x)$\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Length\" feature', fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"sepal width\" feature\n",
    "mean_sepal_width = np.mean(x_train[:, 1])\n",
    "std_sepal_width = np.std(x_train[:, 1])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_sepal_width, std_sepal_width), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal width\" observations\n",
    "ax.hist(x_train[:, 1], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x$\", fontsize=10)\n",
    "ax.set_ylabel(\"$P(x)$\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Width\" feature', fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal length\" feature\n",
    "mean_petal_length = np.mean(x_train[:, 2])\n",
    "std_petal_length = np.std(x_train[:, 2])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_length, std_petal_length), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"petal length\" observations\n",
    "ax.hist(x_train[:, 2], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x$\", fontsize=10)\n",
    "ax.set_ylabel(\"$P(x)$\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Length\" feature', fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal width\" feature\n",
    "mean_petal_width = np.mean(x_train[:, 3])\n",
    "std_petal_width = np.std(x_train[:, 3])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_width, std_petal_width), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"petal width\" observations\n",
    "ax.hist(x_train[:, 3], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x$\", fontsize=10)\n",
    "ax.set_ylabel(\"$P(x)$\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Width\" feature', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2.3. Calculation of the likelihood $P(x|c)$ of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how we can calculate the **likelihood** $P(x|c)$ which is the probability density of a feature given a certain class $c$. We will again can estimate $P(x|c)$ by a Gaussian (Normal) probability distribution $\\mathcal{N}(\\mu, \\sigma)$ applying the \"law of large numbers\".\n",
    "\n",
    "The **likelihood** probability density of a Gaussian \"Normal\" distribution, as defined by the formula below, is determined by its mean $\\mu$, standard deviation $\\sigma$ and it's corresponding class condition $c$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 600px; height: auto\" src=\"likelihood_calculation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by applying the class conditioning. This is usually done by filtering the dataset for each class $c$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all iris setosa measurements, class label = 0\n",
    "x_train_setosa = x_train[y_train == 0]\n",
    "\n",
    "# collect all iris versicolor measurements, class label = 1\n",
    "x_train_versicolor = x_train[y_train == 1]\n",
    "\n",
    "# collect all iris virginica measurements, class label = 2\n",
    "x_train_virginica = x_train[y_train == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by inspecting the true probability density of the **sepal length** feature (the first feature) of the iris dataset given the class **setosa**. The following line of code determines a histogram of the true feature value distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine a histogram of the \"sepal length\" feature value distribution given the class \"setosa\"\n",
    "hist_setosa, bin_edges_setosa = np.histogram(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True)\n",
    "\n",
    "# print the histogram feature value probabilites\n",
    "print(hist_setosa)\n",
    "\n",
    "# print the histogram edges\n",
    "print(bin_edges_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the probability density accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x$\", fontsize=10)\n",
    "ax.set_ylabel(\"$P(x)$\", fontsize=10)\n",
    "ax.set_ylim([0.0, 1.5])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Distribution of the \"Sepal Length\" feature given class \"Setosa\"', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are again able to determine the approximate Gaussian (Normal) probability density distribution $\\mathcal{N}(\\mu, \\sigma, c)$ of the **sepal length** feature given the class **setosa** using the $\\mu$ and $\\sigma$ obtained above as well as the `pdf.norm` function of the `scipy.stats` package.\n",
    "\n",
    "Let's continue by calculating the mean $\\mu$ of the **sepal length** feature given the class **setosa**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean of the sepal length observations given class \"setosa\"\n",
    "mean_sepal_length_setosa = np.mean(x_train_setosa[:, 0])\n",
    "\n",
    "# print the obtained mean\n",
    "print(mean_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue by calculating the standard devition $\\sigma$ of the **sepal length** feature given the class **setosa**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the standard deviation of the sepal length observations given class \"setosa\"\n",
    "std_sepal_length_setosa = np.std(x_train_setosa[:, 0])\n",
    "\n",
    "# print the obtained standard deviation\n",
    "print(std_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability density function of the Gaussian distribution\n",
    "hist_gauss_sepal_length_setosa = norm.pdf(np.arange(0, 10, 0.1), mean_sepal_length_setosa, std_sepal_length_setosa)\n",
    "\n",
    "# print obtained probabilities\n",
    "print(hist_gauss_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the approximate Gaussian (Normal) probability density distribution $P(x | c) \\approx \\mathcal{N}(\\mu, \\sigma, c)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), hist_gauss_sepal_length_setosa, color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x$\", fontsize=10)\n",
    "ax.set_ylabel(\"$P(x)$\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of \"Sepal Length\" feature given class \"Setosa\"', fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"sepal width\" feature given class setosa\n",
    "mean_sepal_width_setosa = np.mean(x_train_setosa[:, 1])\n",
    "std_sepal_width_setosa = np.std(x_train_setosa[:, 1])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_sepal_width_setosa, std_sepal_width_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 1], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x$\", fontsize=10)\n",
    "ax.set_ylabel(\"$P(x)$\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of \"Sepal Width\" feature given class \"Setosa\"', fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal length\" feature given class setosa\n",
    "mean_petal_length_setosa = np.mean(x_train_setosa[:, 2])\n",
    "std_petal_length_setosa = np.std(x_train_setosa[:, 2])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_length_setosa, std_petal_length_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 2], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x$\", fontsize=10)\n",
    "ax.set_ylabel(\"$P(x)$\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of \"Sepal Width\" feature given class \"Setosa\"', fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal width\" feature given class setosa\n",
    "mean_petal_width_setosa = np.mean(x_train_setosa[:, 3])\n",
    "std_petal_width_setosa = np.std(x_train_setosa[:, 3])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_width_setosa, std_petal_width_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 3], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x$\", fontsize=10)\n",
    "ax.set_ylabel(\"$P(x)$\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of \"Sepal Width\" feature given class \"Setosa\"', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have determined all the necessary probabilities and probability distributions $P(c)$, $P(x)$ and $P(x|c)$ to determine if a so far unseen **iris flower** observation corresponds to class **setosa**. Let's calculate the probability of two random observations of beeing of class **setosa**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init a random feature observation \n",
    "sepal_length = 5.8 \n",
    "sepal_width  = 3.5\n",
    "petal_length = 1.5\n",
    "petal_width  = 0.25\n",
    "\n",
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability\n",
    "prior = 0.3333 \n",
    "\n",
    "# determine the likelihood probability\n",
    "likelihood = norm.pdf(sepal_length, mean_sepal_length_setosa, std_sepal_length_setosa) * norm.pdf(sepal_width, mean_sepal_width_setosa, std_sepal_width_setosa) * norm.pdf(petal_length, mean_petal_length_setosa, std_petal_length_setosa) * norm.pdf(petal_width, mean_petal_width_setosa, std_petal_width_setosa)\n",
    "\n",
    "# determine the evidence probability\n",
    "evidence = norm.pdf(sepal_length, mean_sepal_length, std_sepal_length) * norm.pdf(sepal_width, mean_sepal_width, std_sepal_width) * norm.pdf(petal_length, mean_petal_length, std_petal_length) * norm.pdf(petal_width, mean_petal_width, std_petal_width)\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior = (prior * likelihood) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it seems to be likely that this Iris flower is of class **setosa**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to a another **iris flower** observation and determine its probability of beeing of class **setosa**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init a second random feature observation \n",
    "sepal_length = 7.8\n",
    "sepal_width  = 2.3\n",
    "petal_length = 6.4\n",
    "petal_width  = 2.5\n",
    "\n",
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability\n",
    "prior = 0.3333 \n",
    "\n",
    "# determine the likelihood probability\n",
    "likelihood = norm.pdf(sepal_length, mean_sepal_length_setosa, std_sepal_length_setosa) * norm.pdf(sepal_width, mean_sepal_width_setosa, std_sepal_width_setosa) * norm.pdf(petal_length, mean_petal_length_setosa, std_petal_length_setosa) * norm.pdf(petal_width, mean_petal_width_setosa, std_petal_width_setosa)\n",
    "\n",
    "# determine the evidence probability\n",
    "evidence = norm.pdf(sepal_length, mean_sepal_length, std_sepal_length) * norm.pdf(sepal_width, mean_sepal_width, std_sepal_width) * norm.pdf(petal_length, mean_petal_length, std_petal_length) * norm.pdf(petal_width, mean_petal_width, std_petal_width)\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior = (prior * likelihood) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, this observations seems to be very improbable of class **setosa**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2.4. Training and utilization of a Gaussian Naive-Bayes Classifier using Python's Sklearn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, there is a Python library named `sklearn` (https://scikit-learn.org) that provides a variety of machine learning algorithms that can be easily interfaced using the Python programming language. It also contains supervised classification algorithms such as the **Gaussian Naive-Bayes** classifier which we can use of the shelf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `sklearn` and instantiate the **Gaussian Naive-Bayes** classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB(priors=None, var_smoothing=1e-09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train or fit the Gaussian Naive-Bayes classifier using the training dataset features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the Gaussian Naive Bayes classifier\n",
    "gnb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the trained model to predict the classes of the distinct observations contained in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(x_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the class labels **predicted** by the Gaussian Naive-Bayes classifier on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the **true** class labels as contained in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the **prediction accuracy** of the trained model on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine number of **missclassified** data sampels in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of mislabeled points out of a total {} points: {}\".format(x_eval.shape[0], np.sum(y_eval != y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the field of machine learning and in particular the field of statistical classification, a **confusion matrix**, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the number of instances that the classifier predicted per class, while each column represents the instances of the true or actual class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 300px; height: auto\" src=\"confusion_matrix.png\">\n",
    "\n",
    "(Source: https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the prediction confusion matrix\n",
    "mat = confusion_matrix(y_eval, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# plot confusion matrix heatmap\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='BuGn_r', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "\n",
    "# add plot axis labels\n",
    "plt.xlabel('[true label]')\n",
    "plt.ylabel('[predicted label]')\n",
    "\n",
    "# add plot title\n",
    "plt.title('Gaussian NB Confusion Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following exercises as part of the lab:\n",
    "\n",
    "**1. Train and evaluate the prediction accuracy of different train- vs. eval-data ratios.**\n",
    "\n",
    "> Change the ratio of training data vs. evaluation data to 30%/70% (currently 70%/30%), fit your model and calculate the new classification accuracy. Subsequently, repeat the experiment a second time using a 10%/90% fraction of training data/evaluation data. What can be observed in both experiments in terms of classification accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Calculate the true-positive as well as the false-positive rate of the Iris versicolor vs. virginica.**\n",
    "\n",
    "> Calculate the true-positive rate as well as false-positive rate of (1) the experiment exhibiting a 30%/70% ratio of training data vs. evaluation data and (2) the experiment exhibiting a 10%/90% ratio of training data vs. evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. k Nearest-Neighbor (kNN) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have closer into a non-parametric method used for supervised classification tasks referred to as the the **k Nearest-Neighbors (kNN)** algorithm. As you learned during the in lecture k-NN classification, the output of the classifier is a class membership. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to running **k Nearest-Neighbor (k-NN)** classification let's briefly revisit the distinct steps of the algorithm as discussed in the lecture:\n",
    "<img align=\"center\" style=\"max-width: 600px; height: auto\" src=\"hsg_knn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the k Nearest-Neighbour algorithm using the delcious **\"Wine\"** dataset is a classic and very easy multi-class classification dataset. \n",
    "\n",
    "The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators (types). The dataset consists of in total **178 wines** as well as their corresponding **13 different measurements** taken for different constituents found in the three types of wine. Please, find below the list of the individual measurements (features):\n",
    "\n",
    ">- `Alcohol`\n",
    ">- `Malic acid`\n",
    ">- `Ash`\n",
    ">- `Alcalinity of ash`\n",
    ">- `Magnesium`\n",
    ">- `Total phenols`\n",
    ">- `Flavanoids`\n",
    ">- `Nonflavanoid phenols`\n",
    ">- `Proanthocyanins`\n",
    ">- `Color intensity`\n",
    ">- `Hue`\n",
    ">- `OD280/OD315 of diluted wines`\n",
    ">- `CProline`\n",
    "\n",
    "Further details on the dataset can be obtained from the following puplication: *Forina, M. et al, PARVUS - \"An Extendible Package for Data Exploration, Classification and Correlation.\", Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.*\n",
    "\n",
    "Let's load the dataset and conduct a preliminary data assessment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect feature names of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect the class names of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect the top 10 feature rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(wine.data).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect the top 10 labels of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(wine.target).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and print the feature dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and print the label dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data distributions of the distinct features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# prepare the dataset to be plotable using seaborn\n",
    "\n",
    "# convert to Panda's DataFrame\n",
    "wine_plot = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "# add class labels to the DataFrame\n",
    "wine_plot['class'] = wine.target\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(wine_plot, diag_kind='hist', hue='class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max normalize the distinct feature values\n",
    "wine_data_norm = normalize(wine.data, norm='max', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect the top 10 feature rows of the normalized dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(wine_data_norm).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Extraction of Training- and Evaluation-Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand and evaluate the performance of any trained supervised machine learning model, it is good practice, to divide the dataset into a **training set** (the fraction of data records solely used for training purposes) and a **evaluation set** (the fraction of data records solely used for evaluation purposes). Pls. note, the **evaluation set** will never be shown to the model as part of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"train_eval_dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the fraction of testing records to **30%** of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fraction = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly split the dataset into training set and evaluation set using sklearns `train_test_split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% training and 30% evaluation\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(wine_data_norm, wine.target, test_size=eval_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the training set dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the evaluation set dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval.shape, y_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. k Nearest-Neighbor (kNN) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Nearest Neighbors Classification, k=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of neighbors `k` to be considered in the classification of each sample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest_neighbors = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the metric used in calculating the distances $D(x, x_i)$ between a sample $x$ and it's neighbors $x_i$. We will use the Euclidean distance that you learned about in the lecture, given by $\\sqrt{(\\sum^n_{i=1}((x - x_i)^2))}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_metric = 'euclidean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init the **kNN classifier** of Python's `sklearn` libary of data science algoritms: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=k_nearest_neighbors, metric=distance_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the k-NN classifier using the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the trained model to predict the response for the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the predicted class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the true class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine **prediction accuracy** of the trained model on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy, k=5: \", metrics.accuracy_score(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the prediction confusion matrix\n",
    "mat = confusion_matrix(y_eval, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion matrix of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# plot confusion matrix heatmap\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='BuGn_r', xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
    "\n",
    "# add plot axis labels\n",
    "plt.xlabel('[true label]')\n",
    "plt.ylabel('[predicted label]')\n",
    "\n",
    "# add plot title\n",
    "plt.title('k-NN Confusion Matrix, k=8');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that as part of the lecture you learned about several measures to evaluate the quality of a retrieval system, namely **Precision**, **Recall** and **F1-Score**. Let's briefly revisit their definition and subsequently calculate those measures based on the confusion matrix above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- The **Precision**, denoted by Precision $=\\frac{TP}{TP + FP}$, is the probability that a retrieved document is relevant.\n",
    ">- The **Recall**, denoted by Recall $=\\frac{TP}{TP + FN}$, is the probability that a relevant document is retrieved.\n",
    ">- The **F1-Score**, denoted by F1-Score $= 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$, combines precision and recall is the harmonic mean of both measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Nearest Neighbors Classification, k=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of neighbors `k` to be considered in the classification of each sample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest_neighbors = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init the **k-NN classifier** of Python's `sklearn` libary of data science algoritms: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=k_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the k-NN classifier using the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the trained model to predict the response for the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine **prediction accuracy** of the trained model on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy, k=8: \", metrics.accuracy_score(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the prediction confusion matrix\n",
    "mat = confusion_matrix(y_eval, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# plot confusion matrix heatmap\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='BuGn_r', xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
    "\n",
    "# add plot axis labels\n",
    "plt.xlabel('[true label]')\n",
    "plt.ylabel('[predicted label]')\n",
    "\n",
    "# add plot title\n",
    "plt.title('k-NN Confusion Matrix, k=8');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also obtain and compare the **Precision**, **Recall**, and, **F1-Score** of the `k=8` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following exercises as part of the lab:\n",
    "\n",
    "**1. Train and evaluate the prediction accuracy of the k=1,...,40 Nearest Neighbor models.**\n",
    "\n",
    "> Write a Python loop that trains and evaluates the prediction accuracy of all k-Nearest Neighbor parametrizations ranging from k=1,...,40. Collect and print the prediction accuracy of each model respectively and compare the results. What kind of behaviour in terms of prediction accuracy can be observed with increasing k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Plot the prediction accuracy of the k=1,...,40 Nearest Neighbor models.**\n",
    "\n",
    "> Plot the prediction accuracy collected for each model above. The plot should display the distinct values of k at the x-axis and the corresponding model prediction accuracy on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Train, evaluate and plot the prediction accuracy of the k=1,...,124 Nearest Neighbor models.**\n",
    "\n",
    "> Train, evaluate and plot the prediction accuracy of all k-Nearest Neighbor parametrizations ranging from k=1,...,124. Collect and print the prediction accuracy of each model respectively and compare the results. What kind of behaviour in terms of prediction accuracy can be observed with increasing k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third lab, a step by step introduction into (1) **Gaussian Naive-Bayes (GNB)** and (2) **k Nearest-Neighbor (kNN)** classification is presented. The code and exercises presented in this lab may serves as a starting point for more complex and tailored programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Pls. note that to convert the notebook, you need to install Python's **nbconvert** library and its extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the nbconvert library\n",
    "!pip install nbconvert\n",
    "!pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert the Jupyter notebook into a plain Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script mldl_lab_03.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
