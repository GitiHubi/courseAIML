{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGwNwDKEt8lG"
   },
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"https://github.com/GitiHubi/courseAIML/blob/master/lab_03/hsg_logo.png?raw=1\">\n",
    "\n",
    "##  Lab 03 - \"Supervised Machine Learning\"\n",
    "\n",
    "Introduction to AI and ML, University of St. Gallen, Autumn Term 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYpS4wEPt8lI"
   },
   "source": [
    "In the last lab, you got your hands dirty with supervised learning by using the Gaussian Naive-Bayes (GNB) classifier. You learned how to train a model and to evaluate and interpret its results. In this lab, we will look at another popular algorithm, namely the **k Nearest-Neighbors (kNN)** classifier.\n",
    "\n",
    "The *discriminative* **k Nearest-Neighbors (kNN)** classifier is a simple, easy to understand, versatile, but powerful machine learning algorithm. Until recently (prior to the advent of deep learning approaches) it was used in a variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition, e.g. in credit ratings, financial institutes used kNN to predict the solvency of customers.\n",
    "\n",
    "This classification technique is part of the **discriminative** type of classifiers, which can be distinguished from the generative type as shown in the following illustration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMdudNYut8lJ"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 600px; height: auto\" src=\"https://github.com/GitiHubi/courseAIML/blob/master/lab_03/classifiers.png?raw=1\">\n",
    "\n",
    "(Courtesy: Intro to AI & ML lecture, Prof. Dr. Borth, University of St. Gallen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Br5f8mEt8lK"
   },
   "source": [
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0Jnx-Ljt8lK"
   },
   "source": [
    "## 1. Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ybF-i5mQt8lL"
   },
   "source": [
    "After today's lab you should be able to:\n",
    "\n",
    "> 1. Know how to setup a **notebook or \"pipeline\"** that solves a simple supervised classification task.\n",
    "> 2. Recognize the **data elements** needed to train and evaluate a supervised machine learning classifier. \n",
    "> 3. Understand how a Gaussian **k Nearest-Neighbor (kNN)** classifier can be trained and evaluated.\n",
    "> 4. Know how to use Python's sklearn library to **train** and **evaluate** arbitrary classifiers.\n",
    "> 5. Understand how to **evaluate** and **interpret** the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svq48yIQt8lM"
   },
   "source": [
    "Before we start let's watch a motivational video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "colab_type": "code",
    "id": "nyE587hOt8lN",
    "outputId": "9efeeded-6784-420b-c407-cc4caeb7caef"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# Microsoft: \"AI for Health Program\"\n",
    "# YouTubeVideo('ii-FfE-7C-k', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZaa0qAnt8lY"
   },
   "source": [
    "## 2. Setup of the Analysis Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2yTCqemyt8la"
   },
   "source": [
    "Similar to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. In this lab will use the `Pandas`, `Numpy`, `Scikit-Learn`, `Matplotlib` and the `Seaborn` library. Let's import the libraries by the execution of the statements below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "o3ShseCwt8lb",
    "outputId": "4df2d7e0-8d27-4912-ed5f-1171e8ec1a3d"
   },
   "outputs": [],
   "source": [
    "# import the numpy and pandas data science library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import sklearn data and data pre-processing libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import k-nearest neighbor classifier library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# import sklearn classification evaluation library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# import matplotlib data visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFnbcu4yt8le"
   },
   "source": [
    "Enable inline Jupyter notebook plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLbxWoZit8lf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsFqwDkYt8ln"
   },
   "source": [
    "Use the 'Seaborn' plotting style in all subsequent visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMH7Y9-Ht8lo"
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Z2tRqzFt8lu"
   },
   "source": [
    "Set random seed of all our experiments - this insures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzE1FzaSt8lu"
   },
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSYdxSTpt8qs"
   },
   "source": [
    "##3. k Nearest-Neighbor (kNN) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JHGca3Iit8qt"
   },
   "source": [
    "Now, let's have closer look into the non-parametric method used for supervised classification tasks, referred to as the the **k Nearest-Neighbors (kNN)** algorithm. As you learned during the in lecture k-NN classification, the output of the classifier is a class membership. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pm4K5p1Qt8qu"
   },
   "source": [
    "Prior to running **k Nearest-Neighbor (kNN)** classification let's briefly revisit the distinct steps of the algorithm as discussed in the lecture:\n",
    "<img align=\"center\" style=\"max-width: 700px; height: auto\" src=\"https://github.com/GitiHubi/courseAIML/blob/master/lab_03/hsg_knn.png?raw=1\">\n",
    "\n",
    "(Courtesy: Intro to AI & ML lecture, Prof. Dr. Borth, University of St. Gallen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hL6bh6cEt8qv"
   },
   "source": [
    "An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfKxtSAMt8qw"
   },
   "source": [
    "### 3.1 Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OljehqMht8qw"
   },
   "source": [
    "Let's try the k Nearest-Neighbour algorithm using the delicious **Wine Dataset**! It is a classic and straightforward multi-class classification dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zTyJoRggt8qx"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 600px; height: auto\" src=\"https://github.com/GitiHubi/courseAIML/blob/master/lab_03/wine_dataset.jpg?raw=1\">\n",
    "\n",
    "(Source: https://www.empirewine.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYsN2L2Gt8qx"
   },
   "source": [
    "The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators (types). The dataset consists in total of **178 wines** as well as their corresponding **13 different measurements** taken for different constituents found in the three types of wine. Please, find below the list of the individual measurements (features):\n",
    "\n",
    ">- `Alcohol`\n",
    ">- `Malic acid`\n",
    ">- `Ash`\n",
    ">- `Alcalinity of ash`\n",
    ">- `Magnesium`\n",
    ">- `Total phenols`\n",
    ">- `Flavanoids`\n",
    ">- `Nonflavanoid phenols`\n",
    ">- `Proanthocyanins`\n",
    ">- `Color intensity`\n",
    ">- `Hue`\n",
    ">- `OD280/OD315 of diluted wines`\n",
    ">- `CProline`\n",
    "\n",
    "Further details on the dataset can be obtained from the following puplication: *Forina, M. et al, PARVUS - \"An Extendible Package for Data Exploration, Classification and Correlation.\", Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.*\n",
    "\n",
    "Let's load the dataset and conduct a preliminary data assessment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cwm84bmft8qy"
   },
   "outputs": [],
   "source": [
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ty0vOQ3Lt8q3"
   },
   "source": [
    "Print and inspect feature names of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "colab_type": "code",
    "id": "B9HA0ItTt8q3",
    "outputId": "5e52ea54-57a5-44d8-8ee2-955b6967fa66"
   },
   "outputs": [],
   "source": [
    "wine.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uxm1svBIt8q6"
   },
   "source": [
    "Print and inspect the class names of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "7cg3VG6mt8q6",
    "outputId": "ccf66fdd-58b9-44ec-a963-d5d01a5256c5"
   },
   "outputs": [],
   "source": [
    "wine.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nl3CY4DVt8q8"
   },
   "source": [
    "Print and inspect the top 10 feature rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "tI6YnJmvt8q8",
    "outputId": "bd0c259e-e9cb-4e01-87fe-03afd407fd9a"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(wine.data, columns=wine.feature_names).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_XBV6Zoht8q-"
   },
   "source": [
    "Print and inspect the top 10 labels of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "coh7WqpKt8q_",
    "outputId": "472ed6f3-127f-4388-b910-c1dd853c1c40"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(wine.target).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKVBBeXft8rB"
   },
   "source": [
    "Determine and print the feature dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "jrjgx9oct8rC",
    "outputId": "cd8a0b89-2c70-4487-c642-d3029e4fb706"
   },
   "outputs": [],
   "source": [
    "wine.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-oLhWbAGt8rE"
   },
   "source": [
    "Determine and print the label dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "wxKIAouGt8rF",
    "outputId": "37285902-42fd-42fa-8b87-142b875f8be3"
   },
   "outputs": [],
   "source": [
    "wine.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dV81H6ret8rJ"
   },
   "source": [
    "Plot the data distributions of the distinct features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "colab_type": "code",
    "id": "I7unVIEWt8rJ",
    "outputId": "f7e68202-43e5-4759-b925-6c27a465b78a"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# prepare the dataset to be plotable using seaborn\n",
    "\n",
    "# convert to Panda's DataFrame\n",
    "wine_plot = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "# add class labels to the DataFrame\n",
    "wine_plot['class'] = wine.target\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(wine_plot, diag_kind='hist', hue='class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbzKED-et8rK"
   },
   "source": [
    "### 3.2 Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgRYuUMKt8rL"
   },
   "source": [
    "#### 3.2.1 Feature Re-Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVzLw8uot8rL"
   },
   "source": [
    "Observing the features values of the **Wine Dataset** we will notice that their respective value ranges vary widely. This results in a major challenge for distance based machine learning algorithms such as the **k Nearest-Neighbor** classifier. The **k Nearest-Neighbour** classifier calculates the distance between two observations using a distance measure such as the **Euclidean** or **Manhattan** distance.\n",
    "\n",
    "If one of the features exhibits a wide range of values, the calculated distance will be governed by this particular feature. Therefore, the range of all features needs to be **re-scaled** or **normalized** to a value range beween in $[0,1]$ or $[-1,1]$ so that each feature contributes approximately proportionately to the final distance. \n",
    "\n",
    "One widley used method of feature re-scaling is referred to as **Min-Max Normalization** and is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6jo00Alt8rL"
   },
   "source": [
    "$$x'={\\frac  {x-{\\text{min}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bo6ERPyUt8rL"
   },
   "source": [
    "Let's re-scale the distinct feature values of the **Wine Dataset** using **Min-Max Normalization** using the `MinMaxScaler` class of the `sklearn` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccNkX14Vt8rM"
   },
   "outputs": [],
   "source": [
    "# init the min-max scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "\n",
    "# min-max normalize the distinct feature values\n",
    "wine_data_norm = scaler.fit_transform(wine.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsWFNsVTt8rS"
   },
   "source": [
    "Print and inspect the top 10 feature rows of the normalized dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "1cqcjpJZt8rT",
    "outputId": "be595c68-b074-41ee-f57a-d1846b3f63d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(wine_data_norm, columns=wine.feature_names).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I3GbEr0jt8rX"
   },
   "source": [
    "Ok, we can observe that all features values have been re-scaled. Let's also statistically validate this observation and determine if all feature values have been re-scaled to a value range between in $[0,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "MzFsjejPt8rX",
    "outputId": "20aebe50-91d7-4c7f-8cf4-30a2340558d2"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(wine_data_norm, columns=wine.feature_names).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHwTTkWxt8rY"
   },
   "source": [
    "Looks great. All feature values are indeed in a range between $[0,1]$. Let's also visualize the re-scaled feature values and inspect their distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "colab_type": "code",
    "id": "UGDK8Me3t8rZ",
    "outputId": "3af54c21-3275-41d9-f7b2-5484669fd9aa"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# prepare the dataset to be plotable using seaborn\n",
    "\n",
    "# convert to Panda's DataFrame\n",
    "wine_plot = pd.DataFrame(wine_data_norm, columns=wine.feature_names)\n",
    "\n",
    "# add class labels to the DataFrame\n",
    "wine_plot['class'] = wine.target\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(wine_plot, diag_kind='hist', hue='class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7PF3yj1t8ra"
   },
   "source": [
    "Excellent, the characteristics of the distinct feature value distributions remained unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYsmr5Cwt8rb"
   },
   "source": [
    "#### 3.2.2 Extraction of Training- and Evaluation-Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nebfDwFWt8rb"
   },
   "source": [
    "To understand and evaluate the performance of any trained **supervised machine learning** model, it is good practice to divide the dataset into a **training set** (the fraction of data records solely used for training purposes) and a **evaluation set** (the fraction of data records solely used for evaluation purposes). Please note, the **evaluation set** will never be shown to the model as part of the training process. All of this is exactly what we did in the Gaussian \"Naive-Bayes\" lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSOKbRwQt8rb"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"https://github.com/GitiHubi/courseAIML/blob/master/lab_03/train_eval_dataset.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COf_ZHbrt8rb"
   },
   "source": [
    "We set the fraction of testing records to **30%** of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_Grln45t8rc"
   },
   "outputs": [],
   "source": [
    "eval_fraction = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZizWtHIct8re"
   },
   "source": [
    "Randomly split the **Wine Dataset** into training set and evaluation set using sklearn's `train_test_split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gj0XNs8Nt8rf"
   },
   "outputs": [],
   "source": [
    "# 70% training and 30% evaluation\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(wine_data_norm, wine.target, test_size=eval_fraction, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FKsJVtxt8rh"
   },
   "source": [
    "Evaluate the training set dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "-YLKYgVst8ri",
    "outputId": "a9578a7f-2b81-4cf2-b98b-1b16d1f49e14"
   },
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMLky8CTt8rj"
   },
   "source": [
    "Evaluate the evaluation set dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "nLB4I8_Gt8rj",
    "outputId": "5345efe3-e423-423c-f272-3e5d58d9de2a"
   },
   "outputs": [],
   "source": [
    "X_eval.shape, y_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66Kx_lPMt8rl"
   },
   "source": [
    "### 3.3 k Nearest-Neighbor (kNN) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ZzDrwGXt8rm"
   },
   "source": [
    "#### 3.3.1. Nearest Neighbors Classification, k=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Mrlx5MFt8rn"
   },
   "source": [
    "Set the number of neighbors `k` to be considered in the classification of each sample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqQ4MM3It8rn"
   },
   "outputs": [],
   "source": [
    "k_nearest_neighbors = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSc5L4XBt8ro"
   },
   "source": [
    "Please, recall that we discussed two distinct distance measures in the lecture to calculate the distance between an observation $x$ and it's $k$-nearest-neighbors $x'_{j}$ in a $n$-dimensonal feature space:\n",
    "\n",
    "**Manhattan distance (\"L1-norm\"):** $$ D(x, x')=\\|\\sum^k_{j=1}\\sum^n_{i=1}(x_{i} - x'_{j,i})\\|$$\n",
    "\n",
    "**Euclidian distance (\"L2-norm\"):** $$ D(x, x')=\\sqrt{\\sum^k_{j=1}\\sum^n_{i=1}(x_{i} - x'_{j,i})^2}$$\n",
    "\n",
    "where the index $j$ denotes the number of $k$-nearest-neighbors and the index $i$ denotes the $i$-th feature of a single nearest neighbor $x_j$. Since the 13 features of the Wine dataset consist of continuous features we will use the Euclidean distance as the distance metric in our kNN classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqGtCmjZt8ro"
   },
   "outputs": [],
   "source": [
    "distance_metric = 'euclidean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRNdv9TGt8rq"
   },
   "source": [
    "Init the **kNN classifier** of Python's `Scikit-Learn` library of data science algorithms: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WN4KdNpwt8rq"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=k_nearest_neighbors, metric=distance_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5NV8Pect8rr"
   },
   "source": [
    "Train the k-NN classifier using the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4vhFkogt8rr"
   },
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfghQDgpt8rt"
   },
   "source": [
    "Utilize the trained model to predict the response for the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MIDL-PPYt8rt"
   },
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTOBjotzt8r0"
   },
   "source": [
    "Let's have a look at the predicted class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "5zbDPKsrt8r0",
    "outputId": "35c71468-b08c-4e1e-ef40-2ee3be262f9b"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jO8PBi_dt8r6"
   },
   "source": [
    "As well as the true class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "pD-xHgePt8r6",
    "outputId": "f6d87beb-1942-4cac-db45-06a901c19cde"
   },
   "outputs": [],
   "source": [
    "y_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTjsY0kUt8r8"
   },
   "source": [
    "Determine **prediction accuracy** of the trained model on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "RIhzCq9_t8r8",
    "outputId": "c9c39b19-f8c2-4c9d-bd21-96c4a32fd920"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy, k=4: \", metrics.accuracy_score(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7k3p5Uet8r-"
   },
   "source": [
    "Determine and plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pfKY4wat8r-"
   },
   "outputs": [],
   "source": [
    "# determine the prediction confusion matrix\n",
    "mat = confusion_matrix(y_eval, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVLVL1O0t8sA"
   },
   "source": [
    "Visualize the **confusion matrix** of the individual predictions determined by the **k=4 Nearest-Neighbor** classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "n-TpchQjt8sA",
    "outputId": "1716d304-624a-49c5-cc22-21d1ea74637e"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# plot confusion matrix heatmap\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='BuGn_r', xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
    "\n",
    "# add plot axis labels\n",
    "plt.xlabel('[true label]')\n",
    "plt.ylabel('[predicted label]')\n",
    "\n",
    "# add plot title\n",
    "plt.title('k-NN Confusion Matrix, k=4');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKWhMqf8t8sD"
   },
   "source": [
    "Remember that as part of the lecture you learned about several measures to evaluate the quality of a retrieval system, namely **Precision**, **Recall** and **F1-Score**. Let's briefly revisit their definition and subsequently calculate those measures based on the confusion matrix above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4eWtzUIt8sD"
   },
   "source": [
    ">- The **Precision**, denoted by Precision $=\\frac{TP}{TP + FP}$, is the probability that a retrieved document is relevant.\n",
    ">- The **Recall**, denoted by Recall $=\\frac{TP}{TP + FN}$, is the probability that a relevant document is retrieved.\n",
    ">- The **F1-Score**, denoted by F1-Score $= 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$, combines precision and recall is the harmonic mean of both measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "colab_type": "code",
    "id": "slpARTnJt8sE",
    "outputId": "dc4b2132-9583-42cd-d7ea-1b0c6e1589a7"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yuATUw4vt8sF"
   },
   "source": [
    "#### 3.3.2. Nearest Neighbors Classification, k=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNIUFJvkt8sF"
   },
   "source": [
    "Set the number of neighbors `k` to be considered in the classification of each sample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iSMgh-6Nt8sF"
   },
   "outputs": [],
   "source": [
    "k_nearest_neighbors = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l5bS5N7gt8sH"
   },
   "source": [
    "Init the **k-NN classifier** of Python's `sklearn` libary of data science algoritms: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qHX0QRIt8sI"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=k_nearest_neighbors, metric=distance_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dEKmgmstt8sL"
   },
   "source": [
    "Train the k-NN classifier using the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1eqyDxyt8sL"
   },
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J1JcOiVKt8sM"
   },
   "source": [
    "Utilize the trained model to predict the response for the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZCPH0m2xt8sM"
   },
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvEEgKfzt8sN"
   },
   "source": [
    "Determine **prediction accuracy** of the trained model on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "Hv7zgswIt8sO",
    "outputId": "35130ea5-f27d-499b-e516-55954f1fdb6e"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy, k=8: \", metrics.accuracy_score(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4u2VufLt8sP"
   },
   "source": [
    "Determine and plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOBriWR-t8sP"
   },
   "outputs": [],
   "source": [
    "# determine the prediction confusion matrix\n",
    "mat = confusion_matrix(y_eval, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Rf40sz6t8sR"
   },
   "source": [
    "Visualize the **confusion matrix** of the individual predictions determined by the **k=8 Nearest-Neighbor** classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "LE_7IAbOt8sR",
    "outputId": "777a0aad-74b2-4d54-9bb1-9f6619acfa83"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# plot confusion matrix heatmap\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='BuGn_r', xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
    "\n",
    "# add plot axis labels\n",
    "plt.xlabel('[true label]')\n",
    "plt.ylabel('[predicted label]')\n",
    "\n",
    "# add plot title\n",
    "plt.title('k-NN Confusion Matrix, k=8');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "colab_type": "code",
    "id": "BiGJMg79t8sW",
    "outputId": "c5218d6d-9c5a-4401-f50f-cf330db9cc08"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXwgts9Yt8sY"
   },
   "source": [
    "#### 3.3.3. Finding the optimal k of the kNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nV_vgPzDt8sZ"
   },
   "source": [
    "Until now we have investigated the Euclidean distance based kNN classifer for two distinct values of $k=4$ and $k=8$. However the question remains: can we locate a value for k that yields an even higher classification accuracy? \n",
    "\n",
    "Let's therefore investigate the classification performance of a wider range of distinct $k$ values and in particular compare the corresponding classification accuracy. We will do so in the following by specifying a range of distinct $k$ values ranging from $k=1, ..., 50$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6cblKEXt8sZ"
   },
   "outputs": [],
   "source": [
    "# try k=1 through k=50 to be evaluated\n",
    "k_range = range(1, 51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8oLJdsWDt8sb"
   },
   "source": [
    "To run the kNN classification experiments for different values of $k$ we will define a python loop. The loop iterates over the range of distinct k's and conducts the model training (using the training data) and evaluation (using the evaluation data). The classification accuracy for each $k$ value will be collected and stored in a designated list of accuracy scores: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Q0TwLLyt8sb"
   },
   "outputs": [],
   "source": [
    "# init the distinct accuracy scores obtained on the evaluation data\n",
    "eval_accuracy_scores = []\n",
    "\n",
    "# iterate over the distinct k values\n",
    "for k in k_range:\n",
    "    \n",
    "    # init the k-NN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    \n",
    "    # train the k-NN classifer on the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluate the k-NN classifier on the training data\n",
    "    y_train_pred = knn.predict(X_train)\n",
    "    \n",
    "    # evaluate the k-NN classifier on the evaluation data\n",
    "    y_eval_pred = knn.predict(X_eval)\n",
    "    \n",
    "    # collect the classification accuracy of the current k on the evaluation data\n",
    "    eval_accuracy_scores.append(metrics.accuracy_score(y_eval, y_eval_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZIpTs4ut8sd"
   },
   "source": [
    "Visualizing the collected classification accuracy scores of the distinct $k$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "id": "ng4ljiGLt8sd",
    "outputId": "fbccbccc-0a3c-4b10-c379-c648f977f772"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot the classification accuracy of distinct k's\n",
    "ax.plot(range(1, len(eval_accuracy_scores)+1), eval_accuracy_scores, color='green', marker='o')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"[$k$-Nearest-Neighbors]\", fontsize=10)\n",
    "ax.set_ylabel(\"[% classification accuracy]\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('k-NN Classification Accuracy', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Y0XkrSDt8sf"
   },
   "source": [
    "Alright, we can nicely observe a constantly high classification accuracy on the held out evaluation dataset. For a given seed, which we defined at the beginning of this lab (it insures the results are reproducible), you may want to use the $k$ value that is the most accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5u9i6tmSt8sg"
   },
   "source": [
    "### 3.4 Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XamqRlpot8sg"
   },
   "source": [
    "We recommend you to try the following exercises as part of the lab:\n",
    "\n",
    "**1. Train, evaluate and plot the prediction accuracy of the k=1,...,40 Nearest Neighbor models.**\n",
    "\n",
    "> Write a Python loop that trains and evaluates the prediction accuracy of all k-Nearest Neighbor parametrizations ranging from k=1,...,40. Collect and print the prediction accuracy of each model respectively and compare the results. Plot the prediction accuracy collected for each model above. The plot should display the distinct values of k at the x-axis and the corresponding model prediction accuracy on the y-axis. What kind of behaviour in terms of prediction accuracy can be observed with increasing k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMjmHfA6t8sg"
   },
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1vLeQMMt8sj"
   },
   "source": [
    "**2. Train, evaluate and plot the prediction accuracy of the k=1,...,40 Nearest Neighbor models without re-scaling the individual feature values.**\n",
    "\n",
    "> Write a Python loop that trains and evaluates the prediction accuracy of all k-Nearest Neighbor parametrizations ranging from k=1,...,40. Collect and print the prediction accuracy of each model respectively and compare the results. Plot the prediction accuracy collected for each model above. The plot should display the distinct values of k at the x-axis and the corresponding model prediction accuracy on the y-axis. What kind of behaviour in terms of prediction accuracy can be observed with increasing k? What do you observe when comparing the results of the non re-scaled with the results obtained for the scaled features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ikdzkjK0t8sk"
   },
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Lwqs0XLt8sr"
   },
   "source": [
    "**3. Train, evaluate and plot the prediction accuracy of the k=1,...,40 Nearest Neighbor models using the \"Manhattan\" distance.**\n",
    "\n",
    "> Write a Python loop that trains and evaluates the prediction accuracy of all k-Nearest Neighbor parametrizations ranging from k=1,...,40 using the \"Manhattan\" instead of the \"Euclidian\" distance metric. Collect and print the prediction accuracy of each model respectively and compare the results. Plot the prediction accuracy collected for each model above. The plot should display the distinct values of k at the x-axis and the corresponding model prediction accuracy on the y-axis. What kind of behaviour in terms of prediction accuracy can be observed with increasing k? What do you observe when comparing the results obtained for the \"Manhattan\" distance with the ones obtained for the \"Euclidean\" distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIUran5pt8ss"
   },
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n94u0rxat8su"
   },
   "source": [
    "## 4. Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DCOEZj-it8sv"
   },
   "source": [
    "In this lab, a step by step introduction into **k Nearest-Neighbor (kNN)** classification is presented. The code and exercises presented in this lab may serves as a starting point for more complex and tailored programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzWZDKpat8s4"
   },
   "source": [
    "You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Pls. note that to convert the notebook, you need to install Python's **nbconvert** library and its extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-D-azATot8s4",
    "outputId": "63dc54ad-a97d-4a4f-cfeb-31c9f957fa76"
   },
   "outputs": [],
   "source": [
    "# installing the nbconvert library\n",
    "!pip install nbconvert\n",
    "!pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1z07F-kot8s_"
   },
   "source": [
    "Let's now convert the Jupyter notebook into a plain Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Q4Z4L4vIt8s_",
    "outputId": "b43b368e-a369-4454-dda3-84d67e187a32"
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script aiml_lab_03b.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eGwNwDKEt8lG",
    "D0Jnx-Ljt8lK",
    "CZaa0qAnt8lY",
    "fSYdxSTpt8qs",
    "n94u0rxat8su"
   ],
   "name": "aiml_kNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
