{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "1035.5999755859375px",
        "left": "38px",
        "top": "109.39990234375px",
        "width": "276.79998779296875px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Copie de aiml_colab_04b.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KKt6ILZNqyob",
        "eFBdkHHMqyoj",
        "Q5lEeALnqyol",
        "MCdi1OnIqypY"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKt6ILZNqyob"
      },
      "source": [
        "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"https://github.com/GitiHubi/courseAIML/blob/master/lab_04/hsg_logo.png?raw=1\">\n",
        "\n",
        "##  Lab 04 - \"Unsupervised Machine Learning\"\n",
        "\n",
        "Introduction to AI and ML, University of St. Gallen, Autumn Term 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwXiy0Isqyoh"
      },
      "source": [
        "In the last lab you learned about how to utilize **supervised** learning classification techniques namely (1) the Gaussian Naive-Bayes (Gaussian NB) classifier, (2) the k Nearest-Neighbor (kNN) classifier and (3) the Logistic Regression classifer. \n",
        "\n",
        "In this lab we will learn about an **unsupervised** machine learning technique referred to as **Expectation-Maximization (EM) Clustering**. We will use this technique to classify un-labelled data (i.e., data without defined categories or groups). In general, clustering-based techniques are widely used in **unsupervised machine learning**.\n",
        "\n",
        "<img align=\"center\" style=\"max-width: 500px\" src=\"https://github.com/GitiHubi/courseAIML/blob/master/lab_04/machinelearning.png?raw=1\">\n",
        "\n",
        "(Courtesy: Intro to AI & ML lecture, Prof. Dr. Borth, University of St. Gallen)\n",
        "\n",
        "The **Expectation-Maximization (EM) Clustering** algorithm is another popular clustering algorithms used in machine learning. The goal of EM Clustering is to maximize the likelihood of the underlying sub-distributions in a given dataset by estimating the parameters those distributions. Similarly to the **k-Means** algorithm it is often used (1) to **confirm business assumptions** about what types of groups exist or (2) to **identify unknown groups** in complex data sets. Some examples of business-related use cases are:\n",
        "\n",
        ">- Segment customers by purchase history;\n",
        ">- Segment users by activities on an application or a website;\n",
        ">- Group inventory by sales activity; or,\n",
        ">- Group inventory by manufacturing metrics.\n",
        "\n",
        "(Source: https://towardsdatascience.com/a-comparison-between-k-means-clustering-and-expectation-maximization-estimation-for-clustering-8c75a1193eb7)\n",
        "\n",
        "Once the algorithm has been run and the groups are defined, any new data can be easily assigned to the correct group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAQLAvPdqyoi"
      },
      "source": [
        "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFBdkHHMqyoj"
      },
      "source": [
        "## 1. Lab Objectives:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJoaTESPqyok"
      },
      "source": [
        "After today's lab, you should be able to:\n",
        "\n",
        "> 1. Know how to setup a **notebook or \"pipeline\"** that solves a simple unsupervised clustering task.\n",
        "> 2. Understand how a **Expectation-Maximization (EM) Clustering** algorithm can be trained and evaluated.\n",
        "> 3. Know how to select an **optimal number of clusters** or cluster means.\n",
        "> 4. Know how to Python's **sklearn library** to perform unsupervised clustering.\n",
        "> 5. Understand how to **evaluate** and **interpret** the obtained clustering results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5lEeALnqyol"
      },
      "source": [
        "## 2. Setup of the Analysis Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlyHvE4Cqyom"
      },
      "source": [
        "Suppress potential warnings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N5bdi6xqyon"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AlJvxG_qyo0"
      },
      "source": [
        "Similarly to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. In this lab will use the `Pandas`, `Numpy`, `Scikit-Learn (sklearn)`, `Matplotlib` and the `Seaborn` library. Let's import the libraries by the execution of the statements below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4vPdQSyqyo5"
      },
      "source": [
        "# import the pandas data science library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the scipy spatial distance capability\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# import sklearn data sample generator libraries\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "\n",
        "# import sklearn k-means and gaussian-mixture classifier library\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# import matplotlib data visualization library\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# import matplotlibs 3D plotting capabilities\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvAze4dPqypJ"
      },
      "source": [
        "Create nice looking plots using the **seaborn** plotting theme:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTQH0L3aqypK"
      },
      "source": [
        "plt.style.use('seaborn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oxBD4fMqypO"
      },
      "source": [
        "Enable inline Jupyter notebook plotting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH3ouQfUqypP"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfDslDbOqypT"
      },
      "source": [
        "Set random seed of all our experiments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrkrJQqIqypU"
      },
      "source": [
        "random_seed = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCdi1OnIqypY"
      },
      "source": [
        "## 3. Expectation Maximization (EM) Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKlT1u0kqypY"
      },
      "source": [
        "\"One way to think about the k-means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster\" (Jake VanderPlas, The Python Data Science Handbook). To investigate this limitation of the **k-Means Clustering** algorithm let's have a closer look at an exemplary very skewed data distribution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2efLjoQLqypa"
      },
      "source": [
        "### 3.1. Dataset Creation and Data Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8EbNk76qypa"
      },
      "source": [
        "Let's create a very skewed distribution that exhibits a eliptical characteristic:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82wZlfgNqypb"
      },
      "source": [
        "# generate a sample data distribution\n",
        "data, labels = make_blobs(n_samples = 1000, centers = 3, cluster_std = .7, random_state = 0)\n",
        "\n",
        "# init the random state and skew the data\n",
        "random_state = np.random.RandomState(8)\n",
        "data_skewed = np.dot(data, random_state.randn(2, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luj0hHqtqyph"
      },
      "source": [
        "Once created, let's visualize the skewed distribution accordingly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UveptUS0qypi"
      },
      "source": [
        "# init the plot\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# add grid\n",
        "ax.grid(linestyle='dotted')\n",
        "\n",
        "# plot the two dimensions of the skewed distribution\n",
        "scatter = ax.scatter(data_skewed[:,0], data_skewed[:,1])\n",
        "\n",
        "# add axis legends\n",
        "ax.set_xlabel(\"[feature $x_1$]\", fontsize=14)\n",
        "ax.set_ylabel(\"[feature $x_2$]\", fontsize=14)\n",
        "\n",
        "# add plot title\n",
        "plt.title('Sample Skewed Distribution', fontsize=14);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZEtRolcqypm"
      },
      "source": [
        "Ok, the distribution looks indeed very skewed. It furthermore seem to consist of three commingled eliptical clusters. Let's see if we can identify those clusters using the k-Means Clustering algorithm that we discussed before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e5d6E4Pqypn"
      },
      "source": [
        "# init the k-Means Clustering algorithm\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, max_iter=100)\n",
        "\n",
        "# fit the clustering to the the data\n",
        "kmeans.fit(data_skewed)\n",
        "\n",
        "# obtain the cluster labels\n",
        "kmeans_labels_skewed = kmeans.labels_\n",
        "\n",
        "# obtain the cluster means\n",
        "kmeans_means_skewed = kmeans.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxrlFB6Bqypq"
      },
      "source": [
        "Let's now visualize the results of the clustering as well as the obtained clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgER4_Zsqypr"
      },
      "source": [
        "# init the plot\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# add grid\n",
        "ax.grid(linestyle='dotted')\n",
        "\n",
        "# plot petal length vs. petal width and corresponding classes\n",
        "scatter = ax.scatter(data_skewed[:,0], data_skewed[:,1], c=kmeans_labels_skewed.astype(np.float), cmap=plt.cm.Set1)\n",
        "\n",
        "# prepare data legend\n",
        "ax.legend(*scatter.legend_elements(), loc='upper left', title='Cluster')\n",
        "\n",
        "# plot cluster means\n",
        "ax.scatter(kmeans_means_skewed[:,0], kmeans_means_skewed[:,1], marker='x', c='black', s=100)\n",
        "\n",
        "# iterate over distinct cluster means\n",
        "for i, mean_skewed in enumerate(kmeans_means_skewed):\n",
        "    \n",
        "    # determine max cluster point distance\n",
        "    cluster_radi = cdist(data_skewed[:, 0:2][kmeans_labels_skewed==i], [mean_skewed]).max()\n",
        "    \n",
        "    # plot cluster size\n",
        "    ax.add_patch(plt.Circle(mean_skewed, cluster_radi, fc='darkgrey', edgecolor='slategrey', lw=1, alpha=0.1, zorder=1))\n",
        "\n",
        "# add axis legends\n",
        "ax.set_xlabel(\"[feature $x_1$]\", fontsize=14)\n",
        "ax.set_ylabel(\"[feature $x_2$]\", fontsize=14)\n",
        "\n",
        "# add plot title\n",
        "plt.title('Sample Skewed Distribution - k-Means Clustering Results', fontsize=14);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDV08eESqypv"
      },
      "source": [
        "Based on the example above we observe one of the main disadvantages of the **k-Means Clustering** algorithm. The k-Means Clustering has no way to account for oblong or elliptical clusters. Therefore, it fails to recognize the distinct clusters in very skewed distributions.\n",
        "\n",
        "Unlike the k-Means Clustering algorithm, the **Expectation Maximization (EM)** algorithm (introduced in the following section of the notebook) is not limited to spherical shapes. Using the EM algorithm we can constrain the algorithm to provide different covariance matrices (spherical, diagonal and generic). These different covariance matrices in return allow to control the shape of our clusters and hence we can detect sub-populations in our data with different characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OagiVaNFqypw"
      },
      "source": [
        "### 3.2. The Expectation Maximization (EM) Algorithm of Gaussian Mixtures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYzLWkf4qypx"
      },
      "source": [
        "The **Expectation Maximization (EM)** algorithm is similar to the k-Means Clustering technique. However, instead of assigning examples to clusters to maximize the differences in means, the EM Clustering algorithm computes probabilities of cluster memberships based on one or more probability distributions. \n",
        "\n",
        "The goal of the clustering algorithm then is to maximize the overall probability or likelihood of the data, given the (final) clusters. Thereby, the objective of EM clustering is to estimate the means and standard deviations for each cluster so as to maximize the likelihood of the observed data (distribution). To achieve this objective the algorithm iteratively computes an **(1) Expectation-Step** and **(2) Maximization-Step** as described in the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkT7rhVSqypy"
      },
      "source": [
        "#### Step-1: The Expectation-Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBrta3Iaqypy"
      },
      "source": [
        "In the **\"Expectation Step\"** we determine the expected values $E(z_{ij} | \\mu_{j}^{t}, \\sigma_{j}^{t})$ that denotes the probability of a given observation $x_i \\in \\mathcal{R}^d$ that it was drawn from the $j^{th}$ distribution (or cluster):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZkKTG2Eqypz"
      },
      "source": [
        "$$E(z_{ij}) = \\frac{P(x_i|\\mu_{j}^{t}, \\sigma_{j}^{t})}{\\sum_{k'=1}^k P(x_i|\\mu_{k'}^{t},\\sigma_{k'}^{t})},$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEafNCI3qyp0"
      },
      "source": [
        "were:\n",
        "\n",
        "- $x_i \\in X$ denotes a particular data observation;\n",
        "- $t$ denotes the current iteration of the EM Clustering algorithm;\n",
        "- $\\mu_j$ and $\\sigma_j$ denotes the mean and (co-) variance of the $j^{th}$ distribution (or cluster)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_TCkR6bqyp1"
      },
      "source": [
        "During the lecture you learned that the probability distribution of each cluster might be approximated by a Gaussian (Normal) probability distribution $\\mathcal{N}(\\mu, \\sigma)$. Hence the approach is usually referred to as the **EM-Algorithm of Gaussian Mixtures**. This simplification is justified by the application of the **\"law of large numbers\"** or **\"Central Limit Theorem\"** (you may want to have a look at the details of the theorem via the following link: https://en.wikipedia.org/wiki/Central_limit_theorem). In general, the probability density of a Gaussian \"Normal\" distribution, as defined by the formula below. It is parametrized its mean $\\mu$ and corresponding standard deviation $\\sigma$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFvAkmHmqyp2"
      },
      "source": [
        "$$ P(x|\\mu,\\sigma)  \\approx \\mathcal{N}(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^{2}}(x - \\mu)^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RMU2RV0qyp3"
      },
      "source": [
        "Using the **Central Limit Theorem** we can rewrite the formula of the Expectation Step as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHY0mp49qyp4"
      },
      "source": [
        "$$E(z_{ij}) = \\frac{P(x_i|\\mu_j^{t}, \\sigma_j^{t})}{\\sum_{k'=1}^k P(x_i|\\mu_{k'}^{t},\\sigma_{k'}^{t})} = \\frac{\\mathcal{N}(x_i|\\mu_j^{t}, \\sigma_j^{t})}{\\sum_{k'=1}^k \\mathcal{N}(x_i|\\mu_{k'}^{t},\\sigma_{k'}^{t})} = \\frac{ \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma_{j}^{t 2}}(x_i - \\mu_{j}^{t})^2} }{ \\sum_{k'=1}^{k} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma_{k'}^{t 2}}(x_i - \\mu_{k'}^{t})^2} }$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8YKSyJoqyp6"
      },
      "source": [
        "The distinct $E(z_{ij})$ values $z_{1j}$, $z_{2j}$, ..., $z_{nj}$ represent the probability distribution of the $j^{th}$ cluster that the data point is drawn from. Understanding the range of values the $z$ values can take is important. Each observation $x_i$ has k associated $E(z_{ij})$ values. In the **k-Means Clustering** algorithm each $z_{ij}$ can only take the value 0 or 1. This is why the k-Means Clustering algorithm is referred to as **\"hard\"** clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq9A5TUiqyp9"
      },
      "source": [
        "In contrast, the **\"EM-Clustering\"** algorithm is referred to as **\"soft\"** or **\"fuzzy\"** clustering. In EM-Clustering the distinct observations $x_i$ are considered to be drawn probabilistically from the distinct cluster distributions $j$. The corresponding $z_{ij}$ values can therefore be $z_{i1}$=0.85, $z_{i2}$=0.10 and $z_{i3}$=0.05, which represents a strong probability that the $x_{i}$ value originates from distribution (or cluster) 1 and a exhibit a smaller probability that it originates from distribution (or cluster) 2 or 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS0AZqBEqyqc"
      },
      "source": [
        "#### Step-2: The Maximization-Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cocGZHgSqyqe"
      },
      "source": [
        "In the **\"Maximization-Step\"** we calculate update the parameters of each Gaussian \"Normal\" cluster distribution. Therefore, we derive for each $j^{th}$ distribution (or cluster) a new $\\mu_{j}$ and $\\sigma_{j}$ parameter value as given by:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRv3UdNIqyqf"
      },
      "source": [
        "$$ \\mu_{k}^{t+1} = \\frac {\\sum_{k'=1}^{k}E(z_{ij})x_i} {\\sum_{k'=1}^{k}E(z_{ij})}; \\sigma_{j}^{2, t+1} = \\frac {\\sum_{k'=1}^{k}E(z_{ij}) (x_i - \\mu_{k}^{t})^{2}} {\\sum_{k'=1}^{k}E(z_{ij})}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-CikgYoqyqg"
      },
      "source": [
        "### 3.3. Expectation Maximization (EM) in a 2-Dimensional Feature Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaNUu9t6qyqp"
      },
      "source": [
        "Similarly, to k-Means Clustering let's now define the parameters of the **EM-Clustering** algorithm. We will start by specifying the **number of clusters** $k$ we aim to detect in the iris dataset. We again hypothesize that our observations are drawn from an unknown distributions of three iris flower species (each distribution corresponding to a different mean $\\mu_1$, $\\mu_2$, and, $\\mu_3$ and corresponding standard deviation $\\sigma_1$, $\\sigma_2$, and, $\\sigma_3$). Therefore, we set the number of clusters to be detected to $k=3$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XqJtLvwqyqq"
      },
      "source": [
        "no_clusters = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R18ZORVbqyqu"
      },
      "source": [
        "Next, we need to define a corresponding number of **initial 'means' $\\mu_{i}$** (the initial random cluster centers) that will be used as 'starting points' in the first iteration of the clustering process. In our case we will specify $k=3$ cluster means each of dimension 2, since we aim to retrieve 3 clusters based on the 2 features $x_1$ and $x_2$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYFAs0WWqyqu"
      },
      "source": [
        "init_means = np.array([[1.0, 3.0], [2.0, 6.0], [1.0, 7.0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPbybleSqyqy"
      },
      "source": [
        "Finally, we will define a **maximum number of iterations** that we want to run the **EM-Clustering** algorithm. Please, note that the clustering terminates once there will be no further changes in the cluster assignments. However, it's good practice to define an upper bound of the iterations applied in the clustering (especially when analyzing datasets that exhibt a high-dimensional feature space):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNwbwfvdqyqz"
      },
      "source": [
        "max_iterations = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pd0wO0qyq5"
      },
      "source": [
        "Now, we are ready to initialize an instance of the **EM-Clustering** algorithm using Python's `sklearn` library of data science algorithms. Please note again, that for each classifier, available in the `sklearn` library, a designated and detailed documentation is provided. It often also includes a couple of practical examples and use cases. The documentation of the **EM-Clustering** algorithm can be obtained from the following url: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xx1ffnpqyq6"
      },
      "source": [
        "em = GaussianMixture(n_components=no_clusters, means_init=init_means, max_iter=max_iterations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua8Zxn1wqyq_"
      },
      "source": [
        "Let's run the **EM-Clustering** to learn a model of the $x_1$ and $x_2$ features. Similar to the k-Means Clustering we will again make use of the `fit()` method provided by `sklearn` for each of its classifiers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKuvhvs_qyrA"
      },
      "source": [
        "em.fit(data_skewed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9toP_DKRqyrE"
      },
      "source": [
        "Now that we have conducted the clustering, let's inspect the distinct cluster labels that have been assigned to the individual records of our artificial dataset. This can be achieved by calling the `predict()` function of the fitted model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE3cqqNHqyrF"
      },
      "source": [
        "em_labels_skewed = em.predict(data_skewed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNZYNFN7qyrJ"
      },
      "source": [
        "Furthermore, we want to inspect the coordinates of the cluster means (sometimes also referred to as \"centroids\") assigned by the algorithm. This can be achieved by calling the `means_`attribute of the fitted model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMQInyZ5qyrK"
      },
      "source": [
        "em_means_skewed = em.means_ \n",
        "em_means_skewed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XwOfeHmqyrN"
      },
      "source": [
        "Let's now visually inspect the clustering results of the two features $x_1$ and $x_2$ in terms of the cluster assignments of each observation and cluster means learned by the **EM-Clustering** algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2-4AXxoqyrO"
      },
      "source": [
        "# init the plot\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# add grid\n",
        "ax.grid(linestyle='dotted')\n",
        "\n",
        "# plot x_1 vs. x_2 and corresponding cluster labels\n",
        "scatter = ax.scatter(data_skewed[:,0], data_skewed[:,1], c=em_labels_skewed.astype(np.float), cmap=plt.cm.Set1)\n",
        "\n",
        "# plot cluster means\n",
        "ax.scatter(em_means_skewed[:,0], em_means_skewed[:,1], marker='x', c='black', s=100)\n",
        "\n",
        "# add axis legends\n",
        "ax.set_xlabel(\"[feature $x_1$]\", fontsize=14)\n",
        "ax.set_ylabel(\"[feature $x_2$]\", fontsize=14)\n",
        "\n",
        "# add plot title\n",
        "plt.title('Sample Skewed Distribution - Expectation Maximization Results', fontsize=14);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMPaXWh2qyrT"
      },
      "source": [
        "Ok, we can observe that the **EM Clustering** technique nicely clusters the distinct distributions of the skewed distributions. We notice that the results are quite different from those computed by the k-Means clustering. As initially stated, the EM Clustering determines the **probability of cluster memberships** for each observation based on the initially specified $k$ probability distributions. Let's obtain the probability of each observation using the `predict_proba()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3wtSfCtqyrU"
      },
      "source": [
        "em_probabilities_skewed = em.predict_proba(data_skewed)\n",
        "em_probabilities_skewed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ezopZvFqyrX"
      },
      "source": [
        "Let's now visually inspect the clustering results of the two features $x_1$ and $x_2$ in terms of the learned probabilities of each observation corresponding to a particular cluster as determined by the **EM-Clustering** algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzny-qwRqyrZ"
      },
      "source": [
        "# init the plot\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# add grid\n",
        "ax.grid(linestyle='dotted')\n",
        "\n",
        "# plot x_1 vs. x_2 and corresponding cluster labels\n",
        "scatter = ax.scatter(data_skewed[:,0], data_skewed[:,1], c=em_probabilities_skewed.astype(np.float), cmap=plt.cm.Set1)\n",
        "\n",
        "# plot cluster means\n",
        "ax.scatter(em_means_skewed[:,0], em_means_skewed[:,1], marker='x', c='black', s=100)\n",
        "\n",
        "# add axis legends\n",
        "ax.set_xlabel(\"[feature $x_1$]\", fontsize=14)\n",
        "ax.set_ylabel(\"[feature $x_2$]\", fontsize=14)\n",
        "\n",
        "# add plot title\n",
        "plt.title('Sample Skewed Distribution - Expectation Maximization Results', fontsize=14);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u85ChwzLqyre"
      },
      "source": [
        "It can be observed that the EM-Clustering algorithm conducts a **\"soft\"** or **\"fuzzy\"** cluster assignment especially for observations that reside at the edges of the distinct clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIrn7u3vWGn4"
      },
      "source": [
        "### 3.4 Bayesian Information Criterion (BIC) to find Optimum $k$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p--Oo5QT_6uW"
      },
      "source": [
        "In chapter 3.3, we hypothesized that the correct number of clusters in our *skewed data* was $k = 3$. What if we were wrong? How can we be sure that $3$ is the correct value? What if we had no particular guess? One way to increase the likelihood that a given observation belongs to its closest cluster center is by increasing the number of clusters as the distances will be shorter. However, this can lead to **overfitting** (image an extreme scenario in which there are as many clusters as there are observations).\n",
        "\n",
        "One way of mathematically determining the correct value $k$ should take is by using the **'Bayesian Information Criteria (BIC)'** technique described in the lecture.\n",
        "Please note again, that for each algorithm available in the `sklearn` library, a designated and detailed documentation is provided. It often also includes a couple of practical examples and use cases. The documentation of the **BIC** algorithm can be obtained from the following url: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/mixture.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DppVejOtt7OM"
      },
      "source": [
        "**BIC** is a probabilistic statistical measure, which tries to express the model's performance on the training set. It is thus used for model selection and resembles other techniques like the *Akaike Information Criterion (AIC)*. It is useful in that it only requires the training set to find the optimal number of clusters *$k$*. Essentially, it expresses model performance based on different *$k$* values based on the model's performance and complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EevD0Y19w2Cb"
      },
      "source": [
        "It makes sure that on the one hand, model performance is optimized (we want to decrease fitting error) and that on the other hand, complexity is minimized (the simpler the model, the better). **BIC** consists of the following formula (there are some variants - this one is taken from The Elements of Statistical Learning, see https://link.springer.com/book/10.1007%2F978-0-387-84858-7):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJjtKb1IETvc"
      },
      "source": [
        "$$BIC = -2 LL + k*ln(n)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WTD5iehzRkE"
      },
      "source": [
        "where:\n",
        ">- LL is the log-likelihood of the model (measures goodness of fit). It can be expressed as $LL = ln*P(x_{1}, .., x_{n}|k)$\n",
        ">- *$k$* is the number of clusters (parameters)\n",
        ">- *$n$* is the number of training samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S__8L4xr5Kuz"
      },
      "source": [
        "This is expressed very similarly in the class slides:\n",
        "\n",
        "$$K^* = argmin_{K} -2*lnP(x_{1}, .., x_{n}|K) + f(K) \\frac {n}{log(n)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYEyycJ70LfD"
      },
      "source": [
        "The **BIC** score is minimized so that the model with the lowest **BIC** is chosen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHdP6lsf0bwh"
      },
      "source": [
        "As mentionned before, **BIC** deals with the performance/complexity trade-off. The first part of the equation ($-2LL$) looks at the fitting error and tries to minimize it, while the second part ($k*ln(n)$) deals with the model's complexity. A model with low error but a really high number of clusters is useless, as will be apparent in the visualization below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxAVnfVpDY_E"
      },
      "source": [
        "Let's illustrate this using Python! We will write code based on https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html#sphx-glr-auto-examples-mixture-plot-gmm-selection-py."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a6zmoM2Bia1"
      },
      "source": [
        "First, we compute the **BIC** scores for different *$k$* values, let's say in range(1, 10):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFUJBulCWV5R"
      },
      "source": [
        "# Since we will want to replace this lowest BIC score variable with the \n",
        "# lowest computed, we can first let it be infinity\n",
        "lowest_bic = np.infty\n",
        "\n",
        "# Create an empty list where we will append BIC scores for the different k values\n",
        "bic = []\n",
        "\n",
        "# Define range of k values\n",
        "k_range = range(1, 10)\n",
        "\n",
        "# Loop over them\n",
        "for k_value in k_range:\n",
        "\n",
        "    # We define the EM model\n",
        "    gmm = GaussianMixture(n_components=k_value, max_iter=max_iterations, random_state=random_seed)\n",
        "    \n",
        "    # And fit it to the data for each k\n",
        "    gmm.fit(data_skewed)\n",
        "\n",
        "    # Append the BIC score the the list (use .bic method from sklearn)\n",
        "    bic.append(gmm.bic(data_skewed))\n",
        "\n",
        "    # If this particular score is the lowest yet, replace the lowest_bic value with it\n",
        "    if bic[-1] < lowest_bic:\n",
        "        lowest_bic = bic[-1]\n",
        "        best_gmm = gmm\n",
        "\n",
        "# Transform BIC list into an array\n",
        "bic = np.array(bic)\n",
        "\n",
        "# Designate the best of our classifiers\n",
        "clf = best_gmm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXn1mY6t2n49"
      },
      "source": [
        "Let's now verify the results by printing the best classifier's parameters, the **BIC** score for each *$k$* and the lowest **BIC** score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWLwqLwDfmDh"
      },
      "source": [
        "print('Best classifier parameters:', clf)\n",
        "print('\\nBIC score for each value of k:', bic)\n",
        "print('\\nLowest BIC score:', lowest_bic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmBjAPwR21u7"
      },
      "source": [
        "We can now plot each *$k$*'s individual **BIC** score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf827YetkOIR"
      },
      "source": [
        "# Define figure and size\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Ticks\n",
        "plt.xticks(k_range)\n",
        "\n",
        "# Define plot limit\n",
        "plt.ylim([bic.min() - 100, bic.max() + 100])\n",
        "\n",
        "# Title\n",
        "plt.title('BIC score $k$ value')\n",
        "\n",
        "# Plot the BIC scores (line)\n",
        "plt.plot(k_range, bic)\n",
        "\n",
        "# Lables\n",
        "plt.ylabel('BIC score')\n",
        "plt.xlabel('Number of clusters $k$')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG99W-J93dXZ"
      },
      "source": [
        "Finally, let's see how chosing different *$k$* values would have affected the clusters by visualizing them!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hxZdY0-4412"
      },
      "source": [
        "# Define figure and size\n",
        "fig = plt.figure(1, (14, 14))\n",
        "\n",
        "# Loop over k values\n",
        "for k_value in k_range:\n",
        "\n",
        "    # Fit EM with our skewed data\n",
        "    gmm = GaussianMixture(n_components=k_value, max_iter=max_iterations, random_state=random_seed)\n",
        "    gmm.fit(data_skewed)\n",
        "\n",
        "    # Compute cluster membership and cluser means\n",
        "    gmm_labels_skewed = gmm.predict(data_skewed)\n",
        "    gmm_means_skewed = gmm.means_ \n",
        "\n",
        "    # Define subplot\n",
        "    ax = plt.subplot(3, 3, k_value)\n",
        "\n",
        "    # add grid\n",
        "    ax.grid(linestyle='dotted')\n",
        "\n",
        "    # plot x_1 vs. x_2 and corresponding cluster labels\n",
        "    ax.scatter(data_skewed[:,0], data_skewed[:,1], c=gmm_labels_skewed.astype(np.float), cmap=plt.cm.Set1)\n",
        "\n",
        "    # plot cluster means\n",
        "    ax.scatter(gmm_means_skewed[:,0], gmm_means_skewed[:,1], marker='x', c='black', s=100)\n",
        "\n",
        "    # add axis legends\n",
        "    ax.set_xlabel(\"[feature $x_1$]\", fontsize=10)\n",
        "    ax.set_ylabel(\"[feature $x_2$]\", fontsize=10)\n",
        "\n",
        "    # add plot title\n",
        "    ax.set_title(f'Clusters for $k$ = {k_value}', fontsize=12)\n",
        "\n",
        "    # make padding look nice\n",
        "    plt.tight_layout();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTljDdLfqyrf"
      },
      "source": [
        "### Exercises:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oQSAxLLqyrf"
      },
      "source": [
        "We recommend you to try the following exercises as part of the lab:\n",
        "\n",
        "**1. Apply the EM Clustering algorithm to all four features contained in the Iris dataset.**\n",
        "\n",
        "> Use the EM classifier to learn a model of all four features contained in the Iris dataset (that we used in the prior lab on k-Means Clustering), namely `Sepal length (cm)`, `Sepal width (cm)`, `Petal length (cm)` and `Petal width (cm)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZKpSnLyqyrf"
      },
      "source": [
        "# ***************************************************\n",
        "# INSERT YOUR CODE HERE\n",
        "# ***************************************************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpWvrlHzqyri"
      },
      "source": [
        "**2. Determine the optimal number of cluster values $k$ of all four features contained in the iris dataset.**\n",
        "\n",
        "> Determine the optimal number of clusters $k$ needed to cluster the observations of all four features contained in the iris dataset using the **'Bayesian Information Criteria (BIC)'** technique described in the lecture. (Hint: Have a look at the `BIC` method explained in the `sklearn` documentation of the **EM Clustering** algorithm)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brokF6x-qyrj"
      },
      "source": [
        "# ***************************************************\n",
        "# INSERT YOUR CODE HERE\n",
        "# ***************************************************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7GiXw3vqyrx"
      },
      "source": [
        "### Lab Summary:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB_ThM4uqyry"
      },
      "source": [
        "In this lab, a step by step introduction into the unsupervised **EM-Clustering** algorithm was presented. The code and exercises presented in this lab may serve as a starting point for more complex and tailored programs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKT2FXCBqyry"
      },
      "source": [
        "You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Pls. note that to convert the notebook, you need to install Python's **nbconvert** library and its extensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqxeIXqFqyrz"
      },
      "source": [
        "# installing the nbconvert library\n",
        "!pip3 install nbconvert\n",
        "!pip3 install jupyter_contrib_nbextensions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT0Gxmu6qyr4"
      },
      "source": [
        "Let's now convert the Jupyter notebook into a plain Python script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rozJNHwKqyr5"
      },
      "source": [
        "!jupyter nbconvert --to script aiml_colab_04b.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i7IDL4Y481Y"
      },
      "source": [
        "### References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypMszBzG4-1q"
      },
      "source": [
        ">- BIC: https://machinelearningmastery.com/probabilistic-model-selection-measures/"
      ]
    }
  ]
}