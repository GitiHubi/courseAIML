{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"hsg_logo.png\">\n",
    "\n",
    "###  Lab 06 - \"Deep Learning - Convolutional Neural Networks\"\n",
    "\n",
    "Introduction to AI and ML, University of St. Gallen, Spring Term 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lab you learned about how to utilize a **supervised** (deep) machine learning technique namely **Artificial Neural Networks (ANNs)** to classify tiny images of handwritten digits contained in the MNIST dataset. \n",
    "\n",
    "In this lab, we will learn how to enhance ANNs using PyTorch to classify even more complex images. Therefore, we use a special type of deep neural network referred to **Convolutional Neural Networks (CNNs)**. CNNs encompass the ability to take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, CNNs are capable to learn a set of discriminative features 'pattern' and subsequently utilize the learned pattern to classify the content of an image.\n",
    "\n",
    "We will again use the functionality of the `PyTorch` library to implement and train an CNN based neural network. The network will be trained on a set of tiny images to learn a model of the image content. Upon successful training, we will utilize the learned CNN model to classify so far unseen tiny images into distinct categories such as aeroplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. \n",
    "\n",
    "The figure below illustrates a high-level view on the machine learning process we aim to establish in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"classification.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Image of the CNN architecture created via http://alexlenail.me/)\n",
    "\n",
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After today's lab, you should be able to:\n",
    "\n",
    "> 1. Understand the basic concepts, intuitions and major building blocks of **Convolutional Neural Networks (CNNs)**.\n",
    "> 2. Know how to **implement and to train a CNN** to learn a model of tiny image data.\n",
    "> 3. Understand how to apply such a learned model to **classify images** images based on their content into distinct categories.\n",
    "> 4. Know how to **interpret and visualize** the model's classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the Jupyter Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. We will mostly use the `PyTorch`, `Numpy`, `Sklearn`, `Matplotlib`, `Seaborn` and a few utility libraries throughout this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard python libraries\n",
    "import os, urllib, io\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python machine / deep learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the PyTorch deep learning library\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the sklearn classification metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn classification evaluation library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python plotting libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib, seaborn, and PIL data visualization libary\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable notebook matplotlib inline plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create notebook folder structure to store the data as well as the trained neural network models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data'): os.makedirs('./data')  # create data directory\n",
    "if not os.path.exists('./models'): os.makedirs('./models')  # create trained models directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random `seed` value to obtain reproducable results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x107391f30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable GPU computing by setting the `device` flag and init a `CUDA` seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] notebook with cpu computation enabled\n"
     ]
    }
   ],
   "source": [
    "# set cpu or gpu enabled device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "\n",
    "# init deterministic GPU seed\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "# log type of device enabled\n",
    "print('[LOG] notebook with {} computation enabled'.format(str(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine if we have access to a GPU provided by e.g. Google's COLab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CIFAR-10 database** (**C**anadian **I**nstitute **F**or **A**dvanced **R**esearch) is a collection of images that are commonly used to train machine learning and computer vision algorithms. The database is widely used to conduct computer vision research using machine learning and deep learning methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: 500px\" src=\"cifar10.png\">\n",
    "\n",
    "(Source: https://www.kaggle.com/c/cifar-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further details on the dataset can be obtained via: *Krizhevsky, A., 2009. \"Learning Multiple Layers of Features from Tiny Images\",  \n",
    "( https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf ).\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 database contains **60,000 color images** (50,000 training images and 10,000 validation images). The size of each image is 32 by 32 pixels. The collection of images encompasses 10 different classes that represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Let's define the distinct classs for further analytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereby the dataset contains 6,000 images for each of the ten classes. The CIFAR-10 is a straightforward dataset that can be used to teach a computer how to recognize objects in images.\n",
    "\n",
    "Let's download, transform and inspect the training images of the dataset. Therefore, we first will define the directory we aim to store the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train_cifar10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download the training data accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# define pytorch transformation into tensor format\n",
    "transf = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# download and transform training images\n",
    "cifar10_train_data = torchvision.datasets.CIFAR10(root=train_path, train=True, transform=transf, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the volume of training images downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the length of the training data\n",
    "len(cifar10_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's investigate a couple of the training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0118, -0.0039,  0.0118,  ...,  0.0353,  0.0353,  0.0275],\n",
       "          [ 0.0039,  0.0039,  0.0118,  ...,  0.0275,  0.0275,  0.0196],\n",
       "          [ 0.0196,  0.0039,  0.0196,  ...,  0.0431,  0.0353,  0.0353],\n",
       "          ...,\n",
       "          [-0.2000, -0.2627, -0.4745,  ...,  0.0510,  0.0353,  0.0275],\n",
       "          [-0.1529, -0.2471, -0.3725,  ...,  0.0431,  0.0275,  0.0196],\n",
       "          [-0.1373, -0.3725, -0.4902,  ...,  0.0275,  0.0196,  0.0118]],\n",
       " \n",
       "         [[ 0.1922,  0.1765,  0.1922,  ...,  0.2000,  0.2000,  0.1922],\n",
       "          [ 0.1922,  0.1843,  0.1922,  ...,  0.1922,  0.1922,  0.1843],\n",
       "          [ 0.2000,  0.1843,  0.2000,  ...,  0.2078,  0.2000,  0.2000],\n",
       "          ...,\n",
       "          [-0.1137, -0.1843, -0.4667,  ...,  0.2314,  0.2157,  0.2078],\n",
       "          [-0.0667, -0.1451, -0.3569,  ...,  0.2235,  0.2078,  0.2000],\n",
       "          [-0.0588, -0.2471, -0.4588,  ...,  0.2078,  0.2000,  0.1922]],\n",
       " \n",
       "         [[ 0.5451,  0.5216,  0.5373,  ...,  0.5529,  0.5529,  0.5451],\n",
       "          [ 0.5373,  0.5216,  0.5373,  ...,  0.5451,  0.5451,  0.5373],\n",
       "          [ 0.5451,  0.5294,  0.5451,  ...,  0.5608,  0.5529,  0.5529],\n",
       "          ...,\n",
       "          [-0.6863, -0.6235, -0.7020,  ...,  0.5765,  0.5608,  0.5529],\n",
       "          [-0.6314, -0.5765, -0.5922,  ...,  0.5686,  0.5529,  0.5451],\n",
       "          [-0.5608, -0.6627, -0.6941,  ...,  0.5529,  0.5451,  0.5373]]]), 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set (random) image id\n",
    "image_id = 1800\n",
    "\n",
    "# retrieve image exhibiting the image id\n",
    "cifar10_train_data[image_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that doesn't seem easily interpretable ;) Let's first seperate the image from its label information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train_image, cifar10_train_label = cifar10_train_data[image_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we are able to visually inspect our sample image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13632fe10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define tensor to image transformation\n",
    "trans = torchvision.transforms.ToPILImage()\n",
    "\n",
    "# set image plot title \n",
    "plt.title('Example: {}, Label: \"{}\"'.format(str(image_id), str(cifar10_classes[cifar10_train_label])))\n",
    "\n",
    "# un-normalize cifar 10 image sample\n",
    "cifar10_train_image_plot = cifar10_train_image / 2.0 + 0.5\n",
    "\n",
    "# plot 10 image sample\n",
    "plt.imshow(trans(cifar10_train_image_plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic, right? Let's now decide on where we want to store the evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = './data/eval_cifar10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And download the evaluation data accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# define pytorch transformation into tensor format\n",
    "transf = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# download and transform validation images\n",
    "cifar10_eval_data = torchvision.datasets.CIFAR10(root=eval_path, train=False, transform=transf, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the volume of validation images downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the length of the training data\n",
    "len(cifar10_eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we, will implement the architecture of the **neural network** we aim to utilize to learn a model that is capable of classifying the 32x32 pixel CIFAR 10 images according to the objects contained in each image. However, before we start the implementation, let's briefly revisit the process to be established. The following cartoon provides a birds-eye view:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our CNN, which we name 'CIFAR10Net' and aim to implement consists of two **convolutional layers** and three **fully-connected layers**. In general, convolutional layers are specifically designed to learn a set of **high-level features** (\"patterns\") in the processed images, e.g., tiny edges and shapes. The fully-connected layers utilize the learned features to learn **non-linear feature combinations** that allow for highly accurate classification of the image content into the different image classes of the CIFAR-10 dataset, such as, birds, aeroplanes, horses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the network architecture and subsequently have a more in-depth look into its architectural details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the CIFAR10Net network architecture\n",
    "class CIFAR10Net(nn.Module):\n",
    "    \n",
    "    # define the class constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        # call super class constructor\n",
    "        super(CIFAR10Net, self).__init__()\n",
    "        \n",
    "        # specify convolution layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # define max-pooling layer 1\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # specify convolution layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # define max-pooling layer 2\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # specify fc layer 1 - in 16 * 5 * 5, out 120\n",
    "        self.linear1 = nn.Linear(16 * 5 * 5, 120, bias=True) # the linearity W*x+b\n",
    "        self.relu1 = nn.ReLU(inplace=True) # the non-linearity\n",
    "        \n",
    "        # specify fc layer 2 - in 120, out 84\n",
    "        self.linear2 = nn.Linear(120, 84, bias=True) # the linearity W*x+b\n",
    "        self.relu2 = nn.ReLU(inplace=True) # the non-linarity\n",
    "        \n",
    "        # specify fc layer 3 - in 84, out 10\n",
    "        self.linear3 = nn.Linear(84, 10) # the linearity W*x+b\n",
    "        \n",
    "        # add a softmax to the last layer\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1) # the softmax\n",
    "        \n",
    "    # define network forward pass\n",
    "    def forward(self, images):\n",
    "        \n",
    "        # high-level feature learning via convolutional layers\n",
    "        \n",
    "        # define conv layer 1 forward pass\n",
    "        x = self.pool1(self.relu1(self.conv1(images)))\n",
    "        \n",
    "        # define conv layer 2 forward pass\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        \n",
    "        # feature flattening\n",
    "        \n",
    "        # reshape image pixels\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        \n",
    "        # combination of feature learning via non-linear layers\n",
    "        \n",
    "        # define fc layer 1 forward pass\n",
    "        x = self.relu1(self.linear1(x))\n",
    "        \n",
    "        # define fc layer 2 forward pass\n",
    "        x = self.relu2(self.linear2(x))\n",
    "        \n",
    "        # define layer 3 forward pass\n",
    "        x = self.logsoftmax(self.linear3(x))\n",
    "        \n",
    "        # return forward pass result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that we applied two more layers (compared to the MNIST example described in the last lab) before the fully-connected layers. These layers are referred to as **convolutional** layers and are usually comprised of three operations, (1) **convolution**, (2) **non-linearity**, and (3) **max-pooling**. Those operations are usually executed in sequential order during the forward pass through a convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will have a detailed look into the functionality and number of parameters in each layer. We will start with providing images of 3x32x32 dimensions to the network, i.e., the three channels (red, green, blue) of an image each of size 32x32 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. High-Level Feature Learning by Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look into the convolutional layers of the network as illustrated in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 600px\" src=\"convolutions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Convolutional Layer**: The first convolutional layer expects three input channels and will convolve six filters each of size 3x5x5. Let's briefly revisit how we can perform a convolutional operation on a given image. For that, we need to define a kernel which is a matrix of size 5x5, for example. To perform the convolution operation, we slide the kernel along with the image horizontally and vertically and obtain the dot product of the kernel and the pixel values of the image inside the kernel ('receptive field' of the kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following illustration shows an example of a discrete convolution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 800px\" src=\"convsample.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left grid is called the input (an image or feature map). The middle grid, referred to as kernel, slides across the input feature map (or image). At each location, the product between each element of the kernel and the input element it overlaps is computed, and the results are summed up to obtain the output in the current location. In general, a discrete convolution is mathematically expressed by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $y(m, n) = x(m, n) * h(m, n) = \\sum^{m}_{j=0} \\sum^{n}_{i=0} x(i, j) * h(m-i, n-j)$, </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $x$ denotes the input image or feature map, $h$ the applied kernel, and, $y$ the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing the convolution operation the 'stride' defines the number of pixels to pass at a time when sliding the kernel over the input. While 'padding' adds the number of pixels to the input image (or feature map) to ensure that the output has the same shape as the input. Let's have a look at another animated example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 800px\" src=\"convsample_animated.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Source: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "\n",
    "In our implementation padding is set to 0 and stride is set to 1. As a result, the output size of the convolutional layer becomes 6x28x28, because (32 - 5) + 1 = 28. This layer exhibits ((5 x 5 x 3) + 1) x 6 = 456 parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Max-Pooling Layer:** The max-pooling process is a sample-based discretization operation. The objective is to down-sample an input representation (image, hidden-layer output matrix, etc.), reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned.\n",
    "\n",
    "To conduct such an operation, we again need to define a kernel. Max-pooling kernels are usually a tiny matrix of, e.g, of size 2x2. To perform the max-pooling operation, we slide the kernel along the image horizontally and vertically (similarly to a convolution) and compute the maximum pixel value of the image (or feature map) inside the kernel (the receptive field of the kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following illustration shows an example of a max-pooling operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px\" src=\"poolsample.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left grid is called the input (an image or feature map). The middle grid, referred to as kernel, slides across the input feature map (or image). We use a stride of 2, meaning the step distance for stepping over our input will be 2 pixels and won't overlap regions. At each location, the max value of the region that overlaps with the elements of the kernel and the input elements it overlaps is computed, and the results are obtained in the output of the current location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation, we do max-pooling with a 2x2 kernel and stride 2 this effectively drops the original image size from 6x28x28 to 6x14x14. Let's have a look at an exemplary visualization of 64 features learnt in the first convolutional layer on the CIFAR- 10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 600px\" src=\"cnnfeatures.png\">\n",
    "\n",
    "(Source: Yu, Dingjun, Hanli Wang, Peiqiu Chen, and Zhihua Wei. **\"Mixed pooling for convolutional neural networks.\"** In International conference on rough sets and knowledge technology, pp. 364-375. Springer, Cham, 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Convolutional Layer:** The second convolutional layer expects 6 input channels and will convolve 16 filters each of size 6x5x5x. Since padding is set to 0 and stride is set 1, the output size is 16x10x10, because (14  - 5) + 1 = 10. This layer therefore has ((5 x 5 x 6) + 1 x 16) = 24,16 parameter.\n",
    "\n",
    "**Second Max-Pooling Layer:** The second down-sampling layer uses max-pooling with 2x2 kernel and stride set to 2. This effectively drops the size from 16x10x10 to 16x5x5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Feature Flattening\n",
    "\n",
    "The output of the final-max pooling layer needs to be flattened so that we can connect it to a fully connected layer. This is achieved using the `torch.Tensor.view` method. Setting the parameter of the method to `-1` will automatically infer the number of rows required to handle the mini-batch size of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Learning of Feature Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look into the non-linear layers of the network illustrated in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 600px\" src=\"fullyconnected.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first fully connected layer uses 'Rectified Linear Units' (ReLU) activation functions to learn potential nonlinear combinations of features. The layers are implemented similarly to the fifth lab. Therefore, we will only focus on the number of parameters of each fully-connected layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Fully-Connected Layer:** The first fully-connected layer consists of 120 neurons, thus in total exhibits ((16 x 5 x 5) + 1) x 120 = 48,120 parameter. \n",
    "\n",
    "**Second Fully-Connected Layer:** The output of the first fully-connected layer is then transferred to second fully-connected layer. The layer consists of 84 neurons equipped with ReLu activation functions, this in total exhibits (120 + 1) x 84 = 10,164 parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the second fully-connected layer is then transferred to the output-layer (third fully-connected layer). The output layer is equipped with a softmax (that you learned about in the previous lab 05) and is made up of ten neurons, one for each object class contained in the CIFAR-10 dataset. This layer exhibits (84 + 1) x 10 = 850 parameter.\n",
    "\n",
    "\n",
    "As a result our CIFAR-10 convolutional neural exhibits a total of 456 + 2,416 + 48,120 + 10,164 + 850 = 62,006 parameter.\n",
    "\n",
    "(Source: https://www.stefanfiott.com/machine-learning/cifar-10-classifier-using-cnn-in-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have implemented our first Convolutional Neural Network we are ready to instantiate a network model to be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CIFAR10Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's push the initialized `CIFAR10Net` model to the computing `device` that is enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check if our model was deployed to the GPU if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is initialized we can visualize the model structure and review the implemented network architecture by execution of the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] CIFAR10Net architecture:\n",
      "\n",
      "CIFAR10Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (linear2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (linear3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (logsoftmax): LogSoftmax()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "print('[LOG] CIFAR10Net architecture:\\n\\n{}\\n'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like intended? Brilliant! Finally, let's have a look into the number of model parameters that we aim to train in the next steps of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Number of to be trained CIFAR10Net model parameters: 62006.\n"
     ]
    }
   ],
   "source": [
    "# init the number of model parameters\n",
    "num_params = 0\n",
    "\n",
    "# iterate over the distinct parameters\n",
    "for param in model.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    num_params += param.numel()\n",
    "    \n",
    "# print the number of model paramters\n",
    "print('[LOG] Number of to be trained CIFAR10Net model parameters: {}.'.format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, our \"simple\" CIFAR10Net model already encompasses an impressive number 62'006 model parameters to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented the CIFAR10Net, we are ready to train the network. However, before starting the training, we need to define an appropriate loss function. Remember, we aim to train our model to learn a set of model parameters $\\theta$ that minimize the classification error of the true class $c^{i}$ of a given CIFAR-10 image $x^{i}$ and its predicted class $\\hat{c}^{i} = f_\\theta(x^{i})$ as faithfully as possible. \n",
    "\n",
    "In this lab we use (similarly to lab 05) the **'Negative Log-Likelihood (NLL)'** loss. During training the NLL loss will penalize models that result in a high classification error between the predicted class labels $\\hat{c}^{i}$ and their respective true class label $c^{i}$. Now that we have implemented the CIFAR10Net, we are ready to train the network. Before starting the training, we need to define an appropriate loss function. Remember, we aim to train our model to learn a set of model parameters $\\theta$ that minimize the classification error of the true class $c^{i}$ of a given CIFAR-10 image $x^{i}$ and its predicted class $\\hat{c}^{i} = f_\\theta(x^{i})$ as faithfully as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the NLL via the execution of the following PyTorch command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization criterion / loss function\n",
    "nll_loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also push the initialized `nll_loss` computation to the computing `device` that is enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_loss = nll_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the loss magnitude of a certain mini-batch PyTorch automatically computes the gradients. But even better, based on the gradient, the library also helps us in the optimization and update of the network parameters $\\theta$.\n",
    "\n",
    "We will use the **Stochastic Gradient Descent (SGD) optimization** and set the `learning-rate to 0.001`. Each mini-batch step the optimizer will update the model parameters $\\theta$ values according to the degree of classification error (the NLL loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define learning rate and optimization strategy\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully implemented and defined the three CNN building blocks let's take some time to review the `CIFAR10Net` model definition as well as the `loss`. Please, read the above code and comments carefully and don't hesitate to let us know any questions you might have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train our neural network model (as implemented in the section above) using the transformed images. More specifically, we will have a detailed look into the distinct training steps as well as how to monitor the training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Preparing the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have pre-processed the dataset, implemented the CNN and defined the classification error. Let's now start to train a corresponding model for **20 epochs** and a **mini-batch size of 128** CIFAR-10 images per batch. This implies that the whole dataset will be fed to the CNN 20 times in chunks of 4 images yielding to **12,500 mini-batches** (50.000 training images / 4 images per mini-batch) per epoch. After the processing of each mini-batch, the parameters of the network will be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the training parameters\n",
    "num_epochs = 20 # number of training epochs\n",
    "mini_batch_size = 4 # size of the mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, lets specifiy and instantiate a corresponding PyTorch data loader that feeds the image tensors to our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train_dataloader = torch.utils.data.DataLoader(cifar10_train_data, batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Running the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start training the model. The training procedure for each mini-batch is performed as follows: \n",
    "\n",
    ">1. do a forward pass through the CIFAR10Net network, \n",
    ">2. compute the negative log-likelihood classification error $\\mathcal{L}^{NLL}_{\\theta}(c^{i};\\hat{c}^{i})$, \n",
    ">3. do a backward pass through the CIFAR10Net network, and \n",
    ">4. update the parameters of the network $f_\\theta(\\cdot)$.\n",
    "\n",
    "To ensure learning while training our CNN model, we will monitor whether the loss decreases with progressing training. Therefore, we obtain and evaluate the classification performance of the entire training dataset after each training epoch. Based on this evaluation, we can conclude on the training progress and whether the loss is converging (indicating that the model might not improve any further).\n",
    "\n",
    "The following elements of the network training code below should be given particular attention:\n",
    " \n",
    ">- `loss.backward()` computes the gradients based on the magnitude of the reconstruction loss,\n",
    ">- `optimizer.step()` updates the network parameters based on the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:03:33] epoch: 0 train-loss: 2.238219862680435\n",
      "[LOG 20200511-10:04:12] epoch: 1 train-loss: 1.935777970509529\n",
      "[LOG 20200511-10:04:45] epoch: 2 train-loss: 1.684436317577362\n",
      "[LOG 20200511-10:05:17] epoch: 3 train-loss: 1.5414427460861206\n",
      "[LOG 20200511-10:05:47] epoch: 4 train-loss: 1.4574027087664605\n",
      "[LOG 20200511-10:06:17] epoch: 5 train-loss: 1.3850340257668494\n",
      "[LOG 20200511-10:06:48] epoch: 6 train-loss: 1.3218666888189317\n",
      "[LOG 20200511-10:07:17] epoch: 7 train-loss: 1.265575843114853\n",
      "[LOG 20200511-10:07:47] epoch: 8 train-loss: 1.2157743350499868\n",
      "[LOG 20200511-10:08:15] epoch: 9 train-loss: 1.1745015937310457\n",
      "[LOG 20200511-10:08:42] epoch: 10 train-loss: 1.135681571468711\n",
      "[LOG 20200511-10:09:12] epoch: 11 train-loss: 1.0998355753684044\n",
      "[LOG 20200511-10:09:39] epoch: 12 train-loss: 1.0676548833155632\n",
      "[LOG 20200511-10:10:09] epoch: 13 train-loss: 1.0361538963371515\n",
      "[LOG 20200511-10:10:40] epoch: 14 train-loss: 1.0077817464274168\n",
      "[LOG 20200511-10:11:10] epoch: 15 train-loss: 0.9818167420062422\n",
      "[LOG 20200511-10:11:40] epoch: 16 train-loss: 0.9577325039881468\n",
      "[LOG 20200511-10:12:11] epoch: 17 train-loss: 0.9335133895949275\n",
      "[LOG 20200511-10:12:44] epoch: 18 train-loss: 0.9106277276663483\n",
      "[LOG 20200511-10:13:16] epoch: 19 train-loss: 0.8896594128757715\n"
     ]
    }
   ],
   "source": [
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set the model in training mode\n",
    "model.train()\n",
    "\n",
    "# train the CIFAR10 model\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # init collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "    \n",
    "    # iterate over all-mini batches\n",
    "    for i, (images, labels) in enumerate(cifar10_train_dataloader):\n",
    "        \n",
    "        # push mini-batch data to computation device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # run forward pass through the network\n",
    "        output = model(images)\n",
    "        \n",
    "        # reset graph gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # determine classification loss\n",
    "        loss = nll_loss(output, labels)\n",
    "        \n",
    "        # run backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network paramaters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # collect mini-batch reconstruction loss\n",
    "        train_mini_batch_losses.append(loss.data.item())\n",
    "\n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "    \n",
    "    # print epoch loss\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] epoch: {} train-loss: {}'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "    \n",
    "    # save model to local directory\n",
    "    model_name = 'cifar10_model_epoch_{}.pth'.format(str(epoch))\n",
    "    torch.save(model.state_dict(), os.path.join(\"./models\", model_name))\n",
    "    \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successfull training let's visualize and inspect the training loss per epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' classification error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
    "ax.set_ylabel(\"[Classification Error $\\mathcal{L}^{NLL}$]\", fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Epochs $e_i$ vs. Classification Error $L^{NLL}$', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, fantastic. The training error converges nicely. We could definitely train the network a couple more epochs until the error converges. But let's stay with the 20 training epochs for now and continue with evaluating our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation of the Trained Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to evaluating our model, let's load the best performing model. Remember, that we stored a snapshot of the model after each training epoch to our local model directory. We will now load the last snapshot saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restore pre-trained model snapshot\n",
    "best_model_name = \"cifar10_model_epoch_19.pth\"\n",
    "\n",
    "# init pre-trained model class\n",
    "best_model = CIFAR10Net()\n",
    "\n",
    "# load pre-trained models\n",
    "best_model.load_state_dict(torch.load(os.path.join(\"models\", best_model_name), map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect if the model was loaded successfully: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAR10Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (linear2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (linear3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (logsoftmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model in evaluation mode\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our trained model, we need to feed the CIFAR10 images reserved for evaluation (the images that we didn't use as part of the training process) through the model. Therefore, let's again define a corresponding PyTorch data loader that feeds the image tensors to our neural network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_eval_dataloader = torch.utils.data.DataLoader(cifar10_eval_data, batch_size=10000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now evaluate the trained model using the same mini-batch approach as we did when training the network and derive the mean negative log-likelihood loss of all mini-batches processed in an epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:13:20] eval-loss: 1.0348503589630127\n"
     ]
    }
   ],
   "source": [
    "# init collection of mini-batch losses\n",
    "eval_mini_batch_losses = []\n",
    "\n",
    "# iterate over all-mini batches\n",
    "for i, (images, labels) in enumerate(cifar10_eval_dataloader):\n",
    "\n",
    "    # run forward pass through the network\n",
    "    output = best_model(images)\n",
    "\n",
    "    # determine classification loss\n",
    "    loss = nll_loss(output, labels)\n",
    "\n",
    "    # collect mini-batch reconstruction loss\n",
    "    eval_mini_batch_losses.append(loss.data.item())\n",
    "\n",
    "# determine mean min-batch loss of epoch\n",
    "eval_loss = np.mean(eval_mini_batch_losses)\n",
    "\n",
    "# print epoch loss\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] eval-loss: {}'.format(str(now), str(eval_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, great. The evaluation loss looks in-line with our training loss. Let's now inspect a few sample predictions to get an impression of the model quality. Therefore, we will again pick a random image of our evaluation dataset and retrieve its PyTorch tensor as well as the corresponding label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set (random) image id\n",
    "image_id = 777\n",
    "\n",
    "# retrieve image exhibiting the image id\n",
    "cifar10_eval_image, cifar10_eval_label = cifar10_eval_data[image_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the true class of the image we selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_classes[cifar10_eval_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the randomly selected image should contain a two (2). Let's inspect the image accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x146cb1350>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define tensor to image transformation\n",
    "trans = torchvision.transforms.ToPILImage()\n",
    "\n",
    "# set image plot title \n",
    "plt.title('Example: {}, Label: {}'.format(str(image_id), str(cifar10_classes[cifar10_eval_label])))\n",
    "\n",
    "# un-normalize cifar 10 image sample\n",
    "cifar10_eval_image_plot = cifar10_eval_image / 2.0 + 0.5\n",
    "\n",
    "# plot cifar 10 image sample\n",
    "plt.imshow(trans(cifar10_eval_image_plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's compare the true label with the prediction of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8.7498, -11.1383,  -3.6063,  -1.7705,  -3.3294,  -2.5025,  -0.3873,\n",
       "          -5.1666,  -9.0390, -10.6598]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model(cifar10_eval_image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even determine the likelihood of the most probable class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_classes[torch.argmax(best_model(Variable(cifar10_eval_image.unsqueeze(0))), dim=1).item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now obtain the predictions for all the CIFAR-10 images of the evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(best_model(iter(cifar10_eval_dataloader).next()[0]), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's obtain the overall classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6437"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(cifar10_eval_data.targets, predictions.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also inspect the confusion matrix of the model predictions to determine major sources of misclassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# determine classification matrix of the predicted and target classes\n",
    "mat = confusion_matrix(cifar10_eval_data.targets, predictions.detach())\n",
    "\n",
    "# plot corresponding confusion matrix\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='YlOrRd_r', xticklabels=cifar10_classes, yticklabels=cifar10_classes)\n",
    "plt.title('CIFAR-10 classification matrix')\n",
    "plt.xlabel('[true label]')\n",
    "plt.ylabel('[predicted label]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we can easily see that our current model confuses images of cats and dogs as well as images of trucks and cars quite often. This is again not surprising since those image categories exhibit a high semantic and therefore visual similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you try the following exercises as part of the lab:\n",
    "\n",
    "**1. Train the network a couple more epochs and evaluate its prediction accuracy.**\n",
    "\n",
    "> Increase the number of training epochs up to 50 epochs and re-run the network training. Load and evaluate the model exhibiting the lowest training loss. What kind of behavior in terms of prediction accuracy can be observed with increasing the training epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Evaluation of \"shallow\" vs. \"deep\" neural network architectures.**\n",
    "\n",
    "> In addition to the architecture of the lab notebook, evaluate further (more shallow as well as more deep) neural network architectures by (1) either removing or adding layers to the network and/or (2) increasing/decreasing the number of neurons per layer. Train a model (using the architectures you selected) for at least 50 training epochs. Analyze the prediction performance of the trained models in terms of training time and prediction accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. [Bonus]: Deep Residual Learning 'ResNet' Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks have led to a series of breakthroughs for image classification. In general, deep networks naturally integrate low/mid/high-level features and classifiers in an end-to-end multilayer fashion, and the 'levels' of features can be enriched by the depth (or number of stacked layers) of the network. Driven by the significance of depth, the question arises: **Will stacking more layers improve feature learning and therefore increase the classification capability of a model?**\n",
    "\n",
    "In 2015, in their work **'Deep Residual Learning for Image Recognition'** He et al. (https://arxiv.org/abs/1512.03385) proposed an enhanced deep convolutional neural network architecture. The architecture is referred to as **Residual Neural Networks** since it encompasses so called **Residual Layers** or **Residual Blocks**. Using the architecture He et al. demonstrated that they were able to outperform a variety of image classification benchmark challanges at the time. We have a closer look into the distinct characteristic of the architecture on the following section of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Implementation of the \"ResNet\" Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a **residual block** simply denotes that the activation of a layer can be fast-forwarded to a deeper layer in the neural network. As you can observe in the image below, the activation from a previous layer is being added to the final activation of a deeper layer in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 400px\" src=\"residualblock.png\">\n",
    "\n",
    "(Source: 'Deep Residual Learning for Image Recognition', He, K., Zhang, X., Ren, S., and, Sun, J., 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement such a **Residual Block**, as shown above, using `PyTorch`library: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement ResNet residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # init first convolutional layer of residual block\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels) # batch-normalization\n",
    "        self.relu1 = nn.ReLU(inplace=True) # non-linearity\n",
    "        \n",
    "        # init second convolutional layer of residual block\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels) # batch-normalization\n",
    "        self.relu2 = nn.ReLU(inplace=True) # non-linearity\n",
    "        \n",
    "        # init down-sample flag\n",
    "        self.downsample = downsample\n",
    "\n",
    "    # define the block forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # determine residual\n",
    "        residual = x\n",
    "        \n",
    "        # case: down-sampling needed\n",
    "        if self.downsample:\n",
    "            \n",
    "            # determine down-sampled residual\n",
    "            residual = self.downsample(residual)\n",
    "\n",
    "        # run forward pass through first layer\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # run forward pass through second layer\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # merge output and residual\n",
    "        # the skip connection :) \n",
    "        out += residual\n",
    "        \n",
    "        # run second non-linearity\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # return residual block output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a `ResNet` network architecture and subsequently have a more in-depth look into its architectural details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the ResNet architecture\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        # call super class constructor\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        #### feature learning layers\n",
    "\n",
    "        # init initial convolutional layer\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(16) # batch-normalization\n",
    "        self.relu = nn.ReLU(inplace=True) # non-linearity\n",
    "\n",
    "        # init first residual layer \n",
    "        self.layer1 = self.make_residual_layer(in_channels=16, out_channels=16, blocks=layers[0], stride=1)\n",
    "        \n",
    "        # init second residual layer \n",
    "        self.layer2 = self.make_residual_layer(in_channels=16, out_channels=32, blocks=layers[1], stride=1)\n",
    "        \n",
    "        # init third residual layer \n",
    "        self.layer3 = self.make_residual_layer(in_channels=32, out_channels=64, blocks=layers[2], stride=1)\n",
    "\n",
    "        # init average pooling\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "\n",
    "        #### feature classification layers\n",
    "        \n",
    "        # define fully connected layer \n",
    "        self.fc = nn.Linear(1024, 10)\n",
    "\n",
    "        # define log-softmax probability conversion\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # create residual layer \n",
    "    def make_residual_layer(self, in_channels, out_channels, blocks, stride=1):\n",
    "\n",
    "        # init down-sample flag\n",
    "        downsample = None\n",
    "\n",
    "        # init array of residual layer elements \n",
    "        layers = []\n",
    "        \n",
    "        # case: down sampling needed\n",
    "        if in_channels != out_channels:\n",
    "\n",
    "            # init down-sampling layer \n",
    "            downsample = nn.Sequential(\n",
    "                \n",
    "                # init down-sampling convolution\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                \n",
    "                # init down-sampling batch normalization \n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        # init and append initial residual block\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n",
    "\n",
    "        # reset input channels\n",
    "        self.in_channels = out_channels\n",
    "\n",
    "        # iterate over remaining residual blocks\n",
    "        for i in range(1, blocks):\n",
    "\n",
    "            # init and append remaining residual blocks\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "\n",
    "        # stack layers \n",
    "        residual_layer = nn.Sequential(*layers)\n",
    "        \n",
    "        # return residual layer \n",
    "        return residual_layer\n",
    "\n",
    "    # define the network forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # run through inital convolution\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # run through initial batch-normalization\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        # run trough first residual block\n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        # run through second residual block\n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        # run through third residual block\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        # conduct average pooling of learned features\n",
    "        x = self.avg_pool(x)\n",
    "        \n",
    "        # reshape the feature map\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # run final fully-connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # run final log-softmax\n",
    "        x = self.logsoftmax(x)\n",
    "\n",
    "        # return forward pass result\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have implemented our `ResNet` architecture we are ready to instantiate a network model to be trained. In the following we instantiate a `ResNet` where each residual layer consists of two residual blocks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResNet(layers=[2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's push the initialized `ResNet` model to the computing `device` that is enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = resnet_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check if our model was deployed to the GPU if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is initialized we can visualize the model structure and review the implemented network architecture by execution of the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] ResNet architecture:\n",
      "\n",
      "ResNet(\n",
      "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  (logsoftmax): LogSoftmax()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "print('[LOG] ResNet architecture:\\n\\n{}\\n'.format(resnet_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like intended? Brilliant! Finally, let's have a look into the number of model parameters that we aim to train in the next steps of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Number of to be trained ResNet model parameters: 205338.\n"
     ]
    }
   ],
   "source": [
    "# init the number of model parameters\n",
    "num_params = 0\n",
    "\n",
    "# iterate over the distinct parameters\n",
    "for param in resnet_model.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    num_params += param.numel()\n",
    "    \n",
    "# print the number of model paramters\n",
    "print('[LOG] Number of to be trained ResNet model parameters: {}.'.format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, our `ResNet` model encompasses a total of 205'338 model parameters to be trained. That's quite an increase in comparison to the prior `CIFAR10Net` that corresponded to 62'006 parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Training of the \"ResNet\" Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to training the `CIFAR10Net`, we will use the **Stochastic Gradient Descent (SGD) optimization** and set the `learning-rate to 0.001`. Each mini-batch step the optimizer will update the model parameters $\\theta$ values according to the degree of classification error (the NLL loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define learning rate and optimization strategy\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(params=resnet_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now start to train a `ResNet`corresponding model for **20 epochs** and a **mini-batch size of 128** CIFAR-10 images per batch. This implies that the whole dataset will be fed to the architecture 20 times in chunks of 128 images yielding to **391 mini-batches** (50.000 training images / 128 images per mini-batch) per epoch. After the processing of each mini-batch, the parameters of the network will be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the training parameters\n",
    "num_epochs = 1 # 20 # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's conduct the model training accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:13:24] epoch: 0, batch: 0 train-loss: 2.3967509269714355\n",
      "[LOG 20200511-10:13:24] epoch: 0, batch: 1 train-loss: 2.2354273796081543\n",
      "[LOG 20200511-10:13:25] epoch: 0, batch: 2 train-loss: 2.248624801635742\n",
      "[LOG 20200511-10:13:25] epoch: 0, batch: 3 train-loss: 2.09393572807312\n",
      "[LOG 20200511-10:13:25] epoch: 0, batch: 4 train-loss: 2.1641650199890137\n",
      "[LOG 20200511-10:13:25] epoch: 0, batch: 5 train-loss: 2.589729070663452\n",
      "[LOG 20200511-10:13:25] epoch: 0, batch: 6 train-loss: 2.5136914253234863\n",
      "[LOG 20200511-10:13:25] epoch: 0, batch: 7 train-loss: 2.273439407348633\n",
      "[LOG 20200511-10:13:25] epoch: 0, batch: 8 train-loss: 2.1926562786102295\n",
      "[LOG 20200511-10:13:26] epoch: 0, batch: 9 train-loss: 2.4222898483276367\n",
      "[LOG 20200511-10:13:26] epoch: 0, batch: 10 train-loss: 2.6205220222473145\n",
      "[LOG 20200511-10:13:26] epoch: 0, batch: 11 train-loss: 2.57287859916687\n",
      "[LOG 20200511-10:13:26] epoch: 0, batch: 12 train-loss: 2.032496929168701\n",
      "[LOG 20200511-10:13:26] epoch: 0, batch: 13 train-loss: 2.232581615447998\n",
      "[LOG 20200511-10:13:26] epoch: 0, batch: 14 train-loss: 2.553823947906494\n",
      "[LOG 20200511-10:13:26] epoch: 0, batch: 15 train-loss: 2.3085010051727295\n",
      "[LOG 20200511-10:13:27] epoch: 0, batch: 16 train-loss: 1.951395869255066\n",
      "[LOG 20200511-10:13:27] epoch: 0, batch: 17 train-loss: 2.185049533843994\n",
      "[LOG 20200511-10:13:27] epoch: 0, batch: 18 train-loss: 2.309744358062744\n",
      "[LOG 20200511-10:13:27] epoch: 0, batch: 19 train-loss: 2.1776602268218994\n",
      "[LOG 20200511-10:13:27] epoch: 0, batch: 20 train-loss: 2.2136330604553223\n",
      "[LOG 20200511-10:13:27] epoch: 0, batch: 21 train-loss: 2.3033759593963623\n",
      "[LOG 20200511-10:13:27] epoch: 0, batch: 22 train-loss: 2.4184529781341553\n",
      "[LOG 20200511-10:13:28] epoch: 0, batch: 23 train-loss: 2.5318949222564697\n",
      "[LOG 20200511-10:13:28] epoch: 0, batch: 24 train-loss: 2.333510160446167\n",
      "[LOG 20200511-10:13:28] epoch: 0, batch: 25 train-loss: 2.1710009574890137\n",
      "[LOG 20200511-10:13:28] epoch: 0, batch: 26 train-loss: 2.0325374603271484\n",
      "[LOG 20200511-10:13:28] epoch: 0, batch: 27 train-loss: 2.3943541049957275\n",
      "[LOG 20200511-10:13:28] epoch: 0, batch: 28 train-loss: 2.257317543029785\n",
      "[LOG 20200511-10:13:28] epoch: 0, batch: 29 train-loss: 2.2630820274353027\n",
      "[LOG 20200511-10:13:28] epoch: 0, batch: 30 train-loss: 2.402651071548462\n",
      "[LOG 20200511-10:13:29] epoch: 0, batch: 31 train-loss: 2.633769989013672\n",
      "[LOG 20200511-10:13:29] epoch: 0, batch: 32 train-loss: 2.2348368167877197\n",
      "[LOG 20200511-10:13:29] epoch: 0, batch: 33 train-loss: 2.3504581451416016\n",
      "[LOG 20200511-10:13:29] epoch: 0, batch: 34 train-loss: 2.3093695640563965\n",
      "[LOG 20200511-10:13:29] epoch: 0, batch: 35 train-loss: 2.2624869346618652\n",
      "[LOG 20200511-10:13:29] epoch: 0, batch: 36 train-loss: 2.5604493618011475\n",
      "[LOG 20200511-10:13:29] epoch: 0, batch: 37 train-loss: 2.267512083053589\n",
      "[LOG 20200511-10:13:30] epoch: 0, batch: 38 train-loss: 2.473426342010498\n",
      "[LOG 20200511-10:13:30] epoch: 0, batch: 39 train-loss: 2.244420051574707\n",
      "[LOG 20200511-10:13:30] epoch: 0, batch: 40 train-loss: 2.2234606742858887\n",
      "[LOG 20200511-10:13:30] epoch: 0, batch: 41 train-loss: 2.2086803913116455\n",
      "[LOG 20200511-10:13:30] epoch: 0, batch: 42 train-loss: 2.3094000816345215\n",
      "[LOG 20200511-10:13:30] epoch: 0, batch: 43 train-loss: 2.1833250522613525\n",
      "[LOG 20200511-10:13:31] epoch: 0, batch: 44 train-loss: 2.111686944961548\n",
      "[LOG 20200511-10:13:31] epoch: 0, batch: 45 train-loss: 2.468273639678955\n",
      "[LOG 20200511-10:13:31] epoch: 0, batch: 46 train-loss: 2.4541661739349365\n",
      "[LOG 20200511-10:13:31] epoch: 0, batch: 47 train-loss: 1.9645376205444336\n",
      "[LOG 20200511-10:13:31] epoch: 0, batch: 48 train-loss: 2.6466221809387207\n",
      "[LOG 20200511-10:13:31] epoch: 0, batch: 49 train-loss: 2.489612102508545\n",
      "[LOG 20200511-10:13:31] epoch: 0, batch: 50 train-loss: 2.569624423980713\n",
      "[LOG 20200511-10:13:31] epoch: 0, batch: 51 train-loss: 2.316236972808838\n",
      "[LOG 20200511-10:13:32] epoch: 0, batch: 52 train-loss: 1.9616694450378418\n",
      "[LOG 20200511-10:13:32] epoch: 0, batch: 53 train-loss: 2.431994676589966\n",
      "[LOG 20200511-10:13:32] epoch: 0, batch: 54 train-loss: 2.0976192951202393\n",
      "[LOG 20200511-10:13:32] epoch: 0, batch: 55 train-loss: 2.094014883041382\n",
      "[LOG 20200511-10:13:32] epoch: 0, batch: 56 train-loss: 2.2184481620788574\n",
      "[LOG 20200511-10:13:32] epoch: 0, batch: 57 train-loss: 2.9001011848449707\n",
      "[LOG 20200511-10:13:32] epoch: 0, batch: 58 train-loss: 2.3129072189331055\n",
      "[LOG 20200511-10:13:33] epoch: 0, batch: 59 train-loss: 2.312783718109131\n",
      "[LOG 20200511-10:13:33] epoch: 0, batch: 60 train-loss: 2.2748208045959473\n",
      "[LOG 20200511-10:13:33] epoch: 0, batch: 61 train-loss: 2.2260172367095947\n",
      "[LOG 20200511-10:13:33] epoch: 0, batch: 62 train-loss: 2.308405876159668\n",
      "[LOG 20200511-10:13:33] epoch: 0, batch: 63 train-loss: 2.4591236114501953\n",
      "[LOG 20200511-10:13:33] epoch: 0, batch: 64 train-loss: 2.220205307006836\n",
      "[LOG 20200511-10:13:33] epoch: 0, batch: 65 train-loss: 2.0781402587890625\n",
      "[LOG 20200511-10:13:34] epoch: 0, batch: 66 train-loss: 2.2874975204467773\n",
      "[LOG 20200511-10:13:34] epoch: 0, batch: 67 train-loss: 2.170093059539795\n",
      "[LOG 20200511-10:13:34] epoch: 0, batch: 68 train-loss: 2.306511402130127\n",
      "[LOG 20200511-10:13:34] epoch: 0, batch: 69 train-loss: 1.799140453338623\n",
      "[LOG 20200511-10:13:34] epoch: 0, batch: 70 train-loss: 2.5262956619262695\n",
      "[LOG 20200511-10:13:34] epoch: 0, batch: 71 train-loss: 2.608159303665161\n",
      "[LOG 20200511-10:13:34] epoch: 0, batch: 72 train-loss: 2.0476577281951904\n",
      "[LOG 20200511-10:13:35] epoch: 0, batch: 73 train-loss: 2.460758686065674\n",
      "[LOG 20200511-10:13:35] epoch: 0, batch: 74 train-loss: 2.5100793838500977\n",
      "[LOG 20200511-10:13:35] epoch: 0, batch: 75 train-loss: 2.2445411682128906\n",
      "[LOG 20200511-10:13:35] epoch: 0, batch: 76 train-loss: 2.5397419929504395\n",
      "[LOG 20200511-10:13:35] epoch: 0, batch: 77 train-loss: 2.112337112426758\n",
      "[LOG 20200511-10:13:35] epoch: 0, batch: 78 train-loss: 2.4768028259277344\n",
      "[LOG 20200511-10:13:35] epoch: 0, batch: 79 train-loss: 2.2051877975463867\n",
      "[LOG 20200511-10:13:35] epoch: 0, batch: 80 train-loss: 2.0974559783935547\n",
      "[LOG 20200511-10:13:36] epoch: 0, batch: 81 train-loss: 2.460291862487793\n",
      "[LOG 20200511-10:13:36] epoch: 0, batch: 82 train-loss: 2.3083558082580566\n",
      "[LOG 20200511-10:13:36] epoch: 0, batch: 83 train-loss: 1.9733850955963135\n",
      "[LOG 20200511-10:13:36] epoch: 0, batch: 84 train-loss: 2.2010114192962646\n",
      "[LOG 20200511-10:13:36] epoch: 0, batch: 85 train-loss: 2.260646343231201\n",
      "[LOG 20200511-10:13:36] epoch: 0, batch: 86 train-loss: 2.1062188148498535\n",
      "[LOG 20200511-10:13:36] epoch: 0, batch: 87 train-loss: 2.1508002281188965\n",
      "[LOG 20200511-10:13:36] epoch: 0, batch: 88 train-loss: 2.1972670555114746\n",
      "[LOG 20200511-10:13:37] epoch: 0, batch: 89 train-loss: 2.5109543800354004\n",
      "[LOG 20200511-10:13:37] epoch: 0, batch: 90 train-loss: 2.074925422668457\n",
      "[LOG 20200511-10:13:37] epoch: 0, batch: 91 train-loss: 2.269453525543213\n",
      "[LOG 20200511-10:13:37] epoch: 0, batch: 92 train-loss: 2.1789817810058594\n",
      "[LOG 20200511-10:13:37] epoch: 0, batch: 93 train-loss: 2.432319402694702\n",
      "[LOG 20200511-10:13:37] epoch: 0, batch: 94 train-loss: 2.3496012687683105\n",
      "[LOG 20200511-10:13:37] epoch: 0, batch: 95 train-loss: 2.1062464714050293\n",
      "[LOG 20200511-10:13:38] epoch: 0, batch: 96 train-loss: 2.14770770072937\n",
      "[LOG 20200511-10:13:38] epoch: 0, batch: 97 train-loss: 2.3840744495391846\n",
      "[LOG 20200511-10:13:38] epoch: 0, batch: 98 train-loss: 2.255488395690918\n",
      "[LOG 20200511-10:13:38] epoch: 0, batch: 99 train-loss: 2.1744048595428467\n",
      "[LOG 20200511-10:13:38] epoch: 0, batch: 100 train-loss: 2.3768248558044434\n",
      "[LOG 20200511-10:13:38] epoch: 0, batch: 101 train-loss: 1.9902280569076538\n",
      "[LOG 20200511-10:13:38] epoch: 0, batch: 102 train-loss: 2.2658722400665283\n",
      "[LOG 20200511-10:13:38] epoch: 0, batch: 103 train-loss: 2.2509305477142334\n",
      "[LOG 20200511-10:13:39] epoch: 0, batch: 104 train-loss: 2.2314982414245605\n",
      "[LOG 20200511-10:13:39] epoch: 0, batch: 105 train-loss: 1.9509445428848267\n",
      "[LOG 20200511-10:13:39] epoch: 0, batch: 106 train-loss: 2.009577751159668\n",
      "[LOG 20200511-10:13:39] epoch: 0, batch: 107 train-loss: 2.2915382385253906\n",
      "[LOG 20200511-10:13:39] epoch: 0, batch: 108 train-loss: 2.1582112312316895\n",
      "[LOG 20200511-10:13:39] epoch: 0, batch: 109 train-loss: 2.2371814250946045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:13:39] epoch: 0, batch: 110 train-loss: 2.082285165786743\n",
      "[LOG 20200511-10:13:40] epoch: 0, batch: 111 train-loss: 2.2324929237365723\n",
      "[LOG 20200511-10:13:40] epoch: 0, batch: 112 train-loss: 2.0562384128570557\n",
      "[LOG 20200511-10:13:40] epoch: 0, batch: 113 train-loss: 2.1778342723846436\n",
      "[LOG 20200511-10:13:40] epoch: 0, batch: 114 train-loss: 2.3845648765563965\n",
      "[LOG 20200511-10:13:40] epoch: 0, batch: 115 train-loss: 2.39973521232605\n",
      "[LOG 20200511-10:13:40] epoch: 0, batch: 116 train-loss: 2.099135398864746\n",
      "[LOG 20200511-10:13:40] epoch: 0, batch: 117 train-loss: 2.454850912094116\n",
      "[LOG 20200511-10:13:41] epoch: 0, batch: 118 train-loss: 2.2798538208007812\n",
      "[LOG 20200511-10:13:41] epoch: 0, batch: 119 train-loss: 2.2404897212982178\n",
      "[LOG 20200511-10:13:41] epoch: 0, batch: 120 train-loss: 2.2159833908081055\n",
      "[LOG 20200511-10:13:41] epoch: 0, batch: 121 train-loss: 2.111107587814331\n",
      "[LOG 20200511-10:13:41] epoch: 0, batch: 122 train-loss: 2.2358360290527344\n",
      "[LOG 20200511-10:13:41] epoch: 0, batch: 123 train-loss: 2.7269794940948486\n",
      "[LOG 20200511-10:13:41] epoch: 0, batch: 124 train-loss: 2.188248634338379\n",
      "[LOG 20200511-10:13:42] epoch: 0, batch: 125 train-loss: 2.190654993057251\n",
      "[LOG 20200511-10:13:42] epoch: 0, batch: 126 train-loss: 2.0429203510284424\n",
      "[LOG 20200511-10:13:42] epoch: 0, batch: 127 train-loss: 2.104830741882324\n",
      "[LOG 20200511-10:13:42] epoch: 0, batch: 128 train-loss: 2.4682531356811523\n",
      "[LOG 20200511-10:13:42] epoch: 0, batch: 129 train-loss: 2.137882709503174\n",
      "[LOG 20200511-10:13:42] epoch: 0, batch: 130 train-loss: 2.0537896156311035\n",
      "[LOG 20200511-10:13:43] epoch: 0, batch: 131 train-loss: 2.661766290664673\n",
      "[LOG 20200511-10:13:43] epoch: 0, batch: 132 train-loss: 2.277456760406494\n",
      "[LOG 20200511-10:13:43] epoch: 0, batch: 133 train-loss: 2.452479839324951\n",
      "[LOG 20200511-10:13:43] epoch: 0, batch: 134 train-loss: 2.3842825889587402\n",
      "[LOG 20200511-10:13:43] epoch: 0, batch: 135 train-loss: 1.8301959037780762\n",
      "[LOG 20200511-10:13:43] epoch: 0, batch: 136 train-loss: 1.9545238018035889\n",
      "[LOG 20200511-10:13:43] epoch: 0, batch: 137 train-loss: 2.235178232192993\n",
      "[LOG 20200511-10:13:44] epoch: 0, batch: 138 train-loss: 2.5201656818389893\n",
      "[LOG 20200511-10:13:44] epoch: 0, batch: 139 train-loss: 2.029611825942993\n",
      "[LOG 20200511-10:13:44] epoch: 0, batch: 140 train-loss: 1.87880539894104\n",
      "[LOG 20200511-10:13:44] epoch: 0, batch: 141 train-loss: 1.9763154983520508\n",
      "[LOG 20200511-10:13:44] epoch: 0, batch: 142 train-loss: 1.8806465864181519\n",
      "[LOG 20200511-10:13:44] epoch: 0, batch: 143 train-loss: 2.35825252532959\n",
      "[LOG 20200511-10:13:44] epoch: 0, batch: 144 train-loss: 2.4756956100463867\n",
      "[LOG 20200511-10:13:45] epoch: 0, batch: 145 train-loss: 2.678126573562622\n",
      "[LOG 20200511-10:13:45] epoch: 0, batch: 146 train-loss: 2.5529470443725586\n",
      "[LOG 20200511-10:13:45] epoch: 0, batch: 147 train-loss: 2.175337314605713\n",
      "[LOG 20200511-10:13:45] epoch: 0, batch: 148 train-loss: 2.539477586746216\n",
      "[LOG 20200511-10:13:45] epoch: 0, batch: 149 train-loss: 2.3215038776397705\n",
      "[LOG 20200511-10:13:45] epoch: 0, batch: 150 train-loss: 2.158851146697998\n",
      "[LOG 20200511-10:13:45] epoch: 0, batch: 151 train-loss: 1.891546368598938\n",
      "[LOG 20200511-10:13:46] epoch: 0, batch: 152 train-loss: 2.122349977493286\n",
      "[LOG 20200511-10:13:46] epoch: 0, batch: 153 train-loss: 2.1578946113586426\n",
      "[LOG 20200511-10:13:46] epoch: 0, batch: 154 train-loss: 2.2074854373931885\n",
      "[LOG 20200511-10:13:46] epoch: 0, batch: 155 train-loss: 2.412062168121338\n",
      "[LOG 20200511-10:13:46] epoch: 0, batch: 156 train-loss: 2.0230042934417725\n",
      "[LOG 20200511-10:13:46] epoch: 0, batch: 157 train-loss: 2.19962739944458\n",
      "[LOG 20200511-10:13:46] epoch: 0, batch: 158 train-loss: 2.2432665824890137\n",
      "[LOG 20200511-10:13:47] epoch: 0, batch: 159 train-loss: 1.8937042951583862\n",
      "[LOG 20200511-10:13:47] epoch: 0, batch: 160 train-loss: 2.3951077461242676\n",
      "[LOG 20200511-10:13:47] epoch: 0, batch: 161 train-loss: 2.1182661056518555\n",
      "[LOG 20200511-10:13:47] epoch: 0, batch: 162 train-loss: 2.221541166305542\n",
      "[LOG 20200511-10:13:47] epoch: 0, batch: 163 train-loss: 2.5035207271575928\n",
      "[LOG 20200511-10:13:47] epoch: 0, batch: 164 train-loss: 1.9896864891052246\n",
      "[LOG 20200511-10:13:47] epoch: 0, batch: 165 train-loss: 2.4144113063812256\n",
      "[LOG 20200511-10:13:48] epoch: 0, batch: 166 train-loss: 1.9590294361114502\n",
      "[LOG 20200511-10:13:48] epoch: 0, batch: 167 train-loss: 2.516947031021118\n",
      "[LOG 20200511-10:13:48] epoch: 0, batch: 168 train-loss: 2.279827117919922\n",
      "[LOG 20200511-10:13:48] epoch: 0, batch: 169 train-loss: 2.2006020545959473\n",
      "[LOG 20200511-10:13:48] epoch: 0, batch: 170 train-loss: 2.191004753112793\n",
      "[LOG 20200511-10:13:48] epoch: 0, batch: 171 train-loss: 2.2008473873138428\n",
      "[LOG 20200511-10:13:48] epoch: 0, batch: 172 train-loss: 2.330094814300537\n",
      "[LOG 20200511-10:13:49] epoch: 0, batch: 173 train-loss: 2.078561782836914\n",
      "[LOG 20200511-10:13:49] epoch: 0, batch: 174 train-loss: 2.154107093811035\n",
      "[LOG 20200511-10:13:49] epoch: 0, batch: 175 train-loss: 2.253129482269287\n",
      "[LOG 20200511-10:13:49] epoch: 0, batch: 176 train-loss: 2.5102648735046387\n",
      "[LOG 20200511-10:13:49] epoch: 0, batch: 177 train-loss: 2.572638511657715\n",
      "[LOG 20200511-10:13:49] epoch: 0, batch: 178 train-loss: 2.3000073432922363\n",
      "[LOG 20200511-10:13:49] epoch: 0, batch: 179 train-loss: 2.054915428161621\n",
      "[LOG 20200511-10:13:50] epoch: 0, batch: 180 train-loss: 2.140742540359497\n",
      "[LOG 20200511-10:13:50] epoch: 0, batch: 181 train-loss: 1.9881294965744019\n",
      "[LOG 20200511-10:13:50] epoch: 0, batch: 182 train-loss: 2.361586570739746\n",
      "[LOG 20200511-10:13:50] epoch: 0, batch: 183 train-loss: 2.3849360942840576\n",
      "[LOG 20200511-10:13:50] epoch: 0, batch: 184 train-loss: 2.0458879470825195\n",
      "[LOG 20200511-10:13:50] epoch: 0, batch: 185 train-loss: 2.2234795093536377\n",
      "[LOG 20200511-10:13:50] epoch: 0, batch: 186 train-loss: 2.1975929737091064\n",
      "[LOG 20200511-10:13:51] epoch: 0, batch: 187 train-loss: 1.9877729415893555\n",
      "[LOG 20200511-10:13:51] epoch: 0, batch: 188 train-loss: 2.1556448936462402\n",
      "[LOG 20200511-10:13:51] epoch: 0, batch: 189 train-loss: 2.282288074493408\n",
      "[LOG 20200511-10:13:51] epoch: 0, batch: 190 train-loss: 1.9087789058685303\n",
      "[LOG 20200511-10:13:51] epoch: 0, batch: 191 train-loss: 1.9630200862884521\n",
      "[LOG 20200511-10:13:51] epoch: 0, batch: 192 train-loss: 2.241507053375244\n",
      "[LOG 20200511-10:13:52] epoch: 0, batch: 193 train-loss: 1.749182105064392\n",
      "[LOG 20200511-10:13:52] epoch: 0, batch: 194 train-loss: 1.9460349082946777\n",
      "[LOG 20200511-10:13:52] epoch: 0, batch: 195 train-loss: 2.1024107933044434\n",
      "[LOG 20200511-10:13:52] epoch: 0, batch: 196 train-loss: 2.7532358169555664\n",
      "[LOG 20200511-10:13:52] epoch: 0, batch: 197 train-loss: 2.386648416519165\n",
      "[LOG 20200511-10:13:52] epoch: 0, batch: 198 train-loss: 2.2901723384857178\n",
      "[LOG 20200511-10:13:53] epoch: 0, batch: 199 train-loss: 2.297358274459839\n",
      "[LOG 20200511-10:13:53] epoch: 0, batch: 200 train-loss: 1.9307221174240112\n",
      "[LOG 20200511-10:13:53] epoch: 0, batch: 201 train-loss: 2.2638301849365234\n",
      "[LOG 20200511-10:13:53] epoch: 0, batch: 202 train-loss: 2.246918201446533\n",
      "[LOG 20200511-10:13:53] epoch: 0, batch: 203 train-loss: 2.262382745742798\n",
      "[LOG 20200511-10:13:53] epoch: 0, batch: 204 train-loss: 2.5502426624298096\n",
      "[LOG 20200511-10:13:53] epoch: 0, batch: 205 train-loss: 2.247971534729004\n",
      "[LOG 20200511-10:13:54] epoch: 0, batch: 206 train-loss: 2.431670904159546\n",
      "[LOG 20200511-10:13:54] epoch: 0, batch: 207 train-loss: 1.5733613967895508\n",
      "[LOG 20200511-10:13:54] epoch: 0, batch: 208 train-loss: 2.304565906524658\n",
      "[LOG 20200511-10:13:54] epoch: 0, batch: 209 train-loss: 1.9265023469924927\n",
      "[LOG 20200511-10:13:54] epoch: 0, batch: 210 train-loss: 2.0737969875335693\n",
      "[LOG 20200511-10:13:54] epoch: 0, batch: 211 train-loss: 2.163865804672241\n",
      "[LOG 20200511-10:13:54] epoch: 0, batch: 212 train-loss: 1.9582140445709229\n",
      "[LOG 20200511-10:13:55] epoch: 0, batch: 213 train-loss: 2.066437244415283\n",
      "[LOG 20200511-10:13:55] epoch: 0, batch: 214 train-loss: 1.8537003993988037\n",
      "[LOG 20200511-10:13:55] epoch: 0, batch: 215 train-loss: 1.7970459461212158\n",
      "[LOG 20200511-10:13:55] epoch: 0, batch: 216 train-loss: 2.391860246658325\n",
      "[LOG 20200511-10:13:55] epoch: 0, batch: 217 train-loss: 1.7925622463226318\n",
      "[LOG 20200511-10:13:55] epoch: 0, batch: 218 train-loss: 2.122774362564087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:13:55] epoch: 0, batch: 219 train-loss: 1.8623781204223633\n",
      "[LOG 20200511-10:13:56] epoch: 0, batch: 220 train-loss: 2.4086432456970215\n",
      "[LOG 20200511-10:13:56] epoch: 0, batch: 221 train-loss: 2.1106925010681152\n",
      "[LOG 20200511-10:13:56] epoch: 0, batch: 222 train-loss: 1.9180214405059814\n",
      "[LOG 20200511-10:13:56] epoch: 0, batch: 223 train-loss: 2.1901516914367676\n",
      "[LOG 20200511-10:13:56] epoch: 0, batch: 224 train-loss: 1.7667455673217773\n",
      "[LOG 20200511-10:13:56] epoch: 0, batch: 225 train-loss: 2.0182852745056152\n",
      "[LOG 20200511-10:13:56] epoch: 0, batch: 226 train-loss: 2.3178789615631104\n",
      "[LOG 20200511-10:13:56] epoch: 0, batch: 227 train-loss: 2.246184825897217\n",
      "[LOG 20200511-10:13:56] epoch: 0, batch: 228 train-loss: 1.8261213302612305\n",
      "[LOG 20200511-10:13:57] epoch: 0, batch: 229 train-loss: 2.242708921432495\n",
      "[LOG 20200511-10:13:57] epoch: 0, batch: 230 train-loss: 2.099839210510254\n",
      "[LOG 20200511-10:13:57] epoch: 0, batch: 231 train-loss: 2.404082775115967\n",
      "[LOG 20200511-10:13:57] epoch: 0, batch: 232 train-loss: 2.112795352935791\n",
      "[LOG 20200511-10:13:57] epoch: 0, batch: 233 train-loss: 1.9469431638717651\n",
      "[LOG 20200511-10:13:57] epoch: 0, batch: 234 train-loss: 2.2415666580200195\n",
      "[LOG 20200511-10:13:57] epoch: 0, batch: 235 train-loss: 2.216700315475464\n",
      "[LOG 20200511-10:13:57] epoch: 0, batch: 236 train-loss: 2.1897451877593994\n",
      "[LOG 20200511-10:13:57] epoch: 0, batch: 237 train-loss: 1.999191164970398\n",
      "[LOG 20200511-10:13:58] epoch: 0, batch: 238 train-loss: 2.0428266525268555\n",
      "[LOG 20200511-10:13:58] epoch: 0, batch: 239 train-loss: 1.8536736965179443\n",
      "[LOG 20200511-10:13:58] epoch: 0, batch: 240 train-loss: 2.4010729789733887\n",
      "[LOG 20200511-10:13:58] epoch: 0, batch: 241 train-loss: 2.1913108825683594\n",
      "[LOG 20200511-10:13:58] epoch: 0, batch: 242 train-loss: 1.921174168586731\n",
      "[LOG 20200511-10:13:58] epoch: 0, batch: 243 train-loss: 2.0056166648864746\n",
      "[LOG 20200511-10:13:58] epoch: 0, batch: 244 train-loss: 2.200948476791382\n",
      "[LOG 20200511-10:13:59] epoch: 0, batch: 245 train-loss: 1.935502052307129\n",
      "[LOG 20200511-10:13:59] epoch: 0, batch: 246 train-loss: 2.332920789718628\n",
      "[LOG 20200511-10:13:59] epoch: 0, batch: 247 train-loss: 2.047246217727661\n",
      "[LOG 20200511-10:13:59] epoch: 0, batch: 248 train-loss: 1.8857579231262207\n",
      "[LOG 20200511-10:13:59] epoch: 0, batch: 249 train-loss: 1.8880436420440674\n",
      "[LOG 20200511-10:13:59] epoch: 0, batch: 250 train-loss: 2.390528917312622\n",
      "[LOG 20200511-10:13:59] epoch: 0, batch: 251 train-loss: 2.010986328125\n",
      "[LOG 20200511-10:13:59] epoch: 0, batch: 252 train-loss: 2.225686550140381\n",
      "[LOG 20200511-10:13:59] epoch: 0, batch: 253 train-loss: 2.053382158279419\n",
      "[LOG 20200511-10:14:00] epoch: 0, batch: 254 train-loss: 1.9425926208496094\n",
      "[LOG 20200511-10:14:00] epoch: 0, batch: 255 train-loss: 1.7579867839813232\n",
      "[LOG 20200511-10:14:00] epoch: 0, batch: 256 train-loss: 1.838179349899292\n",
      "[LOG 20200511-10:14:00] epoch: 0, batch: 257 train-loss: 2.8768818378448486\n",
      "[LOG 20200511-10:14:00] epoch: 0, batch: 258 train-loss: 2.2207188606262207\n",
      "[LOG 20200511-10:14:00] epoch: 0, batch: 259 train-loss: 1.8946670293807983\n",
      "[LOG 20200511-10:14:00] epoch: 0, batch: 260 train-loss: 1.9376171827316284\n",
      "[LOG 20200511-10:14:00] epoch: 0, batch: 261 train-loss: 2.332613945007324\n",
      "[LOG 20200511-10:14:01] epoch: 0, batch: 262 train-loss: 2.6360583305358887\n",
      "[LOG 20200511-10:14:01] epoch: 0, batch: 263 train-loss: 2.3884966373443604\n",
      "[LOG 20200511-10:14:01] epoch: 0, batch: 264 train-loss: 2.2896361351013184\n",
      "[LOG 20200511-10:14:01] epoch: 0, batch: 265 train-loss: 2.317254066467285\n",
      "[LOG 20200511-10:14:01] epoch: 0, batch: 266 train-loss: 1.688745141029358\n",
      "[LOG 20200511-10:14:01] epoch: 0, batch: 267 train-loss: 1.7444472312927246\n",
      "[LOG 20200511-10:14:01] epoch: 0, batch: 268 train-loss: 2.406332492828369\n",
      "[LOG 20200511-10:14:02] epoch: 0, batch: 269 train-loss: 2.422424793243408\n",
      "[LOG 20200511-10:14:02] epoch: 0, batch: 270 train-loss: 1.829922080039978\n",
      "[LOG 20200511-10:14:02] epoch: 0, batch: 271 train-loss: 2.0630135536193848\n",
      "[LOG 20200511-10:14:02] epoch: 0, batch: 272 train-loss: 1.822630524635315\n",
      "[LOG 20200511-10:14:02] epoch: 0, batch: 273 train-loss: 2.148880958557129\n",
      "[LOG 20200511-10:14:02] epoch: 0, batch: 274 train-loss: 2.4867043495178223\n",
      "[LOG 20200511-10:14:02] epoch: 0, batch: 275 train-loss: 2.1500658988952637\n",
      "[LOG 20200511-10:14:03] epoch: 0, batch: 276 train-loss: 1.953297734260559\n",
      "[LOG 20200511-10:14:03] epoch: 0, batch: 277 train-loss: 2.4247660636901855\n",
      "[LOG 20200511-10:14:03] epoch: 0, batch: 278 train-loss: 2.169948101043701\n",
      "[LOG 20200511-10:14:03] epoch: 0, batch: 279 train-loss: 1.8326658010482788\n",
      "[LOG 20200511-10:14:03] epoch: 0, batch: 280 train-loss: 1.9472863674163818\n",
      "[LOG 20200511-10:14:03] epoch: 0, batch: 281 train-loss: 2.048825263977051\n",
      "[LOG 20200511-10:14:03] epoch: 0, batch: 282 train-loss: 2.235950469970703\n",
      "[LOG 20200511-10:14:03] epoch: 0, batch: 283 train-loss: 2.0633015632629395\n",
      "[LOG 20200511-10:14:04] epoch: 0, batch: 284 train-loss: 2.0138626098632812\n",
      "[LOG 20200511-10:14:04] epoch: 0, batch: 285 train-loss: 2.216017723083496\n",
      "[LOG 20200511-10:14:04] epoch: 0, batch: 286 train-loss: 2.245210886001587\n",
      "[LOG 20200511-10:14:04] epoch: 0, batch: 287 train-loss: 1.993833065032959\n",
      "[LOG 20200511-10:14:04] epoch: 0, batch: 288 train-loss: 2.2606611251831055\n",
      "[LOG 20200511-10:14:04] epoch: 0, batch: 289 train-loss: 2.4233336448669434\n",
      "[LOG 20200511-10:14:04] epoch: 0, batch: 290 train-loss: 1.7013521194458008\n",
      "[LOG 20200511-10:14:04] epoch: 0, batch: 291 train-loss: 1.8467531204223633\n",
      "[LOG 20200511-10:14:04] epoch: 0, batch: 292 train-loss: 1.9298723936080933\n",
      "[LOG 20200511-10:14:05] epoch: 0, batch: 293 train-loss: 1.7745063304901123\n",
      "[LOG 20200511-10:14:05] epoch: 0, batch: 294 train-loss: 1.7776734828948975\n",
      "[LOG 20200511-10:14:05] epoch: 0, batch: 295 train-loss: 1.9212603569030762\n",
      "[LOG 20200511-10:14:05] epoch: 0, batch: 296 train-loss: 2.2042343616485596\n",
      "[LOG 20200511-10:14:05] epoch: 0, batch: 297 train-loss: 2.0230517387390137\n",
      "[LOG 20200511-10:14:05] epoch: 0, batch: 298 train-loss: 2.1241862773895264\n",
      "[LOG 20200511-10:14:05] epoch: 0, batch: 299 train-loss: 1.907800316810608\n",
      "[LOG 20200511-10:14:05] epoch: 0, batch: 300 train-loss: 1.675595998764038\n",
      "[LOG 20200511-10:14:05] epoch: 0, batch: 301 train-loss: 2.380000352859497\n",
      "[LOG 20200511-10:14:06] epoch: 0, batch: 302 train-loss: 2.0646607875823975\n",
      "[LOG 20200511-10:14:06] epoch: 0, batch: 303 train-loss: 1.9637211561203003\n",
      "[LOG 20200511-10:14:06] epoch: 0, batch: 304 train-loss: 1.676132321357727\n",
      "[LOG 20200511-10:14:06] epoch: 0, batch: 305 train-loss: 1.9135518074035645\n",
      "[LOG 20200511-10:14:06] epoch: 0, batch: 306 train-loss: 2.026113510131836\n",
      "[LOG 20200511-10:14:06] epoch: 0, batch: 307 train-loss: 2.2589478492736816\n",
      "[LOG 20200511-10:14:06] epoch: 0, batch: 308 train-loss: 1.8688169717788696\n",
      "[LOG 20200511-10:14:06] epoch: 0, batch: 309 train-loss: 2.0616092681884766\n",
      "[LOG 20200511-10:14:06] epoch: 0, batch: 310 train-loss: 2.077043056488037\n",
      "[LOG 20200511-10:14:07] epoch: 0, batch: 311 train-loss: 2.157432794570923\n",
      "[LOG 20200511-10:14:07] epoch: 0, batch: 312 train-loss: 2.155564069747925\n",
      "[LOG 20200511-10:14:07] epoch: 0, batch: 313 train-loss: 2.3666939735412598\n",
      "[LOG 20200511-10:14:07] epoch: 0, batch: 314 train-loss: 2.499453544616699\n",
      "[LOG 20200511-10:14:07] epoch: 0, batch: 315 train-loss: 1.9721128940582275\n",
      "[LOG 20200511-10:14:07] epoch: 0, batch: 316 train-loss: 2.375123977661133\n",
      "[LOG 20200511-10:14:07] epoch: 0, batch: 317 train-loss: 2.519838571548462\n",
      "[LOG 20200511-10:14:07] epoch: 0, batch: 318 train-loss: 1.9390993118286133\n",
      "[LOG 20200511-10:14:07] epoch: 0, batch: 319 train-loss: 1.8214234113693237\n",
      "[LOG 20200511-10:14:08] epoch: 0, batch: 320 train-loss: 2.0055041313171387\n",
      "[LOG 20200511-10:14:08] epoch: 0, batch: 321 train-loss: 2.0241851806640625\n",
      "[LOG 20200511-10:14:08] epoch: 0, batch: 322 train-loss: 2.1679811477661133\n",
      "[LOG 20200511-10:14:08] epoch: 0, batch: 323 train-loss: 1.3736389875411987\n",
      "[LOG 20200511-10:14:08] epoch: 0, batch: 324 train-loss: 1.9616315364837646\n",
      "[LOG 20200511-10:14:08] epoch: 0, batch: 325 train-loss: 2.3117713928222656\n",
      "[LOG 20200511-10:14:08] epoch: 0, batch: 326 train-loss: 2.38916015625\n",
      "[LOG 20200511-10:14:08] epoch: 0, batch: 327 train-loss: 2.2632439136505127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:14:08] epoch: 0, batch: 328 train-loss: 2.062162399291992\n",
      "[LOG 20200511-10:14:09] epoch: 0, batch: 329 train-loss: 1.6976137161254883\n",
      "[LOG 20200511-10:14:09] epoch: 0, batch: 330 train-loss: 1.5029631853103638\n",
      "[LOG 20200511-10:14:09] epoch: 0, batch: 331 train-loss: 2.1708076000213623\n",
      "[LOG 20200511-10:14:09] epoch: 0, batch: 332 train-loss: 1.3247002363204956\n",
      "[LOG 20200511-10:14:09] epoch: 0, batch: 333 train-loss: 1.7095024585723877\n",
      "[LOG 20200511-10:14:09] epoch: 0, batch: 334 train-loss: 2.389310121536255\n",
      "[LOG 20200511-10:14:09] epoch: 0, batch: 335 train-loss: 2.447578191757202\n",
      "[LOG 20200511-10:14:09] epoch: 0, batch: 336 train-loss: 1.781511664390564\n",
      "[LOG 20200511-10:14:09] epoch: 0, batch: 337 train-loss: 2.326540470123291\n",
      "[LOG 20200511-10:14:09] epoch: 0, batch: 338 train-loss: 1.7733335494995117\n",
      "[LOG 20200511-10:14:10] epoch: 0, batch: 339 train-loss: 1.7214717864990234\n",
      "[LOG 20200511-10:14:10] epoch: 0, batch: 340 train-loss: 2.183126926422119\n",
      "[LOG 20200511-10:14:10] epoch: 0, batch: 341 train-loss: 2.1737325191497803\n",
      "[LOG 20200511-10:14:10] epoch: 0, batch: 342 train-loss: 1.9088313579559326\n",
      "[LOG 20200511-10:14:10] epoch: 0, batch: 343 train-loss: 2.4565834999084473\n",
      "[LOG 20200511-10:14:10] epoch: 0, batch: 344 train-loss: 2.001915216445923\n",
      "[LOG 20200511-10:14:10] epoch: 0, batch: 345 train-loss: 2.005983829498291\n",
      "[LOG 20200511-10:14:10] epoch: 0, batch: 346 train-loss: 1.8861721754074097\n",
      "[LOG 20200511-10:14:10] epoch: 0, batch: 347 train-loss: 2.2281382083892822\n",
      "[LOG 20200511-10:14:11] epoch: 0, batch: 348 train-loss: 2.156477212905884\n",
      "[LOG 20200511-10:14:11] epoch: 0, batch: 349 train-loss: 1.583586573600769\n",
      "[LOG 20200511-10:14:11] epoch: 0, batch: 350 train-loss: 2.4177675247192383\n",
      "[LOG 20200511-10:14:11] epoch: 0, batch: 351 train-loss: 2.1083152294158936\n",
      "[LOG 20200511-10:14:11] epoch: 0, batch: 352 train-loss: 2.838796854019165\n",
      "[LOG 20200511-10:14:11] epoch: 0, batch: 353 train-loss: 1.692654013633728\n",
      "[LOG 20200511-10:14:11] epoch: 0, batch: 354 train-loss: 2.3558971881866455\n",
      "[LOG 20200511-10:14:11] epoch: 0, batch: 355 train-loss: 2.3435583114624023\n",
      "[LOG 20200511-10:14:12] epoch: 0, batch: 356 train-loss: 2.356452703475952\n",
      "[LOG 20200511-10:14:12] epoch: 0, batch: 357 train-loss: 2.0295703411102295\n",
      "[LOG 20200511-10:14:12] epoch: 0, batch: 358 train-loss: 1.6022952795028687\n",
      "[LOG 20200511-10:14:12] epoch: 0, batch: 359 train-loss: 1.9005166292190552\n",
      "[LOG 20200511-10:14:12] epoch: 0, batch: 360 train-loss: 2.157949924468994\n",
      "[LOG 20200511-10:14:12] epoch: 0, batch: 361 train-loss: 1.9157941341400146\n",
      "[LOG 20200511-10:14:12] epoch: 0, batch: 362 train-loss: 1.504309058189392\n",
      "[LOG 20200511-10:14:12] epoch: 0, batch: 363 train-loss: 1.5957715511322021\n",
      "[LOG 20200511-10:14:12] epoch: 0, batch: 364 train-loss: 2.203707695007324\n",
      "[LOG 20200511-10:14:13] epoch: 0, batch: 365 train-loss: 2.3857827186584473\n",
      "[LOG 20200511-10:14:13] epoch: 0, batch: 366 train-loss: 1.8463314771652222\n",
      "[LOG 20200511-10:14:13] epoch: 0, batch: 367 train-loss: 1.885382890701294\n",
      "[LOG 20200511-10:14:13] epoch: 0, batch: 368 train-loss: 2.3922324180603027\n",
      "[LOG 20200511-10:14:13] epoch: 0, batch: 369 train-loss: 2.1112990379333496\n",
      "[LOG 20200511-10:14:13] epoch: 0, batch: 370 train-loss: 2.4682254791259766\n",
      "[LOG 20200511-10:14:13] epoch: 0, batch: 371 train-loss: 2.2591121196746826\n",
      "[LOG 20200511-10:14:13] epoch: 0, batch: 372 train-loss: 1.9201083183288574\n",
      "[LOG 20200511-10:14:13] epoch: 0, batch: 373 train-loss: 1.9275248050689697\n",
      "[LOG 20200511-10:14:14] epoch: 0, batch: 374 train-loss: 2.1258227825164795\n",
      "[LOG 20200511-10:14:14] epoch: 0, batch: 375 train-loss: 1.3957338333129883\n",
      "[LOG 20200511-10:14:14] epoch: 0, batch: 376 train-loss: 1.8201708793640137\n",
      "[LOG 20200511-10:14:14] epoch: 0, batch: 377 train-loss: 1.7506083250045776\n",
      "[LOG 20200511-10:14:14] epoch: 0, batch: 378 train-loss: 1.9438631534576416\n",
      "[LOG 20200511-10:14:14] epoch: 0, batch: 379 train-loss: 1.4950464963912964\n",
      "[LOG 20200511-10:14:14] epoch: 0, batch: 380 train-loss: 1.7033740282058716\n",
      "[LOG 20200511-10:14:14] epoch: 0, batch: 381 train-loss: 2.185004711151123\n",
      "[LOG 20200511-10:14:14] epoch: 0, batch: 382 train-loss: 1.8985263109207153\n",
      "[LOG 20200511-10:14:15] epoch: 0, batch: 383 train-loss: 2.0917718410491943\n",
      "[LOG 20200511-10:14:15] epoch: 0, batch: 384 train-loss: 2.1225931644439697\n",
      "[LOG 20200511-10:14:15] epoch: 0, batch: 385 train-loss: 2.1363818645477295\n",
      "[LOG 20200511-10:14:15] epoch: 0, batch: 386 train-loss: 2.192251443862915\n",
      "[LOG 20200511-10:14:15] epoch: 0, batch: 387 train-loss: 2.286252975463867\n",
      "[LOG 20200511-10:14:15] epoch: 0, batch: 388 train-loss: 2.0463857650756836\n",
      "[LOG 20200511-10:14:15] epoch: 0, batch: 389 train-loss: 1.7358752489089966\n",
      "[LOG 20200511-10:14:15] epoch: 0, batch: 390 train-loss: 2.240386486053467\n",
      "[LOG 20200511-10:14:15] epoch: 0, batch: 391 train-loss: 2.40285587310791\n",
      "[LOG 20200511-10:14:16] epoch: 0, batch: 392 train-loss: 1.8181649446487427\n",
      "[LOG 20200511-10:14:16] epoch: 0, batch: 393 train-loss: 1.8879457712173462\n",
      "[LOG 20200511-10:14:16] epoch: 0, batch: 394 train-loss: 2.2906620502471924\n",
      "[LOG 20200511-10:14:16] epoch: 0, batch: 395 train-loss: 2.1699986457824707\n",
      "[LOG 20200511-10:14:16] epoch: 0, batch: 396 train-loss: 1.988938331604004\n",
      "[LOG 20200511-10:14:16] epoch: 0, batch: 397 train-loss: 1.836814045906067\n",
      "[LOG 20200511-10:14:16] epoch: 0, batch: 398 train-loss: 2.0456044673919678\n",
      "[LOG 20200511-10:14:16] epoch: 0, batch: 399 train-loss: 2.1914210319519043\n",
      "[LOG 20200511-10:14:16] epoch: 0, batch: 400 train-loss: 1.7893836498260498\n",
      "[LOG 20200511-10:14:17] epoch: 0, batch: 401 train-loss: 2.179018020629883\n",
      "[LOG 20200511-10:14:17] epoch: 0, batch: 402 train-loss: 1.9545694589614868\n",
      "[LOG 20200511-10:14:17] epoch: 0, batch: 403 train-loss: 1.7772144079208374\n",
      "[LOG 20200511-10:14:17] epoch: 0, batch: 404 train-loss: 2.075747489929199\n",
      "[LOG 20200511-10:14:17] epoch: 0, batch: 405 train-loss: 1.6521244049072266\n",
      "[LOG 20200511-10:14:17] epoch: 0, batch: 406 train-loss: 1.809328317642212\n",
      "[LOG 20200511-10:14:17] epoch: 0, batch: 407 train-loss: 2.3561532497406006\n",
      "[LOG 20200511-10:14:17] epoch: 0, batch: 408 train-loss: 1.4440534114837646\n",
      "[LOG 20200511-10:14:17] epoch: 0, batch: 409 train-loss: 1.9669458866119385\n",
      "[LOG 20200511-10:14:18] epoch: 0, batch: 410 train-loss: 2.0939552783966064\n",
      "[LOG 20200511-10:14:18] epoch: 0, batch: 411 train-loss: 1.579261064529419\n",
      "[LOG 20200511-10:14:18] epoch: 0, batch: 412 train-loss: 1.6296560764312744\n",
      "[LOG 20200511-10:14:18] epoch: 0, batch: 413 train-loss: 1.9165899753570557\n",
      "[LOG 20200511-10:14:18] epoch: 0, batch: 414 train-loss: 2.029107093811035\n",
      "[LOG 20200511-10:14:18] epoch: 0, batch: 415 train-loss: 1.7958948612213135\n",
      "[LOG 20200511-10:14:18] epoch: 0, batch: 416 train-loss: 1.5417393445968628\n",
      "[LOG 20200511-10:14:18] epoch: 0, batch: 417 train-loss: 2.387051582336426\n",
      "[LOG 20200511-10:14:18] epoch: 0, batch: 418 train-loss: 2.0879156589508057\n",
      "[LOG 20200511-10:14:19] epoch: 0, batch: 419 train-loss: 1.704059362411499\n",
      "[LOG 20200511-10:14:19] epoch: 0, batch: 420 train-loss: 2.068199396133423\n",
      "[LOG 20200511-10:14:19] epoch: 0, batch: 421 train-loss: 2.307886838912964\n",
      "[LOG 20200511-10:14:19] epoch: 0, batch: 422 train-loss: 1.721459150314331\n",
      "[LOG 20200511-10:14:19] epoch: 0, batch: 423 train-loss: 1.8924144506454468\n",
      "[LOG 20200511-10:14:19] epoch: 0, batch: 424 train-loss: 1.8776110410690308\n",
      "[LOG 20200511-10:14:19] epoch: 0, batch: 425 train-loss: 1.522854208946228\n",
      "[LOG 20200511-10:14:19] epoch: 0, batch: 426 train-loss: 1.9565647840499878\n",
      "[LOG 20200511-10:14:19] epoch: 0, batch: 427 train-loss: 1.8861033916473389\n",
      "[LOG 20200511-10:14:20] epoch: 0, batch: 428 train-loss: 1.5995280742645264\n",
      "[LOG 20200511-10:14:20] epoch: 0, batch: 429 train-loss: 1.996372938156128\n",
      "[LOG 20200511-10:14:20] epoch: 0, batch: 430 train-loss: 2.6135737895965576\n",
      "[LOG 20200511-10:14:20] epoch: 0, batch: 431 train-loss: 1.555846095085144\n",
      "[LOG 20200511-10:14:20] epoch: 0, batch: 432 train-loss: 2.2405667304992676\n",
      "[LOG 20200511-10:14:20] epoch: 0, batch: 433 train-loss: 1.9534425735473633\n",
      "[LOG 20200511-10:14:20] epoch: 0, batch: 434 train-loss: 2.190429210662842\n",
      "[LOG 20200511-10:14:20] epoch: 0, batch: 435 train-loss: 2.634901523590088\n",
      "[LOG 20200511-10:14:20] epoch: 0, batch: 436 train-loss: 1.6353702545166016\n",
      "[LOG 20200511-10:14:21] epoch: 0, batch: 437 train-loss: 2.110682964324951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:14:21] epoch: 0, batch: 438 train-loss: 2.3301100730895996\n",
      "[LOG 20200511-10:14:21] epoch: 0, batch: 439 train-loss: 2.410452365875244\n",
      "[LOG 20200511-10:14:21] epoch: 0, batch: 440 train-loss: 1.9257012605667114\n",
      "[LOG 20200511-10:14:21] epoch: 0, batch: 441 train-loss: 2.4418230056762695\n",
      "[LOG 20200511-10:14:21] epoch: 0, batch: 442 train-loss: 2.676530122756958\n",
      "[LOG 20200511-10:14:21] epoch: 0, batch: 443 train-loss: 2.1054880619049072\n",
      "[LOG 20200511-10:14:21] epoch: 0, batch: 444 train-loss: 1.86165189743042\n",
      "[LOG 20200511-10:14:21] epoch: 0, batch: 445 train-loss: 2.0418810844421387\n",
      "[LOG 20200511-10:14:22] epoch: 0, batch: 446 train-loss: 1.5347646474838257\n",
      "[LOG 20200511-10:14:22] epoch: 0, batch: 447 train-loss: 2.2708351612091064\n",
      "[LOG 20200511-10:14:22] epoch: 0, batch: 448 train-loss: 1.700238823890686\n",
      "[LOG 20200511-10:14:22] epoch: 0, batch: 449 train-loss: 2.2042179107666016\n",
      "[LOG 20200511-10:14:22] epoch: 0, batch: 450 train-loss: 1.8560807704925537\n",
      "[LOG 20200511-10:14:22] epoch: 0, batch: 451 train-loss: 1.5008795261383057\n",
      "[LOG 20200511-10:14:22] epoch: 0, batch: 452 train-loss: 2.274637222290039\n",
      "[LOG 20200511-10:14:22] epoch: 0, batch: 453 train-loss: 1.7916152477264404\n",
      "[LOG 20200511-10:14:22] epoch: 0, batch: 454 train-loss: 1.9176251888275146\n",
      "[LOG 20200511-10:14:23] epoch: 0, batch: 455 train-loss: 1.7436814308166504\n",
      "[LOG 20200511-10:14:23] epoch: 0, batch: 456 train-loss: 2.082555055618286\n",
      "[LOG 20200511-10:14:23] epoch: 0, batch: 457 train-loss: 1.4086939096450806\n",
      "[LOG 20200511-10:14:23] epoch: 0, batch: 458 train-loss: 2.4930362701416016\n",
      "[LOG 20200511-10:14:23] epoch: 0, batch: 459 train-loss: 2.114882469177246\n",
      "[LOG 20200511-10:14:23] epoch: 0, batch: 460 train-loss: 1.5199326276779175\n",
      "[LOG 20200511-10:14:23] epoch: 0, batch: 461 train-loss: 1.9659366607666016\n",
      "[LOG 20200511-10:14:23] epoch: 0, batch: 462 train-loss: 1.810199499130249\n",
      "[LOG 20200511-10:14:23] epoch: 0, batch: 463 train-loss: 2.3791966438293457\n",
      "[LOG 20200511-10:14:24] epoch: 0, batch: 464 train-loss: 2.5844802856445312\n",
      "[LOG 20200511-10:14:24] epoch: 0, batch: 465 train-loss: 1.6450462341308594\n",
      "[LOG 20200511-10:14:24] epoch: 0, batch: 466 train-loss: 1.8652642965316772\n",
      "[LOG 20200511-10:14:24] epoch: 0, batch: 467 train-loss: 2.133540153503418\n",
      "[LOG 20200511-10:14:24] epoch: 0, batch: 468 train-loss: 1.3577454090118408\n",
      "[LOG 20200511-10:14:24] epoch: 0, batch: 469 train-loss: 3.002570152282715\n",
      "[LOG 20200511-10:14:24] epoch: 0, batch: 470 train-loss: 2.076063394546509\n",
      "[LOG 20200511-10:14:24] epoch: 0, batch: 471 train-loss: 2.145773410797119\n",
      "[LOG 20200511-10:14:24] epoch: 0, batch: 472 train-loss: 1.941698431968689\n",
      "[LOG 20200511-10:14:24] epoch: 0, batch: 473 train-loss: 1.6660269498825073\n",
      "[LOG 20200511-10:14:25] epoch: 0, batch: 474 train-loss: 1.5341932773590088\n",
      "[LOG 20200511-10:14:25] epoch: 0, batch: 475 train-loss: 2.040266990661621\n",
      "[LOG 20200511-10:14:25] epoch: 0, batch: 476 train-loss: 2.571869373321533\n",
      "[LOG 20200511-10:14:25] epoch: 0, batch: 477 train-loss: 1.849341869354248\n",
      "[LOG 20200511-10:14:25] epoch: 0, batch: 478 train-loss: 2.3074450492858887\n",
      "[LOG 20200511-10:14:25] epoch: 0, batch: 479 train-loss: 1.801561713218689\n",
      "[LOG 20200511-10:14:25] epoch: 0, batch: 480 train-loss: 1.5620665550231934\n",
      "[LOG 20200511-10:14:25] epoch: 0, batch: 481 train-loss: 1.5627553462982178\n",
      "[LOG 20200511-10:14:25] epoch: 0, batch: 482 train-loss: 1.4050451517105103\n",
      "[LOG 20200511-10:14:26] epoch: 0, batch: 483 train-loss: 2.4435715675354004\n",
      "[LOG 20200511-10:14:26] epoch: 0, batch: 484 train-loss: 1.5742188692092896\n",
      "[LOG 20200511-10:14:26] epoch: 0, batch: 485 train-loss: 2.0079126358032227\n",
      "[LOG 20200511-10:14:26] epoch: 0, batch: 486 train-loss: 2.1722772121429443\n",
      "[LOG 20200511-10:14:26] epoch: 0, batch: 487 train-loss: 1.7270162105560303\n",
      "[LOG 20200511-10:14:26] epoch: 0, batch: 488 train-loss: 1.821947693824768\n",
      "[LOG 20200511-10:14:26] epoch: 0, batch: 489 train-loss: 2.0459461212158203\n",
      "[LOG 20200511-10:14:26] epoch: 0, batch: 490 train-loss: 1.649533748626709\n",
      "[LOG 20200511-10:14:27] epoch: 0, batch: 491 train-loss: 1.875056266784668\n",
      "[LOG 20200511-10:14:27] epoch: 0, batch: 492 train-loss: 2.223409414291382\n",
      "[LOG 20200511-10:14:27] epoch: 0, batch: 493 train-loss: 2.0919227600097656\n",
      "[LOG 20200511-10:14:27] epoch: 0, batch: 494 train-loss: 1.9887025356292725\n",
      "[LOG 20200511-10:14:27] epoch: 0, batch: 495 train-loss: 1.390883445739746\n",
      "[LOG 20200511-10:14:27] epoch: 0, batch: 496 train-loss: 1.8302030563354492\n",
      "[LOG 20200511-10:14:27] epoch: 0, batch: 497 train-loss: 1.604060173034668\n",
      "[LOG 20200511-10:14:27] epoch: 0, batch: 498 train-loss: 1.7956351041793823\n",
      "[LOG 20200511-10:14:28] epoch: 0, batch: 499 train-loss: 1.7457551956176758\n",
      "[LOG 20200511-10:14:28] epoch: 0, batch: 500 train-loss: 2.918290376663208\n",
      "[LOG 20200511-10:14:28] epoch: 0, batch: 501 train-loss: 1.6961307525634766\n",
      "[LOG 20200511-10:14:28] epoch: 0, batch: 502 train-loss: 1.07929527759552\n",
      "[LOG 20200511-10:14:28] epoch: 0, batch: 503 train-loss: 1.254838228225708\n",
      "[LOG 20200511-10:14:28] epoch: 0, batch: 504 train-loss: 1.6326920986175537\n",
      "[LOG 20200511-10:14:28] epoch: 0, batch: 505 train-loss: 1.872347116470337\n",
      "[LOG 20200511-10:14:28] epoch: 0, batch: 506 train-loss: 1.947658896446228\n",
      "[LOG 20200511-10:14:29] epoch: 0, batch: 507 train-loss: 1.7941501140594482\n",
      "[LOG 20200511-10:14:29] epoch: 0, batch: 508 train-loss: 1.5833752155303955\n",
      "[LOG 20200511-10:14:29] epoch: 0, batch: 509 train-loss: 2.03252911567688\n",
      "[LOG 20200511-10:14:29] epoch: 0, batch: 510 train-loss: 1.6655848026275635\n",
      "[LOG 20200511-10:14:29] epoch: 0, batch: 511 train-loss: 1.558103322982788\n",
      "[LOG 20200511-10:14:29] epoch: 0, batch: 512 train-loss: 1.9650017023086548\n",
      "[LOG 20200511-10:14:29] epoch: 0, batch: 513 train-loss: 2.1976447105407715\n",
      "[LOG 20200511-10:14:29] epoch: 0, batch: 514 train-loss: 2.1870346069335938\n",
      "[LOG 20200511-10:14:29] epoch: 0, batch: 515 train-loss: 1.9460740089416504\n",
      "[LOG 20200511-10:14:30] epoch: 0, batch: 516 train-loss: 2.1109578609466553\n",
      "[LOG 20200511-10:14:30] epoch: 0, batch: 517 train-loss: 1.5186091661453247\n",
      "[LOG 20200511-10:14:30] epoch: 0, batch: 518 train-loss: 2.299145221710205\n",
      "[LOG 20200511-10:14:30] epoch: 0, batch: 519 train-loss: 2.425870418548584\n",
      "[LOG 20200511-10:14:30] epoch: 0, batch: 520 train-loss: 2.098745822906494\n",
      "[LOG 20200511-10:14:30] epoch: 0, batch: 521 train-loss: 2.631206512451172\n",
      "[LOG 20200511-10:14:30] epoch: 0, batch: 522 train-loss: 1.7908167839050293\n",
      "[LOG 20200511-10:14:30] epoch: 0, batch: 523 train-loss: 1.616614818572998\n",
      "[LOG 20200511-10:14:30] epoch: 0, batch: 524 train-loss: 1.9861948490142822\n",
      "[LOG 20200511-10:14:31] epoch: 0, batch: 525 train-loss: 1.6792913675308228\n",
      "[LOG 20200511-10:14:31] epoch: 0, batch: 526 train-loss: 1.9248661994934082\n",
      "[LOG 20200511-10:14:31] epoch: 0, batch: 527 train-loss: 1.665278673171997\n",
      "[LOG 20200511-10:14:31] epoch: 0, batch: 528 train-loss: 1.9918673038482666\n",
      "[LOG 20200511-10:14:31] epoch: 0, batch: 529 train-loss: 1.5609328746795654\n",
      "[LOG 20200511-10:14:31] epoch: 0, batch: 530 train-loss: 1.678853988647461\n",
      "[LOG 20200511-10:14:31] epoch: 0, batch: 531 train-loss: 2.0053398609161377\n",
      "[LOG 20200511-10:14:31] epoch: 0, batch: 532 train-loss: 1.7079516649246216\n",
      "[LOG 20200511-10:14:32] epoch: 0, batch: 533 train-loss: 1.8668524026870728\n",
      "[LOG 20200511-10:14:32] epoch: 0, batch: 534 train-loss: 1.5387282371520996\n",
      "[LOG 20200511-10:14:32] epoch: 0, batch: 535 train-loss: 1.8613855838775635\n",
      "[LOG 20200511-10:14:32] epoch: 0, batch: 536 train-loss: 2.255192279815674\n",
      "[LOG 20200511-10:14:32] epoch: 0, batch: 537 train-loss: 1.1613638401031494\n",
      "[LOG 20200511-10:14:32] epoch: 0, batch: 538 train-loss: 1.9463387727737427\n",
      "[LOG 20200511-10:14:32] epoch: 0, batch: 539 train-loss: 2.160249710083008\n",
      "[LOG 20200511-10:14:32] epoch: 0, batch: 540 train-loss: 1.879517912864685\n",
      "[LOG 20200511-10:14:32] epoch: 0, batch: 541 train-loss: 1.747141718864441\n",
      "[LOG 20200511-10:14:33] epoch: 0, batch: 542 train-loss: 1.8612444400787354\n",
      "[LOG 20200511-10:14:33] epoch: 0, batch: 543 train-loss: 2.966949939727783\n",
      "[LOG 20200511-10:14:33] epoch: 0, batch: 544 train-loss: 1.9363417625427246\n",
      "[LOG 20200511-10:14:33] epoch: 0, batch: 545 train-loss: 1.4321707487106323\n",
      "[LOG 20200511-10:14:33] epoch: 0, batch: 546 train-loss: 2.342660427093506\n",
      "[LOG 20200511-10:14:33] epoch: 0, batch: 547 train-loss: 1.8279345035552979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:14:33] epoch: 0, batch: 548 train-loss: 1.5949275493621826\n",
      "[LOG 20200511-10:14:33] epoch: 0, batch: 549 train-loss: 1.8762493133544922\n",
      "[LOG 20200511-10:14:34] epoch: 0, batch: 550 train-loss: 1.861290454864502\n",
      "[LOG 20200511-10:14:34] epoch: 0, batch: 551 train-loss: 1.8065348863601685\n",
      "[LOG 20200511-10:14:34] epoch: 0, batch: 552 train-loss: 1.8836753368377686\n",
      "[LOG 20200511-10:14:34] epoch: 0, batch: 553 train-loss: 1.7727255821228027\n",
      "[LOG 20200511-10:14:34] epoch: 0, batch: 554 train-loss: 1.509321689605713\n",
      "[LOG 20200511-10:14:34] epoch: 0, batch: 555 train-loss: 1.2948853969573975\n",
      "[LOG 20200511-10:14:34] epoch: 0, batch: 556 train-loss: 1.9343202114105225\n",
      "[LOG 20200511-10:14:34] epoch: 0, batch: 557 train-loss: 1.7124494314193726\n",
      "[LOG 20200511-10:14:34] epoch: 0, batch: 558 train-loss: 1.314255952835083\n",
      "[LOG 20200511-10:14:35] epoch: 0, batch: 559 train-loss: 1.8049910068511963\n",
      "[LOG 20200511-10:14:35] epoch: 0, batch: 560 train-loss: 1.9842805862426758\n",
      "[LOG 20200511-10:14:35] epoch: 0, batch: 561 train-loss: 1.3410587310791016\n",
      "[LOG 20200511-10:14:35] epoch: 0, batch: 562 train-loss: 1.429704189300537\n",
      "[LOG 20200511-10:14:35] epoch: 0, batch: 563 train-loss: 1.8584010601043701\n",
      "[LOG 20200511-10:14:35] epoch: 0, batch: 564 train-loss: 2.5980443954467773\n",
      "[LOG 20200511-10:14:35] epoch: 0, batch: 565 train-loss: 1.9292041063308716\n",
      "[LOG 20200511-10:14:35] epoch: 0, batch: 566 train-loss: 1.7992697954177856\n",
      "[LOG 20200511-10:14:35] epoch: 0, batch: 567 train-loss: 2.6007094383239746\n",
      "[LOG 20200511-10:14:36] epoch: 0, batch: 568 train-loss: 1.5639142990112305\n",
      "[LOG 20200511-10:14:36] epoch: 0, batch: 569 train-loss: 2.2225756645202637\n",
      "[LOG 20200511-10:14:36] epoch: 0, batch: 570 train-loss: 1.7072502374649048\n",
      "[LOG 20200511-10:14:36] epoch: 0, batch: 571 train-loss: 1.45793616771698\n",
      "[LOG 20200511-10:14:36] epoch: 0, batch: 572 train-loss: 2.1466944217681885\n",
      "[LOG 20200511-10:14:36] epoch: 0, batch: 573 train-loss: 2.460844039916992\n",
      "[LOG 20200511-10:14:36] epoch: 0, batch: 574 train-loss: 1.8901749849319458\n",
      "[LOG 20200511-10:14:36] epoch: 0, batch: 575 train-loss: 1.8300775289535522\n",
      "[LOG 20200511-10:14:36] epoch: 0, batch: 576 train-loss: 1.2008166313171387\n",
      "[LOG 20200511-10:14:37] epoch: 0, batch: 577 train-loss: 2.2335848808288574\n",
      "[LOG 20200511-10:14:37] epoch: 0, batch: 578 train-loss: 2.18074107170105\n",
      "[LOG 20200511-10:14:37] epoch: 0, batch: 579 train-loss: 1.8857582807540894\n",
      "[LOG 20200511-10:14:37] epoch: 0, batch: 580 train-loss: 1.1918760538101196\n",
      "[LOG 20200511-10:14:37] epoch: 0, batch: 581 train-loss: 1.5254576206207275\n",
      "[LOG 20200511-10:14:37] epoch: 0, batch: 582 train-loss: 2.1382012367248535\n",
      "[LOG 20200511-10:14:37] epoch: 0, batch: 583 train-loss: 1.7013041973114014\n",
      "[LOG 20200511-10:14:37] epoch: 0, batch: 584 train-loss: 2.1668972969055176\n",
      "[LOG 20200511-10:14:38] epoch: 0, batch: 585 train-loss: 1.5107001066207886\n",
      "[LOG 20200511-10:14:38] epoch: 0, batch: 586 train-loss: 1.9387884140014648\n",
      "[LOG 20200511-10:14:38] epoch: 0, batch: 587 train-loss: 1.9282492399215698\n",
      "[LOG 20200511-10:14:38] epoch: 0, batch: 588 train-loss: 1.6561166048049927\n",
      "[LOG 20200511-10:14:38] epoch: 0, batch: 589 train-loss: 2.0739216804504395\n",
      "[LOG 20200511-10:14:38] epoch: 0, batch: 590 train-loss: 1.0768656730651855\n",
      "[LOG 20200511-10:14:38] epoch: 0, batch: 591 train-loss: 2.198579788208008\n",
      "[LOG 20200511-10:14:38] epoch: 0, batch: 592 train-loss: 1.5963389873504639\n",
      "[LOG 20200511-10:14:38] epoch: 0, batch: 593 train-loss: 1.0438354015350342\n",
      "[LOG 20200511-10:14:39] epoch: 0, batch: 594 train-loss: 2.79044771194458\n",
      "[LOG 20200511-10:14:39] epoch: 0, batch: 595 train-loss: 1.6228307485580444\n",
      "[LOG 20200511-10:14:39] epoch: 0, batch: 596 train-loss: 1.9914257526397705\n",
      "[LOG 20200511-10:14:39] epoch: 0, batch: 597 train-loss: 2.429621696472168\n",
      "[LOG 20200511-10:14:39] epoch: 0, batch: 598 train-loss: 2.5133752822875977\n",
      "[LOG 20200511-10:14:39] epoch: 0, batch: 599 train-loss: 1.6466667652130127\n",
      "[LOG 20200511-10:14:39] epoch: 0, batch: 600 train-loss: 2.018026113510132\n",
      "[LOG 20200511-10:14:39] epoch: 0, batch: 601 train-loss: 2.0665383338928223\n",
      "[LOG 20200511-10:14:39] epoch: 0, batch: 602 train-loss: 1.5955818891525269\n",
      "[LOG 20200511-10:14:39] epoch: 0, batch: 603 train-loss: 1.6757628917694092\n",
      "[LOG 20200511-10:14:40] epoch: 0, batch: 604 train-loss: 1.8381178379058838\n",
      "[LOG 20200511-10:14:40] epoch: 0, batch: 605 train-loss: 1.749883770942688\n",
      "[LOG 20200511-10:14:40] epoch: 0, batch: 606 train-loss: 1.8340023756027222\n",
      "[LOG 20200511-10:14:40] epoch: 0, batch: 607 train-loss: 1.3996833562850952\n",
      "[LOG 20200511-10:14:40] epoch: 0, batch: 608 train-loss: 2.162899971008301\n",
      "[LOG 20200511-10:14:40] epoch: 0, batch: 609 train-loss: 1.8254458904266357\n",
      "[LOG 20200511-10:14:40] epoch: 0, batch: 610 train-loss: 1.8137866258621216\n",
      "[LOG 20200511-10:14:40] epoch: 0, batch: 611 train-loss: 2.1794447898864746\n",
      "[LOG 20200511-10:14:40] epoch: 0, batch: 612 train-loss: 1.6021000146865845\n",
      "[LOG 20200511-10:14:41] epoch: 0, batch: 613 train-loss: 1.5187883377075195\n",
      "[LOG 20200511-10:14:41] epoch: 0, batch: 614 train-loss: 1.6432554721832275\n",
      "[LOG 20200511-10:14:41] epoch: 0, batch: 615 train-loss: 1.6682367324829102\n",
      "[LOG 20200511-10:14:41] epoch: 0, batch: 616 train-loss: 2.2403175830841064\n",
      "[LOG 20200511-10:14:41] epoch: 0, batch: 617 train-loss: 1.9911282062530518\n",
      "[LOG 20200511-10:14:41] epoch: 0, batch: 618 train-loss: 1.8717155456542969\n",
      "[LOG 20200511-10:14:41] epoch: 0, batch: 619 train-loss: 1.5504754781723022\n",
      "[LOG 20200511-10:14:41] epoch: 0, batch: 620 train-loss: 1.9004521369934082\n",
      "[LOG 20200511-10:14:41] epoch: 0, batch: 621 train-loss: 1.4588640928268433\n",
      "[LOG 20200511-10:14:42] epoch: 0, batch: 622 train-loss: 2.0086171627044678\n",
      "[LOG 20200511-10:14:42] epoch: 0, batch: 623 train-loss: 1.8888019323349\n",
      "[LOG 20200511-10:14:42] epoch: 0, batch: 624 train-loss: 2.328749656677246\n",
      "[LOG 20200511-10:14:42] epoch: 0, batch: 625 train-loss: 2.9702348709106445\n",
      "[LOG 20200511-10:14:42] epoch: 0, batch: 626 train-loss: 2.338597536087036\n",
      "[LOG 20200511-10:14:42] epoch: 0, batch: 627 train-loss: 1.5589613914489746\n",
      "[LOG 20200511-10:14:42] epoch: 0, batch: 628 train-loss: 2.2898459434509277\n",
      "[LOG 20200511-10:14:42] epoch: 0, batch: 629 train-loss: 2.1726019382476807\n",
      "[LOG 20200511-10:14:42] epoch: 0, batch: 630 train-loss: 1.6026862859725952\n",
      "[LOG 20200511-10:14:43] epoch: 0, batch: 631 train-loss: 1.3822603225708008\n",
      "[LOG 20200511-10:14:43] epoch: 0, batch: 632 train-loss: 2.296454906463623\n",
      "[LOG 20200511-10:14:43] epoch: 0, batch: 633 train-loss: 1.6103334426879883\n",
      "[LOG 20200511-10:14:43] epoch: 0, batch: 634 train-loss: 2.101642370223999\n",
      "[LOG 20200511-10:14:43] epoch: 0, batch: 635 train-loss: 2.708526372909546\n",
      "[LOG 20200511-10:14:43] epoch: 0, batch: 636 train-loss: 1.96976637840271\n",
      "[LOG 20200511-10:14:43] epoch: 0, batch: 637 train-loss: 2.1615960597991943\n",
      "[LOG 20200511-10:14:43] epoch: 0, batch: 638 train-loss: 1.1612884998321533\n",
      "[LOG 20200511-10:14:43] epoch: 0, batch: 639 train-loss: 1.5796515941619873\n",
      "[LOG 20200511-10:14:44] epoch: 0, batch: 640 train-loss: 1.5044782161712646\n",
      "[LOG 20200511-10:14:44] epoch: 0, batch: 641 train-loss: 1.1114833354949951\n",
      "[LOG 20200511-10:14:44] epoch: 0, batch: 642 train-loss: 1.6701900959014893\n",
      "[LOG 20200511-10:14:44] epoch: 0, batch: 643 train-loss: 2.005892753601074\n",
      "[LOG 20200511-10:14:44] epoch: 0, batch: 644 train-loss: 1.794507384300232\n",
      "[LOG 20200511-10:14:44] epoch: 0, batch: 645 train-loss: 1.9363877773284912\n",
      "[LOG 20200511-10:14:44] epoch: 0, batch: 646 train-loss: 2.1427431106567383\n",
      "[LOG 20200511-10:14:44] epoch: 0, batch: 647 train-loss: 1.6564255952835083\n",
      "[LOG 20200511-10:14:44] epoch: 0, batch: 648 train-loss: 2.436995267868042\n",
      "[LOG 20200511-10:14:45] epoch: 0, batch: 649 train-loss: 1.67655348777771\n",
      "[LOG 20200511-10:14:45] epoch: 0, batch: 650 train-loss: 1.371086835861206\n",
      "[LOG 20200511-10:14:45] epoch: 0, batch: 651 train-loss: 1.4840288162231445\n",
      "[LOG 20200511-10:14:45] epoch: 0, batch: 652 train-loss: 1.8611352443695068\n",
      "[LOG 20200511-10:14:45] epoch: 0, batch: 653 train-loss: 1.329593539237976\n",
      "[LOG 20200511-10:14:45] epoch: 0, batch: 654 train-loss: 1.678187608718872\n",
      "[LOG 20200511-10:14:45] epoch: 0, batch: 655 train-loss: 1.447746753692627\n",
      "[LOG 20200511-10:14:45] epoch: 0, batch: 656 train-loss: 1.6393054723739624\n",
      "[LOG 20200511-10:14:46] epoch: 0, batch: 657 train-loss: 1.5291157960891724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:14:46] epoch: 0, batch: 658 train-loss: 1.9386107921600342\n",
      "[LOG 20200511-10:14:46] epoch: 0, batch: 659 train-loss: 1.8448232412338257\n",
      "[LOG 20200511-10:14:46] epoch: 0, batch: 660 train-loss: 2.6416406631469727\n",
      "[LOG 20200511-10:14:46] epoch: 0, batch: 661 train-loss: 1.538933515548706\n",
      "[LOG 20200511-10:14:46] epoch: 0, batch: 662 train-loss: 2.120020866394043\n",
      "[LOG 20200511-10:14:46] epoch: 0, batch: 663 train-loss: 2.257188320159912\n",
      "[LOG 20200511-10:14:46] epoch: 0, batch: 664 train-loss: 1.3965742588043213\n",
      "[LOG 20200511-10:14:47] epoch: 0, batch: 665 train-loss: 2.5094027519226074\n",
      "[LOG 20200511-10:14:47] epoch: 0, batch: 666 train-loss: 1.5766276121139526\n",
      "[LOG 20200511-10:14:47] epoch: 0, batch: 667 train-loss: 1.2872498035430908\n",
      "[LOG 20200511-10:14:47] epoch: 0, batch: 668 train-loss: 1.9595141410827637\n",
      "[LOG 20200511-10:14:47] epoch: 0, batch: 669 train-loss: 2.422837257385254\n",
      "[LOG 20200511-10:14:47] epoch: 0, batch: 670 train-loss: 1.343755841255188\n",
      "[LOG 20200511-10:14:47] epoch: 0, batch: 671 train-loss: 1.3456995487213135\n",
      "[LOG 20200511-10:14:47] epoch: 0, batch: 672 train-loss: 1.3720176219940186\n",
      "[LOG 20200511-10:14:47] epoch: 0, batch: 673 train-loss: 1.0901200771331787\n",
      "[LOG 20200511-10:14:48] epoch: 0, batch: 674 train-loss: 1.7310543060302734\n",
      "[LOG 20200511-10:14:48] epoch: 0, batch: 675 train-loss: 1.7437807321548462\n",
      "[LOG 20200511-10:14:48] epoch: 0, batch: 676 train-loss: 1.5259310007095337\n",
      "[LOG 20200511-10:14:48] epoch: 0, batch: 677 train-loss: 2.028765916824341\n",
      "[LOG 20200511-10:14:48] epoch: 0, batch: 678 train-loss: 1.7152352333068848\n",
      "[LOG 20200511-10:14:48] epoch: 0, batch: 679 train-loss: 1.6921480894088745\n",
      "[LOG 20200511-10:14:48] epoch: 0, batch: 680 train-loss: 1.9608441591262817\n",
      "[LOG 20200511-10:14:48] epoch: 0, batch: 681 train-loss: 1.5277612209320068\n",
      "[LOG 20200511-10:14:49] epoch: 0, batch: 682 train-loss: 3.179856300354004\n",
      "[LOG 20200511-10:14:49] epoch: 0, batch: 683 train-loss: 1.6813411712646484\n",
      "[LOG 20200511-10:14:49] epoch: 0, batch: 684 train-loss: 1.8596405982971191\n",
      "[LOG 20200511-10:14:49] epoch: 0, batch: 685 train-loss: 2.4582648277282715\n",
      "[LOG 20200511-10:14:49] epoch: 0, batch: 686 train-loss: 1.8449609279632568\n",
      "[LOG 20200511-10:14:49] epoch: 0, batch: 687 train-loss: 1.9465994834899902\n",
      "[LOG 20200511-10:14:49] epoch: 0, batch: 688 train-loss: 2.084705352783203\n",
      "[LOG 20200511-10:14:49] epoch: 0, batch: 689 train-loss: 1.3344539403915405\n",
      "[LOG 20200511-10:14:50] epoch: 0, batch: 690 train-loss: 1.8312932252883911\n",
      "[LOG 20200511-10:14:50] epoch: 0, batch: 691 train-loss: 1.569227695465088\n",
      "[LOG 20200511-10:14:50] epoch: 0, batch: 692 train-loss: 2.6760411262512207\n",
      "[LOG 20200511-10:14:50] epoch: 0, batch: 693 train-loss: 1.851279854774475\n",
      "[LOG 20200511-10:14:50] epoch: 0, batch: 694 train-loss: 2.6358187198638916\n",
      "[LOG 20200511-10:14:50] epoch: 0, batch: 695 train-loss: 1.4852068424224854\n",
      "[LOG 20200511-10:14:50] epoch: 0, batch: 696 train-loss: 2.3553266525268555\n",
      "[LOG 20200511-10:14:50] epoch: 0, batch: 697 train-loss: 1.8094496726989746\n",
      "[LOG 20200511-10:14:51] epoch: 0, batch: 698 train-loss: 2.321005344390869\n",
      "[LOG 20200511-10:14:51] epoch: 0, batch: 699 train-loss: 0.9095591306686401\n",
      "[LOG 20200511-10:14:51] epoch: 0, batch: 700 train-loss: 1.785348892211914\n",
      "[LOG 20200511-10:14:51] epoch: 0, batch: 701 train-loss: 1.2785637378692627\n",
      "[LOG 20200511-10:14:51] epoch: 0, batch: 702 train-loss: 2.550248622894287\n",
      "[LOG 20200511-10:14:51] epoch: 0, batch: 703 train-loss: 1.7333370447158813\n",
      "[LOG 20200511-10:14:51] epoch: 0, batch: 704 train-loss: 1.9748055934906006\n",
      "[LOG 20200511-10:14:52] epoch: 0, batch: 705 train-loss: 1.8631536960601807\n",
      "[LOG 20200511-10:14:52] epoch: 0, batch: 706 train-loss: 1.8110313415527344\n",
      "[LOG 20200511-10:14:52] epoch: 0, batch: 707 train-loss: 2.1142570972442627\n",
      "[LOG 20200511-10:14:52] epoch: 0, batch: 708 train-loss: 2.3656084537506104\n",
      "[LOG 20200511-10:14:52] epoch: 0, batch: 709 train-loss: 1.7664417028427124\n",
      "[LOG 20200511-10:14:52] epoch: 0, batch: 710 train-loss: 2.13348126411438\n",
      "[LOG 20200511-10:14:52] epoch: 0, batch: 711 train-loss: 1.5677088499069214\n",
      "[LOG 20200511-10:14:52] epoch: 0, batch: 712 train-loss: 2.2947332859039307\n",
      "[LOG 20200511-10:14:52] epoch: 0, batch: 713 train-loss: 2.493335247039795\n",
      "[LOG 20200511-10:14:53] epoch: 0, batch: 714 train-loss: 1.7423198223114014\n",
      "[LOG 20200511-10:14:53] epoch: 0, batch: 715 train-loss: 1.458387017250061\n",
      "[LOG 20200511-10:14:53] epoch: 0, batch: 716 train-loss: 2.091336250305176\n",
      "[LOG 20200511-10:14:53] epoch: 0, batch: 717 train-loss: 1.4580702781677246\n",
      "[LOG 20200511-10:14:53] epoch: 0, batch: 718 train-loss: 1.2897701263427734\n",
      "[LOG 20200511-10:14:53] epoch: 0, batch: 719 train-loss: 2.583369255065918\n",
      "[LOG 20200511-10:14:53] epoch: 0, batch: 720 train-loss: 1.4333455562591553\n",
      "[LOG 20200511-10:14:53] epoch: 0, batch: 721 train-loss: 2.5521183013916016\n",
      "[LOG 20200511-10:14:54] epoch: 0, batch: 722 train-loss: 1.984631896018982\n",
      "[LOG 20200511-10:14:54] epoch: 0, batch: 723 train-loss: 1.3901758193969727\n",
      "[LOG 20200511-10:14:54] epoch: 0, batch: 724 train-loss: 2.033202648162842\n",
      "[LOG 20200511-10:14:54] epoch: 0, batch: 725 train-loss: 1.8267802000045776\n",
      "[LOG 20200511-10:14:54] epoch: 0, batch: 726 train-loss: 3.296614170074463\n",
      "[LOG 20200511-10:14:54] epoch: 0, batch: 727 train-loss: 1.8163461685180664\n",
      "[LOG 20200511-10:14:54] epoch: 0, batch: 728 train-loss: 1.778133511543274\n",
      "[LOG 20200511-10:14:54] epoch: 0, batch: 729 train-loss: 1.4359350204467773\n",
      "[LOG 20200511-10:14:55] epoch: 0, batch: 730 train-loss: 1.8641197681427002\n",
      "[LOG 20200511-10:14:55] epoch: 0, batch: 731 train-loss: 1.3113442659378052\n",
      "[LOG 20200511-10:14:55] epoch: 0, batch: 732 train-loss: 1.925065040588379\n",
      "[LOG 20200511-10:14:55] epoch: 0, batch: 733 train-loss: 1.7099480628967285\n",
      "[LOG 20200511-10:14:55] epoch: 0, batch: 734 train-loss: 2.190398693084717\n",
      "[LOG 20200511-10:14:55] epoch: 0, batch: 735 train-loss: 1.9265248775482178\n",
      "[LOG 20200511-10:14:55] epoch: 0, batch: 736 train-loss: 1.1515212059020996\n",
      "[LOG 20200511-10:14:55] epoch: 0, batch: 737 train-loss: 1.7360385656356812\n",
      "[LOG 20200511-10:14:55] epoch: 0, batch: 738 train-loss: 1.5403656959533691\n",
      "[LOG 20200511-10:14:56] epoch: 0, batch: 739 train-loss: 1.2398744821548462\n",
      "[LOG 20200511-10:14:56] epoch: 0, batch: 740 train-loss: 2.246718168258667\n",
      "[LOG 20200511-10:14:56] epoch: 0, batch: 741 train-loss: 1.889041781425476\n",
      "[LOG 20200511-10:14:56] epoch: 0, batch: 742 train-loss: 2.1022915840148926\n",
      "[LOG 20200511-10:14:56] epoch: 0, batch: 743 train-loss: 1.463352918624878\n",
      "[LOG 20200511-10:14:56] epoch: 0, batch: 744 train-loss: 2.4341259002685547\n",
      "[LOG 20200511-10:14:56] epoch: 0, batch: 745 train-loss: 1.5333163738250732\n",
      "[LOG 20200511-10:14:56] epoch: 0, batch: 746 train-loss: 2.2186970710754395\n",
      "[LOG 20200511-10:14:56] epoch: 0, batch: 747 train-loss: 2.0564775466918945\n",
      "[LOG 20200511-10:14:57] epoch: 0, batch: 748 train-loss: 2.1327319145202637\n",
      "[LOG 20200511-10:14:57] epoch: 0, batch: 749 train-loss: 2.4417238235473633\n",
      "[LOG 20200511-10:14:57] epoch: 0, batch: 750 train-loss: 1.8271719217300415\n",
      "[LOG 20200511-10:14:57] epoch: 0, batch: 751 train-loss: 1.5742409229278564\n",
      "[LOG 20200511-10:14:57] epoch: 0, batch: 752 train-loss: 2.219041347503662\n",
      "[LOG 20200511-10:14:57] epoch: 0, batch: 753 train-loss: 2.0013904571533203\n",
      "[LOG 20200511-10:14:57] epoch: 0, batch: 754 train-loss: 1.6601399183273315\n",
      "[LOG 20200511-10:14:57] epoch: 0, batch: 755 train-loss: 1.814595341682434\n",
      "[LOG 20200511-10:14:57] epoch: 0, batch: 756 train-loss: 1.8629097938537598\n",
      "[LOG 20200511-10:14:58] epoch: 0, batch: 757 train-loss: 1.60277259349823\n",
      "[LOG 20200511-10:14:58] epoch: 0, batch: 758 train-loss: 2.1409335136413574\n",
      "[LOG 20200511-10:14:58] epoch: 0, batch: 759 train-loss: 1.1698637008666992\n",
      "[LOG 20200511-10:14:58] epoch: 0, batch: 760 train-loss: 1.5830382108688354\n",
      "[LOG 20200511-10:14:58] epoch: 0, batch: 761 train-loss: 2.269063711166382\n",
      "[LOG 20200511-10:14:58] epoch: 0, batch: 762 train-loss: 1.4129904508590698\n",
      "[LOG 20200511-10:14:58] epoch: 0, batch: 763 train-loss: 1.237692952156067\n",
      "[LOG 20200511-10:14:58] epoch: 0, batch: 764 train-loss: 2.9859180450439453\n",
      "[LOG 20200511-10:14:58] epoch: 0, batch: 765 train-loss: 1.351820707321167\n",
      "[LOG 20200511-10:14:59] epoch: 0, batch: 766 train-loss: 1.509455680847168\n",
      "[LOG 20200511-10:14:59] epoch: 0, batch: 767 train-loss: 1.7573230266571045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:14:59] epoch: 0, batch: 768 train-loss: 1.616087794303894\n",
      "[LOG 20200511-10:14:59] epoch: 0, batch: 769 train-loss: 1.4490056037902832\n",
      "[LOG 20200511-10:14:59] epoch: 0, batch: 770 train-loss: 1.710770845413208\n",
      "[LOG 20200511-10:14:59] epoch: 0, batch: 771 train-loss: 1.9040043354034424\n",
      "[LOG 20200511-10:14:59] epoch: 0, batch: 772 train-loss: 1.6419583559036255\n",
      "[LOG 20200511-10:14:59] epoch: 0, batch: 773 train-loss: 1.6220827102661133\n",
      "[LOG 20200511-10:14:59] epoch: 0, batch: 774 train-loss: 1.5998204946517944\n",
      "[LOG 20200511-10:15:00] epoch: 0, batch: 775 train-loss: 1.3425610065460205\n",
      "[LOG 20200511-10:15:00] epoch: 0, batch: 776 train-loss: 1.1658967733383179\n",
      "[LOG 20200511-10:15:00] epoch: 0, batch: 777 train-loss: 1.8168822526931763\n",
      "[LOG 20200511-10:15:00] epoch: 0, batch: 778 train-loss: 2.041912078857422\n",
      "[LOG 20200511-10:15:00] epoch: 0, batch: 779 train-loss: 1.5753843784332275\n",
      "[LOG 20200511-10:15:00] epoch: 0, batch: 780 train-loss: 2.54628586769104\n",
      "[LOG 20200511-10:15:00] epoch: 0, batch: 781 train-loss: 1.5081508159637451\n",
      "[LOG 20200511-10:15:00] epoch: 0, batch: 782 train-loss: 2.1633358001708984\n",
      "[LOG 20200511-10:15:00] epoch: 0, batch: 783 train-loss: 1.830838680267334\n",
      "[LOG 20200511-10:15:01] epoch: 0, batch: 784 train-loss: 1.740767002105713\n",
      "[LOG 20200511-10:15:01] epoch: 0, batch: 785 train-loss: 1.8383806943893433\n",
      "[LOG 20200511-10:15:01] epoch: 0, batch: 786 train-loss: 2.0782155990600586\n",
      "[LOG 20200511-10:15:01] epoch: 0, batch: 787 train-loss: 2.408191204071045\n",
      "[LOG 20200511-10:15:01] epoch: 0, batch: 788 train-loss: 1.7077605724334717\n",
      "[LOG 20200511-10:15:01] epoch: 0, batch: 789 train-loss: 1.9435468912124634\n",
      "[LOG 20200511-10:15:01] epoch: 0, batch: 790 train-loss: 1.6737841367721558\n",
      "[LOG 20200511-10:15:01] epoch: 0, batch: 791 train-loss: 1.8433952331542969\n",
      "[LOG 20200511-10:15:01] epoch: 0, batch: 792 train-loss: 1.9969919919967651\n",
      "[LOG 20200511-10:15:02] epoch: 0, batch: 793 train-loss: 1.4473212957382202\n",
      "[LOG 20200511-10:15:02] epoch: 0, batch: 794 train-loss: 1.2769612073898315\n",
      "[LOG 20200511-10:15:02] epoch: 0, batch: 795 train-loss: 2.5384602546691895\n",
      "[LOG 20200511-10:15:02] epoch: 0, batch: 796 train-loss: 1.9343030452728271\n",
      "[LOG 20200511-10:15:02] epoch: 0, batch: 797 train-loss: 1.4796836376190186\n",
      "[LOG 20200511-10:15:02] epoch: 0, batch: 798 train-loss: 1.4288671016693115\n",
      "[LOG 20200511-10:15:02] epoch: 0, batch: 799 train-loss: 1.1024314165115356\n",
      "[LOG 20200511-10:15:02] epoch: 0, batch: 800 train-loss: 1.7968648672103882\n",
      "[LOG 20200511-10:15:02] epoch: 0, batch: 801 train-loss: 1.7106236219406128\n",
      "[LOG 20200511-10:15:02] epoch: 0, batch: 802 train-loss: 2.13791823387146\n",
      "[LOG 20200511-10:15:03] epoch: 0, batch: 803 train-loss: 1.2242605686187744\n",
      "[LOG 20200511-10:15:03] epoch: 0, batch: 804 train-loss: 2.153144598007202\n",
      "[LOG 20200511-10:15:03] epoch: 0, batch: 805 train-loss: 1.7300519943237305\n",
      "[LOG 20200511-10:15:03] epoch: 0, batch: 806 train-loss: 1.4506464004516602\n",
      "[LOG 20200511-10:15:03] epoch: 0, batch: 807 train-loss: 2.0229744911193848\n",
      "[LOG 20200511-10:15:03] epoch: 0, batch: 808 train-loss: 1.4846792221069336\n",
      "[LOG 20200511-10:15:03] epoch: 0, batch: 809 train-loss: 2.597137689590454\n",
      "[LOG 20200511-10:15:03] epoch: 0, batch: 810 train-loss: 1.7743608951568604\n",
      "[LOG 20200511-10:15:03] epoch: 0, batch: 811 train-loss: 1.5212035179138184\n",
      "[LOG 20200511-10:15:04] epoch: 0, batch: 812 train-loss: 1.4010828733444214\n",
      "[LOG 20200511-10:15:04] epoch: 0, batch: 813 train-loss: 2.7457263469696045\n",
      "[LOG 20200511-10:15:04] epoch: 0, batch: 814 train-loss: 2.2706408500671387\n",
      "[LOG 20200511-10:15:04] epoch: 0, batch: 815 train-loss: 1.8499789237976074\n",
      "[LOG 20200511-10:15:04] epoch: 0, batch: 816 train-loss: 1.731709599494934\n",
      "[LOG 20200511-10:15:04] epoch: 0, batch: 817 train-loss: 1.9482178688049316\n",
      "[LOG 20200511-10:15:04] epoch: 0, batch: 818 train-loss: 2.0638561248779297\n",
      "[LOG 20200511-10:15:04] epoch: 0, batch: 819 train-loss: 1.1369445323944092\n",
      "[LOG 20200511-10:15:04] epoch: 0, batch: 820 train-loss: 1.5545120239257812\n",
      "[LOG 20200511-10:15:05] epoch: 0, batch: 821 train-loss: 2.001039743423462\n",
      "[LOG 20200511-10:15:05] epoch: 0, batch: 822 train-loss: 1.6437864303588867\n",
      "[LOG 20200511-10:15:05] epoch: 0, batch: 823 train-loss: 2.5422418117523193\n",
      "[LOG 20200511-10:15:05] epoch: 0, batch: 824 train-loss: 2.272904634475708\n",
      "[LOG 20200511-10:15:05] epoch: 0, batch: 825 train-loss: 2.7238829135894775\n",
      "[LOG 20200511-10:15:05] epoch: 0, batch: 826 train-loss: 1.532089352607727\n",
      "[LOG 20200511-10:15:05] epoch: 0, batch: 827 train-loss: 1.6285854578018188\n",
      "[LOG 20200511-10:15:05] epoch: 0, batch: 828 train-loss: 1.4255414009094238\n",
      "[LOG 20200511-10:15:05] epoch: 0, batch: 829 train-loss: 2.303494453430176\n",
      "[LOG 20200511-10:15:06] epoch: 0, batch: 830 train-loss: 2.0456936359405518\n",
      "[LOG 20200511-10:15:06] epoch: 0, batch: 831 train-loss: 1.9308269023895264\n",
      "[LOG 20200511-10:15:06] epoch: 0, batch: 832 train-loss: 1.4031717777252197\n",
      "[LOG 20200511-10:15:06] epoch: 0, batch: 833 train-loss: 1.1428223848342896\n",
      "[LOG 20200511-10:15:06] epoch: 0, batch: 834 train-loss: 1.8057880401611328\n",
      "[LOG 20200511-10:15:06] epoch: 0, batch: 835 train-loss: 2.0450778007507324\n",
      "[LOG 20200511-10:15:06] epoch: 0, batch: 836 train-loss: 2.172480821609497\n",
      "[LOG 20200511-10:15:06] epoch: 0, batch: 837 train-loss: 2.079620599746704\n",
      "[LOG 20200511-10:15:06] epoch: 0, batch: 838 train-loss: 1.740256667137146\n",
      "[LOG 20200511-10:15:06] epoch: 0, batch: 839 train-loss: 1.6869900226593018\n",
      "[LOG 20200511-10:15:07] epoch: 0, batch: 840 train-loss: 1.1695374250411987\n",
      "[LOG 20200511-10:15:07] epoch: 0, batch: 841 train-loss: 0.9013036489486694\n",
      "[LOG 20200511-10:15:07] epoch: 0, batch: 842 train-loss: 1.8907995223999023\n",
      "[LOG 20200511-10:15:07] epoch: 0, batch: 843 train-loss: 1.4222441911697388\n",
      "[LOG 20200511-10:15:07] epoch: 0, batch: 844 train-loss: 2.2497527599334717\n",
      "[LOG 20200511-10:15:07] epoch: 0, batch: 845 train-loss: 1.7683913707733154\n",
      "[LOG 20200511-10:15:07] epoch: 0, batch: 846 train-loss: 2.413085460662842\n",
      "[LOG 20200511-10:15:08] epoch: 0, batch: 847 train-loss: 1.3565601110458374\n",
      "[LOG 20200511-10:15:08] epoch: 0, batch: 848 train-loss: 2.3674721717834473\n",
      "[LOG 20200511-10:15:08] epoch: 0, batch: 849 train-loss: 1.8626506328582764\n",
      "[LOG 20200511-10:15:08] epoch: 0, batch: 850 train-loss: 2.5816800594329834\n",
      "[LOG 20200511-10:15:08] epoch: 0, batch: 851 train-loss: 2.107517719268799\n",
      "[LOG 20200511-10:15:08] epoch: 0, batch: 852 train-loss: 1.4122315645217896\n",
      "[LOG 20200511-10:15:08] epoch: 0, batch: 853 train-loss: 1.7765281200408936\n",
      "[LOG 20200511-10:15:08] epoch: 0, batch: 854 train-loss: 1.411521553993225\n",
      "[LOG 20200511-10:15:08] epoch: 0, batch: 855 train-loss: 1.873679280281067\n",
      "[LOG 20200511-10:15:09] epoch: 0, batch: 856 train-loss: 1.8504180908203125\n",
      "[LOG 20200511-10:15:09] epoch: 0, batch: 857 train-loss: 2.182088613510132\n",
      "[LOG 20200511-10:15:09] epoch: 0, batch: 858 train-loss: 1.326253890991211\n",
      "[LOG 20200511-10:15:09] epoch: 0, batch: 859 train-loss: 2.2697548866271973\n",
      "[LOG 20200511-10:15:09] epoch: 0, batch: 860 train-loss: 1.7300024032592773\n",
      "[LOG 20200511-10:15:09] epoch: 0, batch: 861 train-loss: 1.8544495105743408\n",
      "[LOG 20200511-10:15:09] epoch: 0, batch: 862 train-loss: 1.885236382484436\n",
      "[LOG 20200511-10:15:09] epoch: 0, batch: 863 train-loss: 2.112668037414551\n",
      "[LOG 20200511-10:15:10] epoch: 0, batch: 864 train-loss: 0.7975824475288391\n",
      "[LOG 20200511-10:15:10] epoch: 0, batch: 865 train-loss: 1.7296793460845947\n",
      "[LOG 20200511-10:15:10] epoch: 0, batch: 866 train-loss: 1.5813583135604858\n",
      "[LOG 20200511-10:15:10] epoch: 0, batch: 867 train-loss: 1.6138989925384521\n",
      "[LOG 20200511-10:15:10] epoch: 0, batch: 868 train-loss: 2.093759059906006\n",
      "[LOG 20200511-10:15:10] epoch: 0, batch: 869 train-loss: 2.263418436050415\n",
      "[LOG 20200511-10:15:10] epoch: 0, batch: 870 train-loss: 2.855782985687256\n",
      "[LOG 20200511-10:15:10] epoch: 0, batch: 871 train-loss: 1.6986496448516846\n",
      "[LOG 20200511-10:15:10] epoch: 0, batch: 872 train-loss: 1.4233638048171997\n",
      "[LOG 20200511-10:15:11] epoch: 0, batch: 873 train-loss: 1.7197647094726562\n",
      "[LOG 20200511-10:15:11] epoch: 0, batch: 874 train-loss: 1.7867175340652466\n",
      "[LOG 20200511-10:15:11] epoch: 0, batch: 875 train-loss: 1.8390698432922363\n",
      "[LOG 20200511-10:15:11] epoch: 0, batch: 876 train-loss: 1.8024539947509766\n",
      "[LOG 20200511-10:15:11] epoch: 0, batch: 877 train-loss: 1.5773365497589111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:15:11] epoch: 0, batch: 878 train-loss: 2.614471673965454\n",
      "[LOG 20200511-10:15:11] epoch: 0, batch: 879 train-loss: 2.00478458404541\n",
      "[LOG 20200511-10:15:11] epoch: 0, batch: 880 train-loss: 1.6731940507888794\n",
      "[LOG 20200511-10:15:12] epoch: 0, batch: 881 train-loss: 1.257015347480774\n",
      "[LOG 20200511-10:15:12] epoch: 0, batch: 882 train-loss: 0.9500822424888611\n",
      "[LOG 20200511-10:15:12] epoch: 0, batch: 883 train-loss: 1.763527750968933\n",
      "[LOG 20200511-10:15:12] epoch: 0, batch: 884 train-loss: 2.099475860595703\n",
      "[LOG 20200511-10:15:12] epoch: 0, batch: 885 train-loss: 1.4104623794555664\n",
      "[LOG 20200511-10:15:12] epoch: 0, batch: 886 train-loss: 1.5283243656158447\n",
      "[LOG 20200511-10:15:13] epoch: 0, batch: 887 train-loss: 2.0416107177734375\n",
      "[LOG 20200511-10:15:13] epoch: 0, batch: 888 train-loss: 2.4902429580688477\n",
      "[LOG 20200511-10:15:13] epoch: 0, batch: 889 train-loss: 1.4037035703659058\n",
      "[LOG 20200511-10:15:13] epoch: 0, batch: 890 train-loss: 1.374596357345581\n",
      "[LOG 20200511-10:15:13] epoch: 0, batch: 891 train-loss: 1.3688075542449951\n",
      "[LOG 20200511-10:15:13] epoch: 0, batch: 892 train-loss: 1.415452480316162\n",
      "[LOG 20200511-10:15:13] epoch: 0, batch: 893 train-loss: 1.1052271127700806\n",
      "[LOG 20200511-10:15:14] epoch: 0, batch: 894 train-loss: 2.1899807453155518\n",
      "[LOG 20200511-10:15:14] epoch: 0, batch: 895 train-loss: 1.4103705883026123\n",
      "[LOG 20200511-10:15:14] epoch: 0, batch: 896 train-loss: 1.3668909072875977\n",
      "[LOG 20200511-10:15:14] epoch: 0, batch: 897 train-loss: 1.4567608833312988\n",
      "[LOG 20200511-10:15:14] epoch: 0, batch: 898 train-loss: 1.7737867832183838\n",
      "[LOG 20200511-10:15:14] epoch: 0, batch: 899 train-loss: 1.120463490486145\n",
      "[LOG 20200511-10:15:14] epoch: 0, batch: 900 train-loss: 1.3699871301651\n",
      "[LOG 20200511-10:15:14] epoch: 0, batch: 901 train-loss: 2.192889451980591\n",
      "[LOG 20200511-10:15:15] epoch: 0, batch: 902 train-loss: 1.7360836267471313\n",
      "[LOG 20200511-10:15:15] epoch: 0, batch: 903 train-loss: 0.9374610781669617\n",
      "[LOG 20200511-10:15:15] epoch: 0, batch: 904 train-loss: 2.760159730911255\n",
      "[LOG 20200511-10:15:15] epoch: 0, batch: 905 train-loss: 1.2067439556121826\n",
      "[LOG 20200511-10:15:15] epoch: 0, batch: 906 train-loss: 1.477351427078247\n",
      "[LOG 20200511-10:15:15] epoch: 0, batch: 907 train-loss: 2.021512508392334\n",
      "[LOG 20200511-10:15:15] epoch: 0, batch: 908 train-loss: 1.6056195497512817\n",
      "[LOG 20200511-10:15:15] epoch: 0, batch: 909 train-loss: 2.478583335876465\n",
      "[LOG 20200511-10:15:16] epoch: 0, batch: 910 train-loss: 1.3552510738372803\n",
      "[LOG 20200511-10:15:16] epoch: 0, batch: 911 train-loss: 1.8370096683502197\n",
      "[LOG 20200511-10:15:16] epoch: 0, batch: 912 train-loss: 2.3264758586883545\n",
      "[LOG 20200511-10:15:16] epoch: 0, batch: 913 train-loss: 2.228464365005493\n",
      "[LOG 20200511-10:15:16] epoch: 0, batch: 914 train-loss: 2.8550169467926025\n",
      "[LOG 20200511-10:15:16] epoch: 0, batch: 915 train-loss: 2.12949275970459\n",
      "[LOG 20200511-10:15:16] epoch: 0, batch: 916 train-loss: 2.1097114086151123\n",
      "[LOG 20200511-10:15:16] epoch: 0, batch: 917 train-loss: 1.3116458654403687\n",
      "[LOG 20200511-10:15:17] epoch: 0, batch: 918 train-loss: 1.7088842391967773\n",
      "[LOG 20200511-10:15:17] epoch: 0, batch: 919 train-loss: 1.3922083377838135\n",
      "[LOG 20200511-10:15:17] epoch: 0, batch: 920 train-loss: 2.398416519165039\n",
      "[LOG 20200511-10:15:17] epoch: 0, batch: 921 train-loss: 2.13657546043396\n",
      "[LOG 20200511-10:15:17] epoch: 0, batch: 922 train-loss: 2.4574837684631348\n",
      "[LOG 20200511-10:15:17] epoch: 0, batch: 923 train-loss: 1.395395040512085\n",
      "[LOG 20200511-10:15:17] epoch: 0, batch: 924 train-loss: 2.4562249183654785\n",
      "[LOG 20200511-10:15:17] epoch: 0, batch: 925 train-loss: 1.661083459854126\n",
      "[LOG 20200511-10:15:18] epoch: 0, batch: 926 train-loss: 1.9248464107513428\n",
      "[LOG 20200511-10:15:18] epoch: 0, batch: 927 train-loss: 2.63970947265625\n",
      "[LOG 20200511-10:15:18] epoch: 0, batch: 928 train-loss: 1.8898262977600098\n",
      "[LOG 20200511-10:15:18] epoch: 0, batch: 929 train-loss: 1.4192992448806763\n",
      "[LOG 20200511-10:15:18] epoch: 0, batch: 930 train-loss: 1.6941486597061157\n",
      "[LOG 20200511-10:15:18] epoch: 0, batch: 931 train-loss: 1.5531882047653198\n",
      "[LOG 20200511-10:15:18] epoch: 0, batch: 932 train-loss: 2.051865816116333\n",
      "[LOG 20200511-10:15:19] epoch: 0, batch: 933 train-loss: 1.9887322187423706\n",
      "[LOG 20200511-10:15:19] epoch: 0, batch: 934 train-loss: 2.102283239364624\n",
      "[LOG 20200511-10:15:19] epoch: 0, batch: 935 train-loss: 1.3955882787704468\n",
      "[LOG 20200511-10:15:19] epoch: 0, batch: 936 train-loss: 1.6054890155792236\n",
      "[LOG 20200511-10:15:19] epoch: 0, batch: 937 train-loss: 1.8805948495864868\n",
      "[LOG 20200511-10:15:19] epoch: 0, batch: 938 train-loss: 2.078413963317871\n",
      "[LOG 20200511-10:15:19] epoch: 0, batch: 939 train-loss: 1.4949922561645508\n",
      "[LOG 20200511-10:15:19] epoch: 0, batch: 940 train-loss: 2.522066831588745\n",
      "[LOG 20200511-10:15:19] epoch: 0, batch: 941 train-loss: 2.1761271953582764\n",
      "[LOG 20200511-10:15:20] epoch: 0, batch: 942 train-loss: 1.3898515701293945\n",
      "[LOG 20200511-10:15:20] epoch: 0, batch: 943 train-loss: 1.793580412864685\n",
      "[LOG 20200511-10:15:20] epoch: 0, batch: 944 train-loss: 1.4092965126037598\n",
      "[LOG 20200511-10:15:20] epoch: 0, batch: 945 train-loss: 1.7929819822311401\n",
      "[LOG 20200511-10:15:20] epoch: 0, batch: 946 train-loss: 1.5348384380340576\n",
      "[LOG 20200511-10:15:20] epoch: 0, batch: 947 train-loss: 3.1713027954101562\n",
      "[LOG 20200511-10:15:20] epoch: 0, batch: 948 train-loss: 1.9181839227676392\n",
      "[LOG 20200511-10:15:20] epoch: 0, batch: 949 train-loss: 1.5116288661956787\n",
      "[LOG 20200511-10:15:21] epoch: 0, batch: 950 train-loss: 2.2991180419921875\n",
      "[LOG 20200511-10:15:21] epoch: 0, batch: 951 train-loss: 1.538182258605957\n",
      "[LOG 20200511-10:15:21] epoch: 0, batch: 952 train-loss: 1.9387500286102295\n",
      "[LOG 20200511-10:15:21] epoch: 0, batch: 953 train-loss: 1.7109375\n",
      "[LOG 20200511-10:15:21] epoch: 0, batch: 954 train-loss: 1.7241169214248657\n",
      "[LOG 20200511-10:15:21] epoch: 0, batch: 955 train-loss: 1.7453529834747314\n",
      "[LOG 20200511-10:15:21] epoch: 0, batch: 956 train-loss: 1.4291470050811768\n",
      "[LOG 20200511-10:15:21] epoch: 0, batch: 957 train-loss: 2.0712928771972656\n",
      "[LOG 20200511-10:15:22] epoch: 0, batch: 958 train-loss: 1.7038931846618652\n",
      "[LOG 20200511-10:15:22] epoch: 0, batch: 959 train-loss: 1.57180917263031\n",
      "[LOG 20200511-10:15:22] epoch: 0, batch: 960 train-loss: 2.5379693508148193\n",
      "[LOG 20200511-10:15:22] epoch: 0, batch: 961 train-loss: 1.85054349899292\n",
      "[LOG 20200511-10:15:22] epoch: 0, batch: 962 train-loss: 2.1247870922088623\n",
      "[LOG 20200511-10:15:22] epoch: 0, batch: 963 train-loss: 1.3924355506896973\n",
      "[LOG 20200511-10:15:22] epoch: 0, batch: 964 train-loss: 2.2887167930603027\n",
      "[LOG 20200511-10:15:22] epoch: 0, batch: 965 train-loss: 1.792291283607483\n",
      "[LOG 20200511-10:15:23] epoch: 0, batch: 966 train-loss: 1.4142183065414429\n",
      "[LOG 20200511-10:15:23] epoch: 0, batch: 967 train-loss: 1.230918526649475\n",
      "[LOG 20200511-10:15:23] epoch: 0, batch: 968 train-loss: 1.548431634902954\n",
      "[LOG 20200511-10:15:23] epoch: 0, batch: 969 train-loss: 1.1203006505966187\n",
      "[LOG 20200511-10:15:23] epoch: 0, batch: 970 train-loss: 2.2223572731018066\n",
      "[LOG 20200511-10:15:23] epoch: 0, batch: 971 train-loss: 2.051528215408325\n",
      "[LOG 20200511-10:15:23] epoch: 0, batch: 972 train-loss: 1.8633151054382324\n",
      "[LOG 20200511-10:15:23] epoch: 0, batch: 973 train-loss: 1.4418939352035522\n",
      "[LOG 20200511-10:15:24] epoch: 0, batch: 974 train-loss: 1.2766640186309814\n",
      "[LOG 20200511-10:15:24] epoch: 0, batch: 975 train-loss: 1.881229281425476\n",
      "[LOG 20200511-10:15:24] epoch: 0, batch: 976 train-loss: 1.8069076538085938\n",
      "[LOG 20200511-10:15:24] epoch: 0, batch: 977 train-loss: 1.6585452556610107\n",
      "[LOG 20200511-10:15:24] epoch: 0, batch: 978 train-loss: 1.8500903844833374\n",
      "[LOG 20200511-10:15:24] epoch: 0, batch: 979 train-loss: 2.132730007171631\n",
      "[LOG 20200511-10:15:24] epoch: 0, batch: 980 train-loss: 1.835719347000122\n",
      "[LOG 20200511-10:15:24] epoch: 0, batch: 981 train-loss: 1.8862310647964478\n",
      "[LOG 20200511-10:15:25] epoch: 0, batch: 982 train-loss: 2.4236974716186523\n",
      "[LOG 20200511-10:15:25] epoch: 0, batch: 983 train-loss: 2.2789499759674072\n",
      "[LOG 20200511-10:15:25] epoch: 0, batch: 984 train-loss: 1.4994752407073975\n",
      "[LOG 20200511-10:15:25] epoch: 0, batch: 985 train-loss: 1.8146848678588867\n",
      "[LOG 20200511-10:15:25] epoch: 0, batch: 986 train-loss: 1.7637698650360107\n",
      "[LOG 20200511-10:15:25] epoch: 0, batch: 987 train-loss: 1.2236318588256836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:15:25] epoch: 0, batch: 988 train-loss: 1.3162850141525269\n",
      "[LOG 20200511-10:15:25] epoch: 0, batch: 989 train-loss: 1.9218854904174805\n",
      "[LOG 20200511-10:15:25] epoch: 0, batch: 990 train-loss: 1.5908012390136719\n",
      "[LOG 20200511-10:15:26] epoch: 0, batch: 991 train-loss: 1.480614185333252\n",
      "[LOG 20200511-10:15:26] epoch: 0, batch: 992 train-loss: 1.6910102367401123\n",
      "[LOG 20200511-10:15:26] epoch: 0, batch: 993 train-loss: 1.9309784173965454\n",
      "[LOG 20200511-10:15:26] epoch: 0, batch: 994 train-loss: 1.633291482925415\n",
      "[LOG 20200511-10:15:26] epoch: 0, batch: 995 train-loss: 1.5974353551864624\n",
      "[LOG 20200511-10:15:26] epoch: 0, batch: 996 train-loss: 2.0293149948120117\n",
      "[LOG 20200511-10:15:26] epoch: 0, batch: 997 train-loss: 1.6822032928466797\n",
      "[LOG 20200511-10:15:27] epoch: 0, batch: 998 train-loss: 1.8976662158966064\n",
      "[LOG 20200511-10:15:27] epoch: 0, batch: 999 train-loss: 1.7447426319122314\n",
      "[LOG 20200511-10:15:27] epoch: 0, batch: 1000 train-loss: 2.0402731895446777\n",
      "[LOG 20200511-10:15:27] epoch: 0, batch: 1001 train-loss: 1.7509983777999878\n",
      "[LOG 20200511-10:15:27] epoch: 0, batch: 1002 train-loss: 1.7883213758468628\n",
      "[LOG 20200511-10:15:27] epoch: 0, batch: 1003 train-loss: 1.7266724109649658\n",
      "[LOG 20200511-10:15:27] epoch: 0, batch: 1004 train-loss: 1.3190745115280151\n",
      "[LOG 20200511-10:15:27] epoch: 0, batch: 1005 train-loss: 1.3725550174713135\n",
      "[LOG 20200511-10:15:28] epoch: 0, batch: 1006 train-loss: 2.0766072273254395\n",
      "[LOG 20200511-10:15:28] epoch: 0, batch: 1007 train-loss: 1.307499647140503\n",
      "[LOG 20200511-10:15:28] epoch: 0, batch: 1008 train-loss: 1.9088311195373535\n",
      "[LOG 20200511-10:15:28] epoch: 0, batch: 1009 train-loss: 1.6893627643585205\n",
      "[LOG 20200511-10:15:28] epoch: 0, batch: 1010 train-loss: 2.1749632358551025\n",
      "[LOG 20200511-10:15:28] epoch: 0, batch: 1011 train-loss: 1.7671034336090088\n",
      "[LOG 20200511-10:15:29] epoch: 0, batch: 1012 train-loss: 1.2239704132080078\n",
      "[LOG 20200511-10:15:29] epoch: 0, batch: 1013 train-loss: 2.425612449645996\n",
      "[LOG 20200511-10:15:29] epoch: 0, batch: 1014 train-loss: 1.7075655460357666\n",
      "[LOG 20200511-10:15:29] epoch: 0, batch: 1015 train-loss: 1.4577025175094604\n",
      "[LOG 20200511-10:15:29] epoch: 0, batch: 1016 train-loss: 1.4874571561813354\n",
      "[LOG 20200511-10:15:29] epoch: 0, batch: 1017 train-loss: 1.7766685485839844\n",
      "[LOG 20200511-10:15:29] epoch: 0, batch: 1018 train-loss: 2.7353787422180176\n",
      "[LOG 20200511-10:15:29] epoch: 0, batch: 1019 train-loss: 1.5319485664367676\n",
      "[LOG 20200511-10:15:30] epoch: 0, batch: 1020 train-loss: 2.4051074981689453\n",
      "[LOG 20200511-10:15:30] epoch: 0, batch: 1021 train-loss: 2.3806278705596924\n",
      "[LOG 20200511-10:15:30] epoch: 0, batch: 1022 train-loss: 1.4262738227844238\n",
      "[LOG 20200511-10:15:30] epoch: 0, batch: 1023 train-loss: 1.824056625366211\n",
      "[LOG 20200511-10:15:30] epoch: 0, batch: 1024 train-loss: 1.6502605676651\n",
      "[LOG 20200511-10:15:30] epoch: 0, batch: 1025 train-loss: 1.833133578300476\n",
      "[LOG 20200511-10:15:30] epoch: 0, batch: 1026 train-loss: 1.8159235715866089\n",
      "[LOG 20200511-10:15:30] epoch: 0, batch: 1027 train-loss: 1.915369987487793\n",
      "[LOG 20200511-10:15:30] epoch: 0, batch: 1028 train-loss: 1.4076794385910034\n",
      "[LOG 20200511-10:15:31] epoch: 0, batch: 1029 train-loss: 2.053515911102295\n",
      "[LOG 20200511-10:15:31] epoch: 0, batch: 1030 train-loss: 2.0559794902801514\n",
      "[LOG 20200511-10:15:31] epoch: 0, batch: 1031 train-loss: 2.0519351959228516\n",
      "[LOG 20200511-10:15:31] epoch: 0, batch: 1032 train-loss: 2.8500356674194336\n",
      "[LOG 20200511-10:15:31] epoch: 0, batch: 1033 train-loss: 1.0291264057159424\n",
      "[LOG 20200511-10:15:31] epoch: 0, batch: 1034 train-loss: 1.934975504875183\n",
      "[LOG 20200511-10:15:31] epoch: 0, batch: 1035 train-loss: 1.8434263467788696\n",
      "[LOG 20200511-10:15:31] epoch: 0, batch: 1036 train-loss: 1.9910175800323486\n",
      "[LOG 20200511-10:15:31] epoch: 0, batch: 1037 train-loss: 1.415295958518982\n",
      "[LOG 20200511-10:15:31] epoch: 0, batch: 1038 train-loss: 0.9698289632797241\n",
      "[LOG 20200511-10:15:32] epoch: 0, batch: 1039 train-loss: 1.33510160446167\n",
      "[LOG 20200511-10:15:32] epoch: 0, batch: 1040 train-loss: 1.7715998888015747\n",
      "[LOG 20200511-10:15:32] epoch: 0, batch: 1041 train-loss: 1.4159146547317505\n",
      "[LOG 20200511-10:15:32] epoch: 0, batch: 1042 train-loss: 1.903435230255127\n",
      "[LOG 20200511-10:15:32] epoch: 0, batch: 1043 train-loss: 1.1869680881500244\n",
      "[LOG 20200511-10:15:32] epoch: 0, batch: 1044 train-loss: 1.4101793766021729\n",
      "[LOG 20200511-10:15:32] epoch: 0, batch: 1045 train-loss: 2.1283440589904785\n",
      "[LOG 20200511-10:15:32] epoch: 0, batch: 1046 train-loss: 1.5967923402786255\n",
      "[LOG 20200511-10:15:32] epoch: 0, batch: 1047 train-loss: 1.1307685375213623\n",
      "[LOG 20200511-10:15:33] epoch: 0, batch: 1048 train-loss: 1.9179469347000122\n",
      "[LOG 20200511-10:15:33] epoch: 0, batch: 1049 train-loss: 2.001934051513672\n",
      "[LOG 20200511-10:15:33] epoch: 0, batch: 1050 train-loss: 1.2465550899505615\n",
      "[LOG 20200511-10:15:33] epoch: 0, batch: 1051 train-loss: 1.519698143005371\n",
      "[LOG 20200511-10:15:33] epoch: 0, batch: 1052 train-loss: 2.444490671157837\n",
      "[LOG 20200511-10:15:33] epoch: 0, batch: 1053 train-loss: 2.0544605255126953\n",
      "[LOG 20200511-10:15:33] epoch: 0, batch: 1054 train-loss: 1.4605299234390259\n",
      "[LOG 20200511-10:15:33] epoch: 0, batch: 1055 train-loss: 1.5235044956207275\n",
      "[LOG 20200511-10:15:33] epoch: 0, batch: 1056 train-loss: 0.7732661962509155\n",
      "[LOG 20200511-10:15:34] epoch: 0, batch: 1057 train-loss: 2.033141613006592\n",
      "[LOG 20200511-10:15:34] epoch: 0, batch: 1058 train-loss: 1.768441915512085\n",
      "[LOG 20200511-10:15:34] epoch: 0, batch: 1059 train-loss: 2.2650344371795654\n",
      "[LOG 20200511-10:15:34] epoch: 0, batch: 1060 train-loss: 0.8932569026947021\n",
      "[LOG 20200511-10:15:34] epoch: 0, batch: 1061 train-loss: 3.3719732761383057\n",
      "[LOG 20200511-10:15:34] epoch: 0, batch: 1062 train-loss: 1.4524863958358765\n",
      "[LOG 20200511-10:15:34] epoch: 0, batch: 1063 train-loss: 2.134032964706421\n",
      "[LOG 20200511-10:15:34] epoch: 0, batch: 1064 train-loss: 2.308216094970703\n",
      "[LOG 20200511-10:15:34] epoch: 0, batch: 1065 train-loss: 1.3563361167907715\n",
      "[LOG 20200511-10:15:35] epoch: 0, batch: 1066 train-loss: 1.2885711193084717\n",
      "[LOG 20200511-10:15:35] epoch: 0, batch: 1067 train-loss: 1.5347309112548828\n",
      "[LOG 20200511-10:15:35] epoch: 0, batch: 1068 train-loss: 1.2348936796188354\n",
      "[LOG 20200511-10:15:35] epoch: 0, batch: 1069 train-loss: 1.809102177619934\n",
      "[LOG 20200511-10:15:35] epoch: 0, batch: 1070 train-loss: 0.9871003031730652\n",
      "[LOG 20200511-10:15:35] epoch: 0, batch: 1071 train-loss: 2.2576260566711426\n",
      "[LOG 20200511-10:15:35] epoch: 0, batch: 1072 train-loss: 1.1742719411849976\n",
      "[LOG 20200511-10:15:35] epoch: 0, batch: 1073 train-loss: 1.2663196325302124\n",
      "[LOG 20200511-10:15:36] epoch: 0, batch: 1074 train-loss: 1.7370580434799194\n",
      "[LOG 20200511-10:15:36] epoch: 0, batch: 1075 train-loss: 2.084092140197754\n",
      "[LOG 20200511-10:15:36] epoch: 0, batch: 1076 train-loss: 1.3644168376922607\n",
      "[LOG 20200511-10:15:36] epoch: 0, batch: 1077 train-loss: 1.5914216041564941\n",
      "[LOG 20200511-10:15:36] epoch: 0, batch: 1078 train-loss: 2.558316469192505\n",
      "[LOG 20200511-10:15:36] epoch: 0, batch: 1079 train-loss: 1.2165462970733643\n",
      "[LOG 20200511-10:15:36] epoch: 0, batch: 1080 train-loss: 1.4143351316452026\n",
      "[LOG 20200511-10:15:36] epoch: 0, batch: 1081 train-loss: 0.9020735025405884\n",
      "[LOG 20200511-10:15:36] epoch: 0, batch: 1082 train-loss: 1.9328856468200684\n",
      "[LOG 20200511-10:15:37] epoch: 0, batch: 1083 train-loss: 1.8885631561279297\n",
      "[LOG 20200511-10:15:37] epoch: 0, batch: 1084 train-loss: 1.3208614587783813\n",
      "[LOG 20200511-10:15:37] epoch: 0, batch: 1085 train-loss: 2.2264740467071533\n",
      "[LOG 20200511-10:15:37] epoch: 0, batch: 1086 train-loss: 1.866355299949646\n",
      "[LOG 20200511-10:15:37] epoch: 0, batch: 1087 train-loss: 1.7699661254882812\n",
      "[LOG 20200511-10:15:37] epoch: 0, batch: 1088 train-loss: 1.6163640022277832\n",
      "[LOG 20200511-10:15:37] epoch: 0, batch: 1089 train-loss: 1.4782949686050415\n",
      "[LOG 20200511-10:15:38] epoch: 0, batch: 1090 train-loss: 1.453043818473816\n",
      "[LOG 20200511-10:15:38] epoch: 0, batch: 1091 train-loss: 1.5290569067001343\n",
      "[LOG 20200511-10:15:38] epoch: 0, batch: 1092 train-loss: 1.543138027191162\n",
      "[LOG 20200511-10:15:38] epoch: 0, batch: 1093 train-loss: 2.525710105895996\n",
      "[LOG 20200511-10:15:38] epoch: 0, batch: 1094 train-loss: 2.1335396766662598\n",
      "[LOG 20200511-10:15:38] epoch: 0, batch: 1095 train-loss: 2.121251106262207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:15:38] epoch: 0, batch: 1096 train-loss: 1.7093290090560913\n",
      "[LOG 20200511-10:15:39] epoch: 0, batch: 1097 train-loss: 1.475326418876648\n",
      "[LOG 20200511-10:15:39] epoch: 0, batch: 1098 train-loss: 1.5795552730560303\n",
      "[LOG 20200511-10:15:39] epoch: 0, batch: 1099 train-loss: 1.3225769996643066\n",
      "[LOG 20200511-10:15:39] epoch: 0, batch: 1100 train-loss: 1.3454830646514893\n",
      "[LOG 20200511-10:15:39] epoch: 0, batch: 1101 train-loss: 2.651524066925049\n",
      "[LOG 20200511-10:15:39] epoch: 0, batch: 1102 train-loss: 2.0290541648864746\n",
      "[LOG 20200511-10:15:39] epoch: 0, batch: 1103 train-loss: 2.2859723567962646\n",
      "[LOG 20200511-10:15:39] epoch: 0, batch: 1104 train-loss: 1.4784140586853027\n",
      "[LOG 20200511-10:15:40] epoch: 0, batch: 1105 train-loss: 2.015942335128784\n",
      "[LOG 20200511-10:15:40] epoch: 0, batch: 1106 train-loss: 2.1536307334899902\n",
      "[LOG 20200511-10:15:40] epoch: 0, batch: 1107 train-loss: 1.9139411449432373\n",
      "[LOG 20200511-10:15:40] epoch: 0, batch: 1108 train-loss: 1.8058335781097412\n",
      "[LOG 20200511-10:15:40] epoch: 0, batch: 1109 train-loss: 1.8702425956726074\n",
      "[LOG 20200511-10:15:40] epoch: 0, batch: 1110 train-loss: 2.2524986267089844\n",
      "[LOG 20200511-10:15:40] epoch: 0, batch: 1111 train-loss: 2.2230606079101562\n",
      "[LOG 20200511-10:15:40] epoch: 0, batch: 1112 train-loss: 0.7882781028747559\n",
      "[LOG 20200511-10:15:40] epoch: 0, batch: 1113 train-loss: 1.982912540435791\n",
      "[LOG 20200511-10:15:41] epoch: 0, batch: 1114 train-loss: 1.911560297012329\n",
      "[LOG 20200511-10:15:41] epoch: 0, batch: 1115 train-loss: 2.044612407684326\n",
      "[LOG 20200511-10:15:41] epoch: 0, batch: 1116 train-loss: 2.184086561203003\n",
      "[LOG 20200511-10:15:41] epoch: 0, batch: 1117 train-loss: 1.0864589214324951\n",
      "[LOG 20200511-10:15:41] epoch: 0, batch: 1118 train-loss: 1.481391429901123\n",
      "[LOG 20200511-10:15:41] epoch: 0, batch: 1119 train-loss: 2.0690245628356934\n",
      "[LOG 20200511-10:15:41] epoch: 0, batch: 1120 train-loss: 1.6162911653518677\n",
      "[LOG 20200511-10:15:41] epoch: 0, batch: 1121 train-loss: 2.0733413696289062\n",
      "[LOG 20200511-10:15:41] epoch: 0, batch: 1122 train-loss: 1.7177155017852783\n",
      "[LOG 20200511-10:15:42] epoch: 0, batch: 1123 train-loss: 1.8608503341674805\n",
      "[LOG 20200511-10:15:42] epoch: 0, batch: 1124 train-loss: 1.523465633392334\n",
      "[LOG 20200511-10:15:42] epoch: 0, batch: 1125 train-loss: 1.4617972373962402\n",
      "[LOG 20200511-10:15:42] epoch: 0, batch: 1126 train-loss: 1.3402643203735352\n",
      "[LOG 20200511-10:15:42] epoch: 0, batch: 1127 train-loss: 2.0226035118103027\n",
      "[LOG 20200511-10:15:42] epoch: 0, batch: 1128 train-loss: 1.150349736213684\n",
      "[LOG 20200511-10:15:42] epoch: 0, batch: 1129 train-loss: 1.708282232284546\n",
      "[LOG 20200511-10:15:42] epoch: 0, batch: 1130 train-loss: 1.3748869895935059\n",
      "[LOG 20200511-10:15:42] epoch: 0, batch: 1131 train-loss: 2.143737554550171\n",
      "[LOG 20200511-10:15:42] epoch: 0, batch: 1132 train-loss: 1.895307183265686\n",
      "[LOG 20200511-10:15:43] epoch: 0, batch: 1133 train-loss: 1.132123589515686\n",
      "[LOG 20200511-10:15:43] epoch: 0, batch: 1134 train-loss: 2.449101448059082\n",
      "[LOG 20200511-10:15:43] epoch: 0, batch: 1135 train-loss: 2.677351951599121\n",
      "[LOG 20200511-10:15:43] epoch: 0, batch: 1136 train-loss: 1.079525113105774\n",
      "[LOG 20200511-10:15:43] epoch: 0, batch: 1137 train-loss: 1.5722954273223877\n",
      "[LOG 20200511-10:15:43] epoch: 0, batch: 1138 train-loss: 2.111501932144165\n",
      "[LOG 20200511-10:15:43] epoch: 0, batch: 1139 train-loss: 1.5011789798736572\n",
      "[LOG 20200511-10:15:43] epoch: 0, batch: 1140 train-loss: 1.5187950134277344\n",
      "[LOG 20200511-10:15:43] epoch: 0, batch: 1141 train-loss: 1.4595820903778076\n",
      "[LOG 20200511-10:15:44] epoch: 0, batch: 1142 train-loss: 1.927760362625122\n",
      "[LOG 20200511-10:15:44] epoch: 0, batch: 1143 train-loss: 1.351144790649414\n",
      "[LOG 20200511-10:15:44] epoch: 0, batch: 1144 train-loss: 1.5445976257324219\n",
      "[LOG 20200511-10:15:44] epoch: 0, batch: 1145 train-loss: 2.163275718688965\n",
      "[LOG 20200511-10:15:44] epoch: 0, batch: 1146 train-loss: 2.0328903198242188\n",
      "[LOG 20200511-10:15:44] epoch: 0, batch: 1147 train-loss: 0.6845670342445374\n",
      "[LOG 20200511-10:15:44] epoch: 0, batch: 1148 train-loss: 2.1232504844665527\n",
      "[LOG 20200511-10:15:44] epoch: 0, batch: 1149 train-loss: 1.5261883735656738\n",
      "[LOG 20200511-10:15:44] epoch: 0, batch: 1150 train-loss: 1.550309419631958\n",
      "[LOG 20200511-10:15:45] epoch: 0, batch: 1151 train-loss: 1.2162936925888062\n",
      "[LOG 20200511-10:15:45] epoch: 0, batch: 1152 train-loss: 2.086589813232422\n",
      "[LOG 20200511-10:15:45] epoch: 0, batch: 1153 train-loss: 1.2550655603408813\n",
      "[LOG 20200511-10:15:45] epoch: 0, batch: 1154 train-loss: 1.4021828174591064\n",
      "[LOG 20200511-10:15:45] epoch: 0, batch: 1155 train-loss: 1.6770827770233154\n",
      "[LOG 20200511-10:15:45] epoch: 0, batch: 1156 train-loss: 1.3098492622375488\n",
      "[LOG 20200511-10:15:45] epoch: 0, batch: 1157 train-loss: 1.6097495555877686\n",
      "[LOG 20200511-10:15:45] epoch: 0, batch: 1158 train-loss: 1.6733826398849487\n",
      "[LOG 20200511-10:15:45] epoch: 0, batch: 1159 train-loss: 1.2191153764724731\n",
      "[LOG 20200511-10:15:45] epoch: 0, batch: 1160 train-loss: 2.3688948154449463\n",
      "[LOG 20200511-10:15:46] epoch: 0, batch: 1161 train-loss: 1.8240278959274292\n",
      "[LOG 20200511-10:15:46] epoch: 0, batch: 1162 train-loss: 1.1820180416107178\n",
      "[LOG 20200511-10:15:46] epoch: 0, batch: 1163 train-loss: 1.6011093854904175\n",
      "[LOG 20200511-10:15:46] epoch: 0, batch: 1164 train-loss: 1.6821084022521973\n",
      "[LOG 20200511-10:15:46] epoch: 0, batch: 1165 train-loss: 2.1564769744873047\n",
      "[LOG 20200511-10:15:46] epoch: 0, batch: 1166 train-loss: 2.8293280601501465\n",
      "[LOG 20200511-10:15:46] epoch: 0, batch: 1167 train-loss: 1.2653484344482422\n",
      "[LOG 20200511-10:15:46] epoch: 0, batch: 1168 train-loss: 2.0355637073516846\n",
      "[LOG 20200511-10:15:46] epoch: 0, batch: 1169 train-loss: 1.2173709869384766\n",
      "[LOG 20200511-10:15:47] epoch: 0, batch: 1170 train-loss: 3.0109705924987793\n",
      "[LOG 20200511-10:15:47] epoch: 0, batch: 1171 train-loss: 1.9976139068603516\n",
      "[LOG 20200511-10:15:47] epoch: 0, batch: 1172 train-loss: 1.5054937601089478\n",
      "[LOG 20200511-10:15:47] epoch: 0, batch: 1173 train-loss: 1.5054830312728882\n",
      "[LOG 20200511-10:15:47] epoch: 0, batch: 1174 train-loss: 1.959280252456665\n",
      "[LOG 20200511-10:15:47] epoch: 0, batch: 1175 train-loss: 2.335653781890869\n",
      "[LOG 20200511-10:15:47] epoch: 0, batch: 1176 train-loss: 1.6209100484848022\n",
      "[LOG 20200511-10:15:47] epoch: 0, batch: 1177 train-loss: 2.0279858112335205\n",
      "[LOG 20200511-10:15:47] epoch: 0, batch: 1178 train-loss: 1.4456965923309326\n",
      "[LOG 20200511-10:15:48] epoch: 0, batch: 1179 train-loss: 1.2562379837036133\n",
      "[LOG 20200511-10:15:48] epoch: 0, batch: 1180 train-loss: 1.7169413566589355\n",
      "[LOG 20200511-10:15:48] epoch: 0, batch: 1181 train-loss: 1.6446484327316284\n",
      "[LOG 20200511-10:15:48] epoch: 0, batch: 1182 train-loss: 2.055460214614868\n",
      "[LOG 20200511-10:15:48] epoch: 0, batch: 1183 train-loss: 1.1480563879013062\n",
      "[LOG 20200511-10:15:48] epoch: 0, batch: 1184 train-loss: 1.6371774673461914\n",
      "[LOG 20200511-10:15:48] epoch: 0, batch: 1185 train-loss: 1.243558645248413\n",
      "[LOG 20200511-10:15:48] epoch: 0, batch: 1186 train-loss: 1.9048147201538086\n",
      "[LOG 20200511-10:15:49] epoch: 0, batch: 1187 train-loss: 1.3475511074066162\n",
      "[LOG 20200511-10:15:49] epoch: 0, batch: 1188 train-loss: 1.3508979082107544\n",
      "[LOG 20200511-10:15:49] epoch: 0, batch: 1189 train-loss: 1.2289714813232422\n",
      "[LOG 20200511-10:15:49] epoch: 0, batch: 1190 train-loss: 1.691248893737793\n",
      "[LOG 20200511-10:15:49] epoch: 0, batch: 1191 train-loss: 2.5082974433898926\n",
      "[LOG 20200511-10:15:49] epoch: 0, batch: 1192 train-loss: 1.875464677810669\n",
      "[LOG 20200511-10:15:49] epoch: 0, batch: 1193 train-loss: 1.728196144104004\n",
      "[LOG 20200511-10:15:50] epoch: 0, batch: 1194 train-loss: 1.7229214906692505\n",
      "[LOG 20200511-10:15:50] epoch: 0, batch: 1195 train-loss: 1.2081828117370605\n",
      "[LOG 20200511-10:15:50] epoch: 0, batch: 1196 train-loss: 2.625553846359253\n",
      "[LOG 20200511-10:15:50] epoch: 0, batch: 1197 train-loss: 1.7483115196228027\n",
      "[LOG 20200511-10:15:50] epoch: 0, batch: 1198 train-loss: 2.047423839569092\n",
      "[LOG 20200511-10:15:50] epoch: 0, batch: 1199 train-loss: 2.6611218452453613\n",
      "[LOG 20200511-10:15:50] epoch: 0, batch: 1200 train-loss: 1.8064417839050293\n",
      "[LOG 20200511-10:15:50] epoch: 0, batch: 1201 train-loss: 1.872740626335144\n",
      "[LOG 20200511-10:15:50] epoch: 0, batch: 1202 train-loss: 1.9161008596420288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:15:51] epoch: 0, batch: 1203 train-loss: 1.865727186203003\n",
      "[LOG 20200511-10:15:51] epoch: 0, batch: 1204 train-loss: 2.9758925437927246\n",
      "[LOG 20200511-10:15:51] epoch: 0, batch: 1205 train-loss: 2.511371374130249\n",
      "[LOG 20200511-10:15:51] epoch: 0, batch: 1206 train-loss: 2.112215757369995\n",
      "[LOG 20200511-10:15:51] epoch: 0, batch: 1207 train-loss: 1.3655197620391846\n",
      "[LOG 20200511-10:15:51] epoch: 0, batch: 1208 train-loss: 2.413283348083496\n",
      "[LOG 20200511-10:15:51] epoch: 0, batch: 1209 train-loss: 1.5218666791915894\n",
      "[LOG 20200511-10:15:51] epoch: 0, batch: 1210 train-loss: 0.9539378881454468\n",
      "[LOG 20200511-10:15:52] epoch: 0, batch: 1211 train-loss: 2.0275416374206543\n",
      "[LOG 20200511-10:15:52] epoch: 0, batch: 1212 train-loss: 1.960482120513916\n",
      "[LOG 20200511-10:15:52] epoch: 0, batch: 1213 train-loss: 1.2487844228744507\n",
      "[LOG 20200511-10:15:52] epoch: 0, batch: 1214 train-loss: 2.528904914855957\n",
      "[LOG 20200511-10:15:52] epoch: 0, batch: 1215 train-loss: 2.1055283546447754\n",
      "[LOG 20200511-10:15:52] epoch: 0, batch: 1216 train-loss: 1.3851796388626099\n",
      "[LOG 20200511-10:15:52] epoch: 0, batch: 1217 train-loss: 1.3571008443832397\n",
      "[LOG 20200511-10:15:52] epoch: 0, batch: 1218 train-loss: 1.0567355155944824\n",
      "[LOG 20200511-10:15:53] epoch: 0, batch: 1219 train-loss: 2.369105339050293\n",
      "[LOG 20200511-10:15:53] epoch: 0, batch: 1220 train-loss: 1.5368703603744507\n",
      "[LOG 20200511-10:15:53] epoch: 0, batch: 1221 train-loss: 1.5810434818267822\n",
      "[LOG 20200511-10:15:53] epoch: 0, batch: 1222 train-loss: 1.421477198600769\n",
      "[LOG 20200511-10:15:53] epoch: 0, batch: 1223 train-loss: 1.5543323755264282\n",
      "[LOG 20200511-10:15:53] epoch: 0, batch: 1224 train-loss: 1.797999382019043\n",
      "[LOG 20200511-10:15:53] epoch: 0, batch: 1225 train-loss: 1.5485315322875977\n",
      "[LOG 20200511-10:15:53] epoch: 0, batch: 1226 train-loss: 1.4309587478637695\n",
      "[LOG 20200511-10:15:53] epoch: 0, batch: 1227 train-loss: 2.6239490509033203\n",
      "[LOG 20200511-10:15:54] epoch: 0, batch: 1228 train-loss: 2.0597660541534424\n",
      "[LOG 20200511-10:15:54] epoch: 0, batch: 1229 train-loss: 1.321268081665039\n",
      "[LOG 20200511-10:15:54] epoch: 0, batch: 1230 train-loss: 1.9354922771453857\n",
      "[LOG 20200511-10:15:54] epoch: 0, batch: 1231 train-loss: 2.103257179260254\n",
      "[LOG 20200511-10:15:54] epoch: 0, batch: 1232 train-loss: 1.852117657661438\n",
      "[LOG 20200511-10:15:54] epoch: 0, batch: 1233 train-loss: 1.681414246559143\n",
      "[LOG 20200511-10:15:54] epoch: 0, batch: 1234 train-loss: 1.7067632675170898\n",
      "[LOG 20200511-10:15:54] epoch: 0, batch: 1235 train-loss: 1.7204465866088867\n",
      "[LOG 20200511-10:15:54] epoch: 0, batch: 1236 train-loss: 1.523036241531372\n",
      "[LOG 20200511-10:15:54] epoch: 0, batch: 1237 train-loss: 1.5543491840362549\n",
      "[LOG 20200511-10:15:55] epoch: 0, batch: 1238 train-loss: 1.3532559871673584\n",
      "[LOG 20200511-10:15:55] epoch: 0, batch: 1239 train-loss: 2.10266375541687\n",
      "[LOG 20200511-10:15:55] epoch: 0, batch: 1240 train-loss: 2.2080562114715576\n",
      "[LOG 20200511-10:15:55] epoch: 0, batch: 1241 train-loss: 2.3728301525115967\n",
      "[LOG 20200511-10:15:55] epoch: 0, batch: 1242 train-loss: 1.797213077545166\n",
      "[LOG 20200511-10:15:55] epoch: 0, batch: 1243 train-loss: 1.4674991369247437\n",
      "[LOG 20200511-10:15:55] epoch: 0, batch: 1244 train-loss: 1.3564317226409912\n",
      "[LOG 20200511-10:15:55] epoch: 0, batch: 1245 train-loss: 1.4942190647125244\n",
      "[LOG 20200511-10:15:56] epoch: 0, batch: 1246 train-loss: 1.2034738063812256\n",
      "[LOG 20200511-10:15:56] epoch: 0, batch: 1247 train-loss: 2.7634475231170654\n",
      "[LOG 20200511-10:15:56] epoch: 0, batch: 1248 train-loss: 1.8188707828521729\n",
      "[LOG 20200511-10:15:56] epoch: 0, batch: 1249 train-loss: 1.2404770851135254\n",
      "[LOG 20200511-10:15:56] epoch: 0, batch: 1250 train-loss: 1.1377726793289185\n",
      "[LOG 20200511-10:15:56] epoch: 0, batch: 1251 train-loss: 1.5288255214691162\n",
      "[LOG 20200511-10:15:56] epoch: 0, batch: 1252 train-loss: 1.4994338750839233\n",
      "[LOG 20200511-10:15:56] epoch: 0, batch: 1253 train-loss: 1.1713935136795044\n",
      "[LOG 20200511-10:15:56] epoch: 0, batch: 1254 train-loss: 1.1738028526306152\n",
      "[LOG 20200511-10:15:57] epoch: 0, batch: 1255 train-loss: 1.527683973312378\n",
      "[LOG 20200511-10:15:57] epoch: 0, batch: 1256 train-loss: 1.2115315198898315\n",
      "[LOG 20200511-10:15:57] epoch: 0, batch: 1257 train-loss: 2.5142014026641846\n",
      "[LOG 20200511-10:15:57] epoch: 0, batch: 1258 train-loss: 1.9831626415252686\n",
      "[LOG 20200511-10:15:57] epoch: 0, batch: 1259 train-loss: 2.1446101665496826\n",
      "[LOG 20200511-10:15:57] epoch: 0, batch: 1260 train-loss: 1.0916579961776733\n",
      "[LOG 20200511-10:15:57] epoch: 0, batch: 1261 train-loss: 1.9813296794891357\n",
      "[LOG 20200511-10:15:57] epoch: 0, batch: 1262 train-loss: 2.080150604248047\n",
      "[LOG 20200511-10:15:58] epoch: 0, batch: 1263 train-loss: 2.4429519176483154\n",
      "[LOG 20200511-10:15:58] epoch: 0, batch: 1264 train-loss: 0.964633584022522\n",
      "[LOG 20200511-10:15:58] epoch: 0, batch: 1265 train-loss: 2.118647575378418\n",
      "[LOG 20200511-10:15:58] epoch: 0, batch: 1266 train-loss: 1.9533085823059082\n",
      "[LOG 20200511-10:15:58] epoch: 0, batch: 1267 train-loss: 1.1711753606796265\n",
      "[LOG 20200511-10:15:58] epoch: 0, batch: 1268 train-loss: 1.9765677452087402\n",
      "[LOG 20200511-10:15:58] epoch: 0, batch: 1269 train-loss: 2.1338512897491455\n",
      "[LOG 20200511-10:15:58] epoch: 0, batch: 1270 train-loss: 1.8448705673217773\n",
      "[LOG 20200511-10:15:59] epoch: 0, batch: 1271 train-loss: 1.8348174095153809\n",
      "[LOG 20200511-10:15:59] epoch: 0, batch: 1272 train-loss: 1.9882746934890747\n",
      "[LOG 20200511-10:15:59] epoch: 0, batch: 1273 train-loss: 1.9178135395050049\n",
      "[LOG 20200511-10:15:59] epoch: 0, batch: 1274 train-loss: 2.094435214996338\n",
      "[LOG 20200511-10:15:59] epoch: 0, batch: 1275 train-loss: 1.3946129083633423\n",
      "[LOG 20200511-10:15:59] epoch: 0, batch: 1276 train-loss: 1.927056074142456\n",
      "[LOG 20200511-10:15:59] epoch: 0, batch: 1277 train-loss: 1.6334031820297241\n",
      "[LOG 20200511-10:15:59] epoch: 0, batch: 1278 train-loss: 1.440136432647705\n",
      "[LOG 20200511-10:15:59] epoch: 0, batch: 1279 train-loss: 1.575297474861145\n",
      "[LOG 20200511-10:16:00] epoch: 0, batch: 1280 train-loss: 1.332074522972107\n",
      "[LOG 20200511-10:16:00] epoch: 0, batch: 1281 train-loss: 0.9888253211975098\n",
      "[LOG 20200511-10:16:00] epoch: 0, batch: 1282 train-loss: 2.4155161380767822\n",
      "[LOG 20200511-10:16:00] epoch: 0, batch: 1283 train-loss: 1.5079500675201416\n",
      "[LOG 20200511-10:16:00] epoch: 0, batch: 1284 train-loss: 1.3878684043884277\n",
      "[LOG 20200511-10:16:00] epoch: 0, batch: 1285 train-loss: 1.862407922744751\n",
      "[LOG 20200511-10:16:00] epoch: 0, batch: 1286 train-loss: 1.0518670082092285\n",
      "[LOG 20200511-10:16:00] epoch: 0, batch: 1287 train-loss: 2.303194046020508\n",
      "[LOG 20200511-10:16:00] epoch: 0, batch: 1288 train-loss: 1.1908446550369263\n",
      "[LOG 20200511-10:16:01] epoch: 0, batch: 1289 train-loss: 1.3859832286834717\n",
      "[LOG 20200511-10:16:01] epoch: 0, batch: 1290 train-loss: 1.6825308799743652\n",
      "[LOG 20200511-10:16:01] epoch: 0, batch: 1291 train-loss: 1.0442674160003662\n",
      "[LOG 20200511-10:16:01] epoch: 0, batch: 1292 train-loss: 2.4161643981933594\n",
      "[LOG 20200511-10:16:01] epoch: 0, batch: 1293 train-loss: 2.4943554401397705\n",
      "[LOG 20200511-10:16:01] epoch: 0, batch: 1294 train-loss: 2.497831344604492\n",
      "[LOG 20200511-10:16:01] epoch: 0, batch: 1295 train-loss: 2.818734884262085\n",
      "[LOG 20200511-10:16:01] epoch: 0, batch: 1296 train-loss: 2.2046244144439697\n",
      "[LOG 20200511-10:16:02] epoch: 0, batch: 1297 train-loss: 2.084681987762451\n",
      "[LOG 20200511-10:16:02] epoch: 0, batch: 1298 train-loss: 1.8145239353179932\n",
      "[LOG 20200511-10:16:02] epoch: 0, batch: 1299 train-loss: 2.0996971130371094\n",
      "[LOG 20200511-10:16:02] epoch: 0, batch: 1300 train-loss: 1.8665153980255127\n",
      "[LOG 20200511-10:16:02] epoch: 0, batch: 1301 train-loss: 1.2563936710357666\n",
      "[LOG 20200511-10:16:02] epoch: 0, batch: 1302 train-loss: 1.2143874168395996\n",
      "[LOG 20200511-10:16:02] epoch: 0, batch: 1303 train-loss: 1.3464977741241455\n",
      "[LOG 20200511-10:16:02] epoch: 0, batch: 1304 train-loss: 1.3405910730361938\n",
      "[LOG 20200511-10:16:02] epoch: 0, batch: 1305 train-loss: 2.09289813041687\n",
      "[LOG 20200511-10:16:03] epoch: 0, batch: 1306 train-loss: 2.083110809326172\n",
      "[LOG 20200511-10:16:03] epoch: 0, batch: 1307 train-loss: 2.268716812133789\n",
      "[LOG 20200511-10:16:03] epoch: 0, batch: 1308 train-loss: 1.5839046239852905\n",
      "[LOG 20200511-10:16:03] epoch: 0, batch: 1309 train-loss: 1.9140104055404663\n",
      "[LOG 20200511-10:16:03] epoch: 0, batch: 1310 train-loss: 1.6175942420959473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:16:03] epoch: 0, batch: 1311 train-loss: 2.2751336097717285\n",
      "[LOG 20200511-10:16:03] epoch: 0, batch: 1312 train-loss: 1.3472157716751099\n",
      "[LOG 20200511-10:16:03] epoch: 0, batch: 1313 train-loss: 2.2767210006713867\n",
      "[LOG 20200511-10:16:04] epoch: 0, batch: 1314 train-loss: 1.367995262145996\n",
      "[LOG 20200511-10:16:04] epoch: 0, batch: 1315 train-loss: 2.009060859680176\n",
      "[LOG 20200511-10:16:04] epoch: 0, batch: 1316 train-loss: 1.3867055177688599\n",
      "[LOG 20200511-10:16:04] epoch: 0, batch: 1317 train-loss: 1.5627609491348267\n",
      "[LOG 20200511-10:16:04] epoch: 0, batch: 1318 train-loss: 2.1581478118896484\n",
      "[LOG 20200511-10:16:04] epoch: 0, batch: 1319 train-loss: 1.3078625202178955\n",
      "[LOG 20200511-10:16:04] epoch: 0, batch: 1320 train-loss: 1.1930516958236694\n",
      "[LOG 20200511-10:16:04] epoch: 0, batch: 1321 train-loss: 1.4329677820205688\n",
      "[LOG 20200511-10:16:04] epoch: 0, batch: 1322 train-loss: 1.7157039642333984\n",
      "[LOG 20200511-10:16:05] epoch: 0, batch: 1323 train-loss: 1.2979494333267212\n",
      "[LOG 20200511-10:16:05] epoch: 0, batch: 1324 train-loss: 1.8408763408660889\n",
      "[LOG 20200511-10:16:05] epoch: 0, batch: 1325 train-loss: 2.3392021656036377\n",
      "[LOG 20200511-10:16:05] epoch: 0, batch: 1326 train-loss: 2.426480770111084\n",
      "[LOG 20200511-10:16:05] epoch: 0, batch: 1327 train-loss: 1.79522705078125\n",
      "[LOG 20200511-10:16:05] epoch: 0, batch: 1328 train-loss: 1.7187178134918213\n",
      "[LOG 20200511-10:16:05] epoch: 0, batch: 1329 train-loss: 1.1360836029052734\n",
      "[LOG 20200511-10:16:05] epoch: 0, batch: 1330 train-loss: 1.8300538063049316\n",
      "[LOG 20200511-10:16:05] epoch: 0, batch: 1331 train-loss: 1.626514196395874\n",
      "[LOG 20200511-10:16:06] epoch: 0, batch: 1332 train-loss: 2.2686264514923096\n",
      "[LOG 20200511-10:16:06] epoch: 0, batch: 1333 train-loss: 2.308933734893799\n",
      "[LOG 20200511-10:16:06] epoch: 0, batch: 1334 train-loss: 2.062631130218506\n",
      "[LOG 20200511-10:16:06] epoch: 0, batch: 1335 train-loss: 1.4432740211486816\n",
      "[LOG 20200511-10:16:06] epoch: 0, batch: 1336 train-loss: 1.7588883638381958\n",
      "[LOG 20200511-10:16:06] epoch: 0, batch: 1337 train-loss: 2.6114585399627686\n",
      "[LOG 20200511-10:16:06] epoch: 0, batch: 1338 train-loss: 1.6663365364074707\n",
      "[LOG 20200511-10:16:06] epoch: 0, batch: 1339 train-loss: 1.7903156280517578\n",
      "[LOG 20200511-10:16:07] epoch: 0, batch: 1340 train-loss: 2.0310378074645996\n",
      "[LOG 20200511-10:16:07] epoch: 0, batch: 1341 train-loss: 1.352099061012268\n",
      "[LOG 20200511-10:16:07] epoch: 0, batch: 1342 train-loss: 2.374101400375366\n",
      "[LOG 20200511-10:16:07] epoch: 0, batch: 1343 train-loss: 1.9396016597747803\n",
      "[LOG 20200511-10:16:07] epoch: 0, batch: 1344 train-loss: 2.7021074295043945\n",
      "[LOG 20200511-10:16:07] epoch: 0, batch: 1345 train-loss: 1.419566035270691\n",
      "[LOG 20200511-10:16:07] epoch: 0, batch: 1346 train-loss: 1.4028220176696777\n",
      "[LOG 20200511-10:16:07] epoch: 0, batch: 1347 train-loss: 1.898716688156128\n",
      "[LOG 20200511-10:16:08] epoch: 0, batch: 1348 train-loss: 0.9880657196044922\n",
      "[LOG 20200511-10:16:08] epoch: 0, batch: 1349 train-loss: 1.049651026725769\n",
      "[LOG 20200511-10:16:08] epoch: 0, batch: 1350 train-loss: 1.9233932495117188\n",
      "[LOG 20200511-10:16:08] epoch: 0, batch: 1351 train-loss: 1.2761902809143066\n",
      "[LOG 20200511-10:16:08] epoch: 0, batch: 1352 train-loss: 0.9812180399894714\n",
      "[LOG 20200511-10:16:08] epoch: 0, batch: 1353 train-loss: 2.3921256065368652\n",
      "[LOG 20200511-10:16:08] epoch: 0, batch: 1354 train-loss: 1.5197255611419678\n",
      "[LOG 20200511-10:16:08] epoch: 0, batch: 1355 train-loss: 2.539515495300293\n",
      "[LOG 20200511-10:16:09] epoch: 0, batch: 1356 train-loss: 2.5060837268829346\n",
      "[LOG 20200511-10:16:09] epoch: 0, batch: 1357 train-loss: 1.5119669437408447\n",
      "[LOG 20200511-10:16:09] epoch: 0, batch: 1358 train-loss: 1.807510256767273\n",
      "[LOG 20200511-10:16:09] epoch: 0, batch: 1359 train-loss: 1.716224193572998\n",
      "[LOG 20200511-10:16:09] epoch: 0, batch: 1360 train-loss: 1.7388253211975098\n",
      "[LOG 20200511-10:16:09] epoch: 0, batch: 1361 train-loss: 1.664792776107788\n",
      "[LOG 20200511-10:16:09] epoch: 0, batch: 1362 train-loss: 1.3101006746292114\n",
      "[LOG 20200511-10:16:09] epoch: 0, batch: 1363 train-loss: 1.2899770736694336\n",
      "[LOG 20200511-10:16:09] epoch: 0, batch: 1364 train-loss: 2.177464008331299\n",
      "[LOG 20200511-10:16:10] epoch: 0, batch: 1365 train-loss: 1.3945460319519043\n",
      "[LOG 20200511-10:16:10] epoch: 0, batch: 1366 train-loss: 2.175096035003662\n",
      "[LOG 20200511-10:16:10] epoch: 0, batch: 1367 train-loss: 1.472495675086975\n",
      "[LOG 20200511-10:16:10] epoch: 0, batch: 1368 train-loss: 1.7993464469909668\n",
      "[LOG 20200511-10:16:10] epoch: 0, batch: 1369 train-loss: 1.0665425062179565\n",
      "[LOG 20200511-10:16:10] epoch: 0, batch: 1370 train-loss: 1.755397081375122\n",
      "[LOG 20200511-10:16:10] epoch: 0, batch: 1371 train-loss: 1.5552873611450195\n",
      "[LOG 20200511-10:16:10] epoch: 0, batch: 1372 train-loss: 2.0749917030334473\n",
      "[LOG 20200511-10:16:10] epoch: 0, batch: 1373 train-loss: 1.7473403215408325\n",
      "[LOG 20200511-10:16:11] epoch: 0, batch: 1374 train-loss: 2.6047511100769043\n",
      "[LOG 20200511-10:16:11] epoch: 0, batch: 1375 train-loss: 2.262888193130493\n",
      "[LOG 20200511-10:16:11] epoch: 0, batch: 1376 train-loss: 0.7973845601081848\n",
      "[LOG 20200511-10:16:11] epoch: 0, batch: 1377 train-loss: 2.2908778190612793\n",
      "[LOG 20200511-10:16:11] epoch: 0, batch: 1378 train-loss: 2.497365951538086\n",
      "[LOG 20200511-10:16:11] epoch: 0, batch: 1379 train-loss: 1.8528870344161987\n",
      "[LOG 20200511-10:16:11] epoch: 0, batch: 1380 train-loss: 2.1619601249694824\n",
      "[LOG 20200511-10:16:11] epoch: 0, batch: 1381 train-loss: 2.5121653079986572\n",
      "[LOG 20200511-10:16:12] epoch: 0, batch: 1382 train-loss: 1.0866198539733887\n",
      "[LOG 20200511-10:16:12] epoch: 0, batch: 1383 train-loss: 1.376321792602539\n",
      "[LOG 20200511-10:16:12] epoch: 0, batch: 1384 train-loss: 1.758726716041565\n",
      "[LOG 20200511-10:16:12] epoch: 0, batch: 1385 train-loss: 1.7108659744262695\n",
      "[LOG 20200511-10:16:12] epoch: 0, batch: 1386 train-loss: 2.113346576690674\n",
      "[LOG 20200511-10:16:12] epoch: 0, batch: 1387 train-loss: 1.4301855564117432\n",
      "[LOG 20200511-10:16:12] epoch: 0, batch: 1388 train-loss: 1.0256866216659546\n",
      "[LOG 20200511-10:16:12] epoch: 0, batch: 1389 train-loss: 2.3370254039764404\n",
      "[LOG 20200511-10:16:12] epoch: 0, batch: 1390 train-loss: 1.967642903327942\n",
      "[LOG 20200511-10:16:13] epoch: 0, batch: 1391 train-loss: 1.6280511617660522\n",
      "[LOG 20200511-10:16:13] epoch: 0, batch: 1392 train-loss: 1.2712349891662598\n",
      "[LOG 20200511-10:16:13] epoch: 0, batch: 1393 train-loss: 2.1693904399871826\n",
      "[LOG 20200511-10:16:13] epoch: 0, batch: 1394 train-loss: 1.2300249338150024\n",
      "[LOG 20200511-10:16:13] epoch: 0, batch: 1395 train-loss: 1.4307751655578613\n",
      "[LOG 20200511-10:16:13] epoch: 0, batch: 1396 train-loss: 2.538367509841919\n",
      "[LOG 20200511-10:16:13] epoch: 0, batch: 1397 train-loss: 2.094853401184082\n",
      "[LOG 20200511-10:16:13] epoch: 0, batch: 1398 train-loss: 1.311870813369751\n",
      "[LOG 20200511-10:16:14] epoch: 0, batch: 1399 train-loss: 1.915982961654663\n",
      "[LOG 20200511-10:16:14] epoch: 0, batch: 1400 train-loss: 1.5667355060577393\n",
      "[LOG 20200511-10:16:14] epoch: 0, batch: 1401 train-loss: 1.2053802013397217\n",
      "[LOG 20200511-10:16:14] epoch: 0, batch: 1402 train-loss: 1.2844984531402588\n",
      "[LOG 20200511-10:16:14] epoch: 0, batch: 1403 train-loss: 1.5765321254730225\n",
      "[LOG 20200511-10:16:14] epoch: 0, batch: 1404 train-loss: 2.0253000259399414\n",
      "[LOG 20200511-10:16:14] epoch: 0, batch: 1405 train-loss: 0.9824749231338501\n",
      "[LOG 20200511-10:16:14] epoch: 0, batch: 1406 train-loss: 1.1122204065322876\n",
      "[LOG 20200511-10:16:14] epoch: 0, batch: 1407 train-loss: 1.1262280941009521\n",
      "[LOG 20200511-10:16:15] epoch: 0, batch: 1408 train-loss: 1.5415844917297363\n",
      "[LOG 20200511-10:16:15] epoch: 0, batch: 1409 train-loss: 1.498349666595459\n",
      "[LOG 20200511-10:16:15] epoch: 0, batch: 1410 train-loss: 1.6499539613723755\n",
      "[LOG 20200511-10:16:15] epoch: 0, batch: 1411 train-loss: 0.7889497876167297\n",
      "[LOG 20200511-10:16:15] epoch: 0, batch: 1412 train-loss: 2.5420753955841064\n",
      "[LOG 20200511-10:16:15] epoch: 0, batch: 1413 train-loss: 2.0913045406341553\n",
      "[LOG 20200511-10:16:15] epoch: 0, batch: 1414 train-loss: 1.651228666305542\n",
      "[LOG 20200511-10:16:15] epoch: 0, batch: 1415 train-loss: 2.7428579330444336\n",
      "[LOG 20200511-10:16:15] epoch: 0, batch: 1416 train-loss: 1.0783798694610596\n",
      "[LOG 20200511-10:16:16] epoch: 0, batch: 1417 train-loss: 1.384298324584961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:16:16] epoch: 0, batch: 1418 train-loss: 1.0002021789550781\n",
      "[LOG 20200511-10:16:16] epoch: 0, batch: 1419 train-loss: 1.908136248588562\n",
      "[LOG 20200511-10:16:16] epoch: 0, batch: 1420 train-loss: 1.547131061553955\n",
      "[LOG 20200511-10:16:16] epoch: 0, batch: 1421 train-loss: 1.767810344696045\n",
      "[LOG 20200511-10:16:16] epoch: 0, batch: 1422 train-loss: 2.2995152473449707\n",
      "[LOG 20200511-10:16:16] epoch: 0, batch: 1423 train-loss: 1.7363392114639282\n",
      "[LOG 20200511-10:16:16] epoch: 0, batch: 1424 train-loss: 2.241741180419922\n",
      "[LOG 20200511-10:16:16] epoch: 0, batch: 1425 train-loss: 1.6334598064422607\n",
      "[LOG 20200511-10:16:17] epoch: 0, batch: 1426 train-loss: 1.6246057748794556\n",
      "[LOG 20200511-10:16:17] epoch: 0, batch: 1427 train-loss: 1.716772198677063\n",
      "[LOG 20200511-10:16:17] epoch: 0, batch: 1428 train-loss: 1.134040355682373\n",
      "[LOG 20200511-10:16:17] epoch: 0, batch: 1429 train-loss: 1.804157018661499\n",
      "[LOG 20200511-10:16:17] epoch: 0, batch: 1430 train-loss: 1.692829966545105\n",
      "[LOG 20200511-10:16:17] epoch: 0, batch: 1431 train-loss: 1.6407067775726318\n",
      "[LOG 20200511-10:16:17] epoch: 0, batch: 1432 train-loss: 2.056845188140869\n",
      "[LOG 20200511-10:16:17] epoch: 0, batch: 1433 train-loss: 1.375415325164795\n",
      "[LOG 20200511-10:16:17] epoch: 0, batch: 1434 train-loss: 1.871246337890625\n",
      "[LOG 20200511-10:16:18] epoch: 0, batch: 1435 train-loss: 1.1661361455917358\n",
      "[LOG 20200511-10:16:18] epoch: 0, batch: 1436 train-loss: 2.2342476844787598\n",
      "[LOG 20200511-10:16:18] epoch: 0, batch: 1437 train-loss: 2.242121696472168\n",
      "[LOG 20200511-10:16:18] epoch: 0, batch: 1438 train-loss: 1.8929787874221802\n",
      "[LOG 20200511-10:16:18] epoch: 0, batch: 1439 train-loss: 0.9416382908821106\n",
      "[LOG 20200511-10:16:18] epoch: 0, batch: 1440 train-loss: 1.3972828388214111\n",
      "[LOG 20200511-10:16:18] epoch: 0, batch: 1441 train-loss: 0.9673311710357666\n",
      "[LOG 20200511-10:16:18] epoch: 0, batch: 1442 train-loss: 1.5923993587493896\n",
      "[LOG 20200511-10:16:18] epoch: 0, batch: 1443 train-loss: 1.3521530628204346\n",
      "[LOG 20200511-10:16:18] epoch: 0, batch: 1444 train-loss: 1.192758560180664\n",
      "[LOG 20200511-10:16:19] epoch: 0, batch: 1445 train-loss: 1.315405011177063\n",
      "[LOG 20200511-10:16:19] epoch: 0, batch: 1446 train-loss: 1.3372080326080322\n",
      "[LOG 20200511-10:16:19] epoch: 0, batch: 1447 train-loss: 1.9651144742965698\n",
      "[LOG 20200511-10:16:19] epoch: 0, batch: 1448 train-loss: 1.7407649755477905\n",
      "[LOG 20200511-10:16:19] epoch: 0, batch: 1449 train-loss: 0.7136515378952026\n",
      "[LOG 20200511-10:16:19] epoch: 0, batch: 1450 train-loss: 1.7956047058105469\n",
      "[LOG 20200511-10:16:19] epoch: 0, batch: 1451 train-loss: 1.7036023139953613\n",
      "[LOG 20200511-10:16:19] epoch: 0, batch: 1452 train-loss: 1.5028456449508667\n",
      "[LOG 20200511-10:16:20] epoch: 0, batch: 1453 train-loss: 1.3487173318862915\n",
      "[LOG 20200511-10:16:20] epoch: 0, batch: 1454 train-loss: 1.61250901222229\n",
      "[LOG 20200511-10:16:20] epoch: 0, batch: 1455 train-loss: 1.932264804840088\n",
      "[LOG 20200511-10:16:20] epoch: 0, batch: 1456 train-loss: 1.8667900562286377\n",
      "[LOG 20200511-10:16:20] epoch: 0, batch: 1457 train-loss: 1.1901130676269531\n",
      "[LOG 20200511-10:16:20] epoch: 0, batch: 1458 train-loss: 1.5787780284881592\n",
      "[LOG 20200511-10:16:20] epoch: 0, batch: 1459 train-loss: 1.7185269594192505\n",
      "[LOG 20200511-10:16:20] epoch: 0, batch: 1460 train-loss: 2.366346597671509\n",
      "[LOG 20200511-10:16:20] epoch: 0, batch: 1461 train-loss: 1.5454506874084473\n",
      "[LOG 20200511-10:16:21] epoch: 0, batch: 1462 train-loss: 1.8951139450073242\n",
      "[LOG 20200511-10:16:21] epoch: 0, batch: 1463 train-loss: 1.4154365062713623\n",
      "[LOG 20200511-10:16:21] epoch: 0, batch: 1464 train-loss: 1.6784440279006958\n",
      "[LOG 20200511-10:16:21] epoch: 0, batch: 1465 train-loss: 1.5658214092254639\n",
      "[LOG 20200511-10:16:21] epoch: 0, batch: 1466 train-loss: 2.160525321960449\n",
      "[LOG 20200511-10:16:21] epoch: 0, batch: 1467 train-loss: 1.9949595928192139\n",
      "[LOG 20200511-10:16:21] epoch: 0, batch: 1468 train-loss: 2.34207820892334\n",
      "[LOG 20200511-10:16:21] epoch: 0, batch: 1469 train-loss: 1.37269127368927\n",
      "[LOG 20200511-10:16:21] epoch: 0, batch: 1470 train-loss: 1.6771141290664673\n",
      "[LOG 20200511-10:16:22] epoch: 0, batch: 1471 train-loss: 1.667909860610962\n",
      "[LOG 20200511-10:16:22] epoch: 0, batch: 1472 train-loss: 0.8106243014335632\n",
      "[LOG 20200511-10:16:22] epoch: 0, batch: 1473 train-loss: 1.8406637907028198\n",
      "[LOG 20200511-10:16:22] epoch: 0, batch: 1474 train-loss: 0.8576798439025879\n",
      "[LOG 20200511-10:16:22] epoch: 0, batch: 1475 train-loss: 1.7242830991744995\n",
      "[LOG 20200511-10:16:22] epoch: 0, batch: 1476 train-loss: 2.418853998184204\n",
      "[LOG 20200511-10:16:22] epoch: 0, batch: 1477 train-loss: 1.812849998474121\n",
      "[LOG 20200511-10:16:22] epoch: 0, batch: 1478 train-loss: 1.4936821460723877\n",
      "[LOG 20200511-10:16:22] epoch: 0, batch: 1479 train-loss: 1.7901053428649902\n",
      "[LOG 20200511-10:16:23] epoch: 0, batch: 1480 train-loss: 1.4773696660995483\n",
      "[LOG 20200511-10:16:23] epoch: 0, batch: 1481 train-loss: 1.2205952405929565\n",
      "[LOG 20200511-10:16:23] epoch: 0, batch: 1482 train-loss: 1.3723946809768677\n",
      "[LOG 20200511-10:16:23] epoch: 0, batch: 1483 train-loss: 1.9134116172790527\n",
      "[LOG 20200511-10:16:23] epoch: 0, batch: 1484 train-loss: 1.2633131742477417\n",
      "[LOG 20200511-10:16:23] epoch: 0, batch: 1485 train-loss: 1.0729436874389648\n",
      "[LOG 20200511-10:16:23] epoch: 0, batch: 1486 train-loss: 2.451663017272949\n",
      "[LOG 20200511-10:16:23] epoch: 0, batch: 1487 train-loss: 1.8325384855270386\n",
      "[LOG 20200511-10:16:23] epoch: 0, batch: 1488 train-loss: 2.4980340003967285\n",
      "[LOG 20200511-10:16:24] epoch: 0, batch: 1489 train-loss: 1.8400216102600098\n",
      "[LOG 20200511-10:16:24] epoch: 0, batch: 1490 train-loss: 1.2974464893341064\n",
      "[LOG 20200511-10:16:24] epoch: 0, batch: 1491 train-loss: 1.35097074508667\n",
      "[LOG 20200511-10:16:24] epoch: 0, batch: 1492 train-loss: 2.1559393405914307\n",
      "[LOG 20200511-10:16:24] epoch: 0, batch: 1493 train-loss: 2.051246166229248\n",
      "[LOG 20200511-10:16:24] epoch: 0, batch: 1494 train-loss: 1.7308897972106934\n",
      "[LOG 20200511-10:16:24] epoch: 0, batch: 1495 train-loss: 0.9397656917572021\n",
      "[LOG 20200511-10:16:24] epoch: 0, batch: 1496 train-loss: 2.3337955474853516\n",
      "[LOG 20200511-10:16:24] epoch: 0, batch: 1497 train-loss: 2.2201619148254395\n",
      "[LOG 20200511-10:16:25] epoch: 0, batch: 1498 train-loss: 2.0103278160095215\n",
      "[LOG 20200511-10:16:25] epoch: 0, batch: 1499 train-loss: 1.245639443397522\n",
      "[LOG 20200511-10:16:25] epoch: 0, batch: 1500 train-loss: 1.2956044673919678\n",
      "[LOG 20200511-10:16:25] epoch: 0, batch: 1501 train-loss: 3.0394773483276367\n",
      "[LOG 20200511-10:16:25] epoch: 0, batch: 1502 train-loss: 1.0484473705291748\n",
      "[LOG 20200511-10:16:25] epoch: 0, batch: 1503 train-loss: 1.720833659172058\n",
      "[LOG 20200511-10:16:25] epoch: 0, batch: 1504 train-loss: 0.9960659742355347\n",
      "[LOG 20200511-10:16:25] epoch: 0, batch: 1505 train-loss: 1.9646369218826294\n",
      "[LOG 20200511-10:16:25] epoch: 0, batch: 1506 train-loss: 2.500229835510254\n",
      "[LOG 20200511-10:16:26] epoch: 0, batch: 1507 train-loss: 1.542472243309021\n",
      "[LOG 20200511-10:16:26] epoch: 0, batch: 1508 train-loss: 2.7058334350585938\n",
      "[LOG 20200511-10:16:26] epoch: 0, batch: 1509 train-loss: 2.288182020187378\n",
      "[LOG 20200511-10:16:26] epoch: 0, batch: 1510 train-loss: 1.0781338214874268\n",
      "[LOG 20200511-10:16:26] epoch: 0, batch: 1511 train-loss: 1.7906196117401123\n",
      "[LOG 20200511-10:16:26] epoch: 0, batch: 1512 train-loss: 1.9900636672973633\n",
      "[LOG 20200511-10:16:26] epoch: 0, batch: 1513 train-loss: 2.984604835510254\n",
      "[LOG 20200511-10:16:26] epoch: 0, batch: 1514 train-loss: 1.1582509279251099\n",
      "[LOG 20200511-10:16:27] epoch: 0, batch: 1515 train-loss: 2.1290178298950195\n",
      "[LOG 20200511-10:16:27] epoch: 0, batch: 1516 train-loss: 1.9922035932540894\n",
      "[LOG 20200511-10:16:27] epoch: 0, batch: 1517 train-loss: 1.3084444999694824\n",
      "[LOG 20200511-10:16:27] epoch: 0, batch: 1518 train-loss: 1.714583396911621\n",
      "[LOG 20200511-10:16:27] epoch: 0, batch: 1519 train-loss: 2.138582229614258\n",
      "[LOG 20200511-10:16:27] epoch: 0, batch: 1520 train-loss: 1.9943394660949707\n",
      "[LOG 20200511-10:16:27] epoch: 0, batch: 1521 train-loss: 2.2535815238952637\n",
      "[LOG 20200511-10:16:27] epoch: 0, batch: 1522 train-loss: 1.4008572101593018\n",
      "[LOG 20200511-10:16:27] epoch: 0, batch: 1523 train-loss: 1.2198635339736938\n",
      "[LOG 20200511-10:16:28] epoch: 0, batch: 1524 train-loss: 2.162588596343994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:16:28] epoch: 0, batch: 1525 train-loss: 1.4869999885559082\n",
      "[LOG 20200511-10:16:28] epoch: 0, batch: 1526 train-loss: 0.7434726357460022\n",
      "[LOG 20200511-10:16:28] epoch: 0, batch: 1527 train-loss: 1.8879475593566895\n",
      "[LOG 20200511-10:16:28] epoch: 0, batch: 1528 train-loss: 1.7858810424804688\n",
      "[LOG 20200511-10:16:28] epoch: 0, batch: 1529 train-loss: 1.9963188171386719\n",
      "[LOG 20200511-10:16:28] epoch: 0, batch: 1530 train-loss: 1.8850963115692139\n",
      "[LOG 20200511-10:16:28] epoch: 0, batch: 1531 train-loss: 1.9764330387115479\n",
      "[LOG 20200511-10:16:28] epoch: 0, batch: 1532 train-loss: 1.5335633754730225\n",
      "[LOG 20200511-10:16:29] epoch: 0, batch: 1533 train-loss: 1.7970499992370605\n",
      "[LOG 20200511-10:16:29] epoch: 0, batch: 1534 train-loss: 2.331082582473755\n",
      "[LOG 20200511-10:16:29] epoch: 0, batch: 1535 train-loss: 1.6559433937072754\n",
      "[LOG 20200511-10:16:29] epoch: 0, batch: 1536 train-loss: 0.9060202836990356\n",
      "[LOG 20200511-10:16:29] epoch: 0, batch: 1537 train-loss: 0.980540931224823\n",
      "[LOG 20200511-10:16:29] epoch: 0, batch: 1538 train-loss: 1.5985164642333984\n",
      "[LOG 20200511-10:16:29] epoch: 0, batch: 1539 train-loss: 1.4436794519424438\n",
      "[LOG 20200511-10:16:29] epoch: 0, batch: 1540 train-loss: 1.517527461051941\n",
      "[LOG 20200511-10:16:29] epoch: 0, batch: 1541 train-loss: 2.211509943008423\n",
      "[LOG 20200511-10:16:30] epoch: 0, batch: 1542 train-loss: 2.1852943897247314\n",
      "[LOG 20200511-10:16:30] epoch: 0, batch: 1543 train-loss: 2.4478774070739746\n",
      "[LOG 20200511-10:16:30] epoch: 0, batch: 1544 train-loss: 1.1313683986663818\n",
      "[LOG 20200511-10:16:30] epoch: 0, batch: 1545 train-loss: 1.6323689222335815\n",
      "[LOG 20200511-10:16:30] epoch: 0, batch: 1546 train-loss: 1.8566827774047852\n",
      "[LOG 20200511-10:16:30] epoch: 0, batch: 1547 train-loss: 1.1088136434555054\n",
      "[LOG 20200511-10:16:30] epoch: 0, batch: 1548 train-loss: 1.7416162490844727\n",
      "[LOG 20200511-10:16:30] epoch: 0, batch: 1549 train-loss: 2.6895079612731934\n",
      "[LOG 20200511-10:16:31] epoch: 0, batch: 1550 train-loss: 1.278854250907898\n",
      "[LOG 20200511-10:16:31] epoch: 0, batch: 1551 train-loss: 0.6350444555282593\n",
      "[LOG 20200511-10:16:31] epoch: 0, batch: 1552 train-loss: 1.1497888565063477\n",
      "[LOG 20200511-10:16:31] epoch: 0, batch: 1553 train-loss: 1.7634532451629639\n",
      "[LOG 20200511-10:16:31] epoch: 0, batch: 1554 train-loss: 1.5739398002624512\n",
      "[LOG 20200511-10:16:31] epoch: 0, batch: 1555 train-loss: 1.58273184299469\n",
      "[LOG 20200511-10:16:31] epoch: 0, batch: 1556 train-loss: 1.5736228227615356\n",
      "[LOG 20200511-10:16:31] epoch: 0, batch: 1557 train-loss: 2.6180315017700195\n",
      "[LOG 20200511-10:16:31] epoch: 0, batch: 1558 train-loss: 1.013001561164856\n",
      "[LOG 20200511-10:16:32] epoch: 0, batch: 1559 train-loss: 2.0113511085510254\n",
      "[LOG 20200511-10:16:32] epoch: 0, batch: 1560 train-loss: 2.2640883922576904\n",
      "[LOG 20200511-10:16:32] epoch: 0, batch: 1561 train-loss: 1.3866782188415527\n",
      "[LOG 20200511-10:16:32] epoch: 0, batch: 1562 train-loss: 1.7042677402496338\n",
      "[LOG 20200511-10:16:32] epoch: 0, batch: 1563 train-loss: 2.0956830978393555\n",
      "[LOG 20200511-10:16:32] epoch: 0, batch: 1564 train-loss: 1.0991672277450562\n",
      "[LOG 20200511-10:16:32] epoch: 0, batch: 1565 train-loss: 2.4411375522613525\n",
      "[LOG 20200511-10:16:32] epoch: 0, batch: 1566 train-loss: 1.0573151111602783\n",
      "[LOG 20200511-10:16:33] epoch: 0, batch: 1567 train-loss: 2.2543752193450928\n",
      "[LOG 20200511-10:16:33] epoch: 0, batch: 1568 train-loss: 1.3585163354873657\n",
      "[LOG 20200511-10:16:33] epoch: 0, batch: 1569 train-loss: 1.6784851551055908\n",
      "[LOG 20200511-10:16:33] epoch: 0, batch: 1570 train-loss: 1.4897644519805908\n",
      "[LOG 20200511-10:16:33] epoch: 0, batch: 1571 train-loss: 2.0165247917175293\n",
      "[LOG 20200511-10:16:33] epoch: 0, batch: 1572 train-loss: 2.025113344192505\n",
      "[LOG 20200511-10:16:33] epoch: 0, batch: 1573 train-loss: 1.5283114910125732\n",
      "[LOG 20200511-10:16:33] epoch: 0, batch: 1574 train-loss: 1.4871301651000977\n",
      "[LOG 20200511-10:16:33] epoch: 0, batch: 1575 train-loss: 1.3185962438583374\n",
      "[LOG 20200511-10:16:34] epoch: 0, batch: 1576 train-loss: 1.8124537467956543\n",
      "[LOG 20200511-10:16:34] epoch: 0, batch: 1577 train-loss: 2.3159823417663574\n",
      "[LOG 20200511-10:16:34] epoch: 0, batch: 1578 train-loss: 1.0230727195739746\n",
      "[LOG 20200511-10:16:34] epoch: 0, batch: 1579 train-loss: 1.503117561340332\n",
      "[LOG 20200511-10:16:34] epoch: 0, batch: 1580 train-loss: 1.9339152574539185\n",
      "[LOG 20200511-10:16:34] epoch: 0, batch: 1581 train-loss: 1.9188735485076904\n",
      "[LOG 20200511-10:16:34] epoch: 0, batch: 1582 train-loss: 1.495252251625061\n",
      "[LOG 20200511-10:16:34] epoch: 0, batch: 1583 train-loss: 0.958875298500061\n",
      "[LOG 20200511-10:16:35] epoch: 0, batch: 1584 train-loss: 1.150216817855835\n",
      "[LOG 20200511-10:16:35] epoch: 0, batch: 1585 train-loss: 1.894203782081604\n",
      "[LOG 20200511-10:16:35] epoch: 0, batch: 1586 train-loss: 1.69947350025177\n",
      "[LOG 20200511-10:16:35] epoch: 0, batch: 1587 train-loss: 2.193472385406494\n",
      "[LOG 20200511-10:16:35] epoch: 0, batch: 1588 train-loss: 1.355954647064209\n",
      "[LOG 20200511-10:16:35] epoch: 0, batch: 1589 train-loss: 2.0701541900634766\n",
      "[LOG 20200511-10:16:35] epoch: 0, batch: 1590 train-loss: 2.013824462890625\n",
      "[LOG 20200511-10:16:35] epoch: 0, batch: 1591 train-loss: 1.7980916500091553\n",
      "[LOG 20200511-10:16:36] epoch: 0, batch: 1592 train-loss: 1.8635258674621582\n",
      "[LOG 20200511-10:16:36] epoch: 0, batch: 1593 train-loss: 1.4154607057571411\n",
      "[LOG 20200511-10:16:36] epoch: 0, batch: 1594 train-loss: 1.8527133464813232\n",
      "[LOG 20200511-10:16:36] epoch: 0, batch: 1595 train-loss: 1.0635024309158325\n",
      "[LOG 20200511-10:16:36] epoch: 0, batch: 1596 train-loss: 1.689129114151001\n",
      "[LOG 20200511-10:16:36] epoch: 0, batch: 1597 train-loss: 1.2170488834381104\n",
      "[LOG 20200511-10:16:36] epoch: 0, batch: 1598 train-loss: 1.7568244934082031\n",
      "[LOG 20200511-10:16:36] epoch: 0, batch: 1599 train-loss: 0.9343783855438232\n",
      "[LOG 20200511-10:16:36] epoch: 0, batch: 1600 train-loss: 1.5381560325622559\n",
      "[LOG 20200511-10:16:37] epoch: 0, batch: 1601 train-loss: 1.99356210231781\n",
      "[LOG 20200511-10:16:37] epoch: 0, batch: 1602 train-loss: 1.721326470375061\n",
      "[LOG 20200511-10:16:37] epoch: 0, batch: 1603 train-loss: 1.4846543073654175\n",
      "[LOG 20200511-10:16:37] epoch: 0, batch: 1604 train-loss: 1.5265252590179443\n",
      "[LOG 20200511-10:16:37] epoch: 0, batch: 1605 train-loss: 1.545122504234314\n",
      "[LOG 20200511-10:16:37] epoch: 0, batch: 1606 train-loss: 0.83376145362854\n",
      "[LOG 20200511-10:16:37] epoch: 0, batch: 1607 train-loss: 2.273043632507324\n",
      "[LOG 20200511-10:16:37] epoch: 0, batch: 1608 train-loss: 1.5939009189605713\n",
      "[LOG 20200511-10:16:37] epoch: 0, batch: 1609 train-loss: 1.8624839782714844\n",
      "[LOG 20200511-10:16:38] epoch: 0, batch: 1610 train-loss: 1.3035099506378174\n",
      "[LOG 20200511-10:16:38] epoch: 0, batch: 1611 train-loss: 1.4418474435806274\n",
      "[LOG 20200511-10:16:38] epoch: 0, batch: 1612 train-loss: 1.603304386138916\n",
      "[LOG 20200511-10:16:38] epoch: 0, batch: 1613 train-loss: 1.5203423500061035\n",
      "[LOG 20200511-10:16:38] epoch: 0, batch: 1614 train-loss: 1.6968283653259277\n",
      "[LOG 20200511-10:16:38] epoch: 0, batch: 1615 train-loss: 1.6001397371292114\n",
      "[LOG 20200511-10:16:38] epoch: 0, batch: 1616 train-loss: 1.7134404182434082\n",
      "[LOG 20200511-10:16:38] epoch: 0, batch: 1617 train-loss: 1.4964390993118286\n",
      "[LOG 20200511-10:16:38] epoch: 0, batch: 1618 train-loss: 1.5379962921142578\n",
      "[LOG 20200511-10:16:39] epoch: 0, batch: 1619 train-loss: 1.1559419631958008\n",
      "[LOG 20200511-10:16:39] epoch: 0, batch: 1620 train-loss: 1.4345026016235352\n",
      "[LOG 20200511-10:16:39] epoch: 0, batch: 1621 train-loss: 1.1955254077911377\n",
      "[LOG 20200511-10:16:39] epoch: 0, batch: 1622 train-loss: 1.152504801750183\n",
      "[LOG 20200511-10:16:39] epoch: 0, batch: 1623 train-loss: 1.7051339149475098\n",
      "[LOG 20200511-10:16:39] epoch: 0, batch: 1624 train-loss: 2.3731064796447754\n",
      "[LOG 20200511-10:16:39] epoch: 0, batch: 1625 train-loss: 1.110270619392395\n",
      "[LOG 20200511-10:16:39] epoch: 0, batch: 1626 train-loss: 1.964901089668274\n",
      "[LOG 20200511-10:16:39] epoch: 0, batch: 1627 train-loss: 1.8267837762832642\n",
      "[LOG 20200511-10:16:40] epoch: 0, batch: 1628 train-loss: 1.910029411315918\n",
      "[LOG 20200511-10:16:40] epoch: 0, batch: 1629 train-loss: 2.050773859024048\n",
      "[LOG 20200511-10:16:40] epoch: 0, batch: 1630 train-loss: 2.1838998794555664\n",
      "[LOG 20200511-10:16:40] epoch: 0, batch: 1631 train-loss: 1.5030417442321777\n",
      "[LOG 20200511-10:16:40] epoch: 0, batch: 1632 train-loss: 1.243430733680725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:16:40] epoch: 0, batch: 1633 train-loss: 2.196692705154419\n",
      "[LOG 20200511-10:16:40] epoch: 0, batch: 1634 train-loss: 1.3739691972732544\n",
      "[LOG 20200511-10:16:40] epoch: 0, batch: 1635 train-loss: 1.796375036239624\n",
      "[LOG 20200511-10:16:41] epoch: 0, batch: 1636 train-loss: 1.8165075778961182\n",
      "[LOG 20200511-10:16:41] epoch: 0, batch: 1637 train-loss: 1.3775084018707275\n",
      "[LOG 20200511-10:16:41] epoch: 0, batch: 1638 train-loss: 1.3783591985702515\n",
      "[LOG 20200511-10:16:41] epoch: 0, batch: 1639 train-loss: 1.9707763195037842\n",
      "[LOG 20200511-10:16:41] epoch: 0, batch: 1640 train-loss: 2.108383893966675\n",
      "[LOG 20200511-10:16:41] epoch: 0, batch: 1641 train-loss: 2.2245261669158936\n",
      "[LOG 20200511-10:16:41] epoch: 0, batch: 1642 train-loss: 1.5546514987945557\n",
      "[LOG 20200511-10:16:41] epoch: 0, batch: 1643 train-loss: 2.0562057495117188\n",
      "[LOG 20200511-10:16:41] epoch: 0, batch: 1644 train-loss: 1.506951093673706\n",
      "[LOG 20200511-10:16:41] epoch: 0, batch: 1645 train-loss: 1.2825566530227661\n",
      "[LOG 20200511-10:16:42] epoch: 0, batch: 1646 train-loss: 1.8487681150436401\n",
      "[LOG 20200511-10:16:42] epoch: 0, batch: 1647 train-loss: 1.2939435243606567\n",
      "[LOG 20200511-10:16:42] epoch: 0, batch: 1648 train-loss: 0.811256468296051\n",
      "[LOG 20200511-10:16:42] epoch: 0, batch: 1649 train-loss: 1.5365122556686401\n",
      "[LOG 20200511-10:16:42] epoch: 0, batch: 1650 train-loss: 1.3255443572998047\n",
      "[LOG 20200511-10:16:42] epoch: 0, batch: 1651 train-loss: 1.474366545677185\n",
      "[LOG 20200511-10:16:42] epoch: 0, batch: 1652 train-loss: 1.7884098291397095\n",
      "[LOG 20200511-10:16:42] epoch: 0, batch: 1653 train-loss: 1.27987802028656\n",
      "[LOG 20200511-10:16:42] epoch: 0, batch: 1654 train-loss: 1.9046688079833984\n",
      "[LOG 20200511-10:16:43] epoch: 0, batch: 1655 train-loss: 1.5709459781646729\n",
      "[LOG 20200511-10:16:43] epoch: 0, batch: 1656 train-loss: 1.625570297241211\n",
      "[LOG 20200511-10:16:43] epoch: 0, batch: 1657 train-loss: 1.7042837142944336\n",
      "[LOG 20200511-10:16:43] epoch: 0, batch: 1658 train-loss: 1.3445680141448975\n",
      "[LOG 20200511-10:16:43] epoch: 0, batch: 1659 train-loss: 1.4418840408325195\n",
      "[LOG 20200511-10:16:43] epoch: 0, batch: 1660 train-loss: 1.3693699836730957\n",
      "[LOG 20200511-10:16:43] epoch: 0, batch: 1661 train-loss: 1.4926649332046509\n",
      "[LOG 20200511-10:16:43] epoch: 0, batch: 1662 train-loss: 1.2170605659484863\n",
      "[LOG 20200511-10:16:43] epoch: 0, batch: 1663 train-loss: 1.6111727952957153\n",
      "[LOG 20200511-10:16:44] epoch: 0, batch: 1664 train-loss: 1.5000722408294678\n",
      "[LOG 20200511-10:16:44] epoch: 0, batch: 1665 train-loss: 2.213123321533203\n",
      "[LOG 20200511-10:16:44] epoch: 0, batch: 1666 train-loss: 1.7705209255218506\n",
      "[LOG 20200511-10:16:44] epoch: 0, batch: 1667 train-loss: 1.8929393291473389\n",
      "[LOG 20200511-10:16:44] epoch: 0, batch: 1668 train-loss: 1.8615264892578125\n",
      "[LOG 20200511-10:16:44] epoch: 0, batch: 1669 train-loss: 1.563525915145874\n",
      "[LOG 20200511-10:16:44] epoch: 0, batch: 1670 train-loss: 1.7912564277648926\n",
      "[LOG 20200511-10:16:44] epoch: 0, batch: 1671 train-loss: 2.1402857303619385\n",
      "[LOG 20200511-10:16:45] epoch: 0, batch: 1672 train-loss: 1.929950475692749\n",
      "[LOG 20200511-10:16:45] epoch: 0, batch: 1673 train-loss: 1.313012957572937\n",
      "[LOG 20200511-10:16:45] epoch: 0, batch: 1674 train-loss: 1.3819735050201416\n",
      "[LOG 20200511-10:16:45] epoch: 0, batch: 1675 train-loss: 1.9001166820526123\n",
      "[LOG 20200511-10:16:45] epoch: 0, batch: 1676 train-loss: 2.1663570404052734\n",
      "[LOG 20200511-10:16:45] epoch: 0, batch: 1677 train-loss: 1.6746317148208618\n",
      "[LOG 20200511-10:16:45] epoch: 0, batch: 1678 train-loss: 1.8139243125915527\n",
      "[LOG 20200511-10:16:45] epoch: 0, batch: 1679 train-loss: 2.4752607345581055\n",
      "[LOG 20200511-10:16:45] epoch: 0, batch: 1680 train-loss: 1.8135029077529907\n",
      "[LOG 20200511-10:16:46] epoch: 0, batch: 1681 train-loss: 1.2516920566558838\n",
      "[LOG 20200511-10:16:46] epoch: 0, batch: 1682 train-loss: 1.4151978492736816\n",
      "[LOG 20200511-10:16:46] epoch: 0, batch: 1683 train-loss: 1.890537977218628\n",
      "[LOG 20200511-10:16:46] epoch: 0, batch: 1684 train-loss: 1.6844828128814697\n",
      "[LOG 20200511-10:16:46] epoch: 0, batch: 1685 train-loss: 1.4379708766937256\n",
      "[LOG 20200511-10:16:46] epoch: 0, batch: 1686 train-loss: 1.3012559413909912\n",
      "[LOG 20200511-10:16:46] epoch: 0, batch: 1687 train-loss: 2.2483115196228027\n",
      "[LOG 20200511-10:16:46] epoch: 0, batch: 1688 train-loss: 2.3819026947021484\n",
      "[LOG 20200511-10:16:46] epoch: 0, batch: 1689 train-loss: 2.10073184967041\n",
      "[LOG 20200511-10:16:47] epoch: 0, batch: 1690 train-loss: 0.9167007207870483\n",
      "[LOG 20200511-10:16:47] epoch: 0, batch: 1691 train-loss: 1.7393689155578613\n",
      "[LOG 20200511-10:16:47] epoch: 0, batch: 1692 train-loss: 1.8448078632354736\n",
      "[LOG 20200511-10:16:47] epoch: 0, batch: 1693 train-loss: 1.5414704084396362\n",
      "[LOG 20200511-10:16:47] epoch: 0, batch: 1694 train-loss: 1.3594188690185547\n",
      "[LOG 20200511-10:16:47] epoch: 0, batch: 1695 train-loss: 1.7283086776733398\n",
      "[LOG 20200511-10:16:47] epoch: 0, batch: 1696 train-loss: 2.2136425971984863\n",
      "[LOG 20200511-10:16:47] epoch: 0, batch: 1697 train-loss: 0.9682345390319824\n",
      "[LOG 20200511-10:16:47] epoch: 0, batch: 1698 train-loss: 1.5598454475402832\n",
      "[LOG 20200511-10:16:48] epoch: 0, batch: 1699 train-loss: 2.0838229656219482\n",
      "[LOG 20200511-10:16:48] epoch: 0, batch: 1700 train-loss: 1.6876353025436401\n",
      "[LOG 20200511-10:16:48] epoch: 0, batch: 1701 train-loss: 2.559878349304199\n",
      "[LOG 20200511-10:16:48] epoch: 0, batch: 1702 train-loss: 0.8012425303459167\n",
      "[LOG 20200511-10:16:48] epoch: 0, batch: 1703 train-loss: 3.1106150150299072\n",
      "[LOG 20200511-10:16:48] epoch: 0, batch: 1704 train-loss: 0.9907940030097961\n",
      "[LOG 20200511-10:16:48] epoch: 0, batch: 1705 train-loss: 2.741501808166504\n",
      "[LOG 20200511-10:16:48] epoch: 0, batch: 1706 train-loss: 1.7675909996032715\n",
      "[LOG 20200511-10:16:49] epoch: 0, batch: 1707 train-loss: 0.9166125655174255\n",
      "[LOG 20200511-10:16:49] epoch: 0, batch: 1708 train-loss: 1.524013638496399\n",
      "[LOG 20200511-10:16:49] epoch: 0, batch: 1709 train-loss: 2.7852725982666016\n",
      "[LOG 20200511-10:16:49] epoch: 0, batch: 1710 train-loss: 2.084467887878418\n",
      "[LOG 20200511-10:16:49] epoch: 0, batch: 1711 train-loss: 1.1292383670806885\n",
      "[LOG 20200511-10:16:49] epoch: 0, batch: 1712 train-loss: 0.9377922415733337\n",
      "[LOG 20200511-10:16:49] epoch: 0, batch: 1713 train-loss: 1.4855014085769653\n",
      "[LOG 20200511-10:16:49] epoch: 0, batch: 1714 train-loss: 1.6566033363342285\n",
      "[LOG 20200511-10:16:49] epoch: 0, batch: 1715 train-loss: 1.7175803184509277\n",
      "[LOG 20200511-10:16:50] epoch: 0, batch: 1716 train-loss: 1.6761747598648071\n",
      "[LOG 20200511-10:16:50] epoch: 0, batch: 1717 train-loss: 1.2192310094833374\n",
      "[LOG 20200511-10:16:50] epoch: 0, batch: 1718 train-loss: 1.2838330268859863\n",
      "[LOG 20200511-10:16:50] epoch: 0, batch: 1719 train-loss: 1.261289119720459\n",
      "[LOG 20200511-10:16:50] epoch: 0, batch: 1720 train-loss: 1.4626502990722656\n",
      "[LOG 20200511-10:16:50] epoch: 0, batch: 1721 train-loss: 1.467606544494629\n",
      "[LOG 20200511-10:16:50] epoch: 0, batch: 1722 train-loss: 2.726351737976074\n",
      "[LOG 20200511-10:16:50] epoch: 0, batch: 1723 train-loss: 1.5291049480438232\n",
      "[LOG 20200511-10:16:50] epoch: 0, batch: 1724 train-loss: 0.9902359843254089\n",
      "[LOG 20200511-10:16:51] epoch: 0, batch: 1725 train-loss: 1.433502197265625\n",
      "[LOG 20200511-10:16:51] epoch: 0, batch: 1726 train-loss: 1.6812063455581665\n",
      "[LOG 20200511-10:16:51] epoch: 0, batch: 1727 train-loss: 2.0600039958953857\n",
      "[LOG 20200511-10:16:51] epoch: 0, batch: 1728 train-loss: 1.6217877864837646\n",
      "[LOG 20200511-10:16:51] epoch: 0, batch: 1729 train-loss: 1.2719717025756836\n",
      "[LOG 20200511-10:16:51] epoch: 0, batch: 1730 train-loss: 1.2047401666641235\n",
      "[LOG 20200511-10:16:51] epoch: 0, batch: 1731 train-loss: 1.8750969171524048\n",
      "[LOG 20200511-10:16:51] epoch: 0, batch: 1732 train-loss: 1.8398667573928833\n",
      "[LOG 20200511-10:16:51] epoch: 0, batch: 1733 train-loss: 1.9011027812957764\n",
      "[LOG 20200511-10:16:52] epoch: 0, batch: 1734 train-loss: 2.103792428970337\n",
      "[LOG 20200511-10:16:52] epoch: 0, batch: 1735 train-loss: 1.216512680053711\n",
      "[LOG 20200511-10:16:52] epoch: 0, batch: 1736 train-loss: 1.5031156539916992\n",
      "[LOG 20200511-10:16:52] epoch: 0, batch: 1737 train-loss: 1.4326756000518799\n",
      "[LOG 20200511-10:16:52] epoch: 0, batch: 1738 train-loss: 2.6506500244140625\n",
      "[LOG 20200511-10:16:52] epoch: 0, batch: 1739 train-loss: 1.1927062273025513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:16:52] epoch: 0, batch: 1740 train-loss: 1.9688820838928223\n",
      "[LOG 20200511-10:16:53] epoch: 0, batch: 1741 train-loss: 1.606581211090088\n",
      "[LOG 20200511-10:16:53] epoch: 0, batch: 1742 train-loss: 1.5242738723754883\n",
      "[LOG 20200511-10:16:53] epoch: 0, batch: 1743 train-loss: 2.4249563217163086\n",
      "[LOG 20200511-10:16:53] epoch: 0, batch: 1744 train-loss: 2.1031582355499268\n",
      "[LOG 20200511-10:16:53] epoch: 0, batch: 1745 train-loss: 1.484243392944336\n",
      "[LOG 20200511-10:16:53] epoch: 0, batch: 1746 train-loss: 2.70405912399292\n",
      "[LOG 20200511-10:16:53] epoch: 0, batch: 1747 train-loss: 1.6634728908538818\n",
      "[LOG 20200511-10:16:53] epoch: 0, batch: 1748 train-loss: 2.1001768112182617\n",
      "[LOG 20200511-10:16:54] epoch: 0, batch: 1749 train-loss: 1.7765412330627441\n",
      "[LOG 20200511-10:16:54] epoch: 0, batch: 1750 train-loss: 1.6710023880004883\n",
      "[LOG 20200511-10:16:54] epoch: 0, batch: 1751 train-loss: 0.8672256469726562\n",
      "[LOG 20200511-10:16:54] epoch: 0, batch: 1752 train-loss: 1.0813970565795898\n",
      "[LOG 20200511-10:16:54] epoch: 0, batch: 1753 train-loss: 1.7194249629974365\n",
      "[LOG 20200511-10:16:54] epoch: 0, batch: 1754 train-loss: 1.2224770784378052\n",
      "[LOG 20200511-10:16:54] epoch: 0, batch: 1755 train-loss: 1.5044716596603394\n",
      "[LOG 20200511-10:16:55] epoch: 0, batch: 1756 train-loss: 1.7168196439743042\n",
      "[LOG 20200511-10:16:55] epoch: 0, batch: 1757 train-loss: 1.1796433925628662\n",
      "[LOG 20200511-10:16:55] epoch: 0, batch: 1758 train-loss: 1.4030531644821167\n",
      "[LOG 20200511-10:16:55] epoch: 0, batch: 1759 train-loss: 1.720546841621399\n",
      "[LOG 20200511-10:16:55] epoch: 0, batch: 1760 train-loss: 2.476109743118286\n",
      "[LOG 20200511-10:16:55] epoch: 0, batch: 1761 train-loss: 1.6435644626617432\n",
      "[LOG 20200511-10:16:55] epoch: 0, batch: 1762 train-loss: 0.9215474128723145\n",
      "[LOG 20200511-10:16:56] epoch: 0, batch: 1763 train-loss: 1.98777174949646\n",
      "[LOG 20200511-10:16:56] epoch: 0, batch: 1764 train-loss: 0.9940146207809448\n",
      "[LOG 20200511-10:16:56] epoch: 0, batch: 1765 train-loss: 1.4192935228347778\n",
      "[LOG 20200511-10:16:56] epoch: 0, batch: 1766 train-loss: 1.9979450702667236\n",
      "[LOG 20200511-10:16:56] epoch: 0, batch: 1767 train-loss: 0.982739269733429\n",
      "[LOG 20200511-10:16:56] epoch: 0, batch: 1768 train-loss: 2.9633376598358154\n",
      "[LOG 20200511-10:16:56] epoch: 0, batch: 1769 train-loss: 1.5622498989105225\n",
      "[LOG 20200511-10:16:56] epoch: 0, batch: 1770 train-loss: 2.392068862915039\n",
      "[LOG 20200511-10:16:57] epoch: 0, batch: 1771 train-loss: 0.9861968755722046\n",
      "[LOG 20200511-10:16:57] epoch: 0, batch: 1772 train-loss: 1.2815395593643188\n",
      "[LOG 20200511-10:16:57] epoch: 0, batch: 1773 train-loss: 1.3061368465423584\n",
      "[LOG 20200511-10:16:57] epoch: 0, batch: 1774 train-loss: 2.681168794631958\n",
      "[LOG 20200511-10:16:57] epoch: 0, batch: 1775 train-loss: 1.4027149677276611\n",
      "[LOG 20200511-10:16:57] epoch: 0, batch: 1776 train-loss: 1.1351213455200195\n",
      "[LOG 20200511-10:16:57] epoch: 0, batch: 1777 train-loss: 2.1242129802703857\n",
      "[LOG 20200511-10:16:57] epoch: 0, batch: 1778 train-loss: 1.348874807357788\n",
      "[LOG 20200511-10:16:58] epoch: 0, batch: 1779 train-loss: 1.5292472839355469\n",
      "[LOG 20200511-10:16:58] epoch: 0, batch: 1780 train-loss: 2.007030487060547\n",
      "[LOG 20200511-10:16:58] epoch: 0, batch: 1781 train-loss: 1.4331395626068115\n",
      "[LOG 20200511-10:16:58] epoch: 0, batch: 1782 train-loss: 2.456399440765381\n",
      "[LOG 20200511-10:16:58] epoch: 0, batch: 1783 train-loss: 1.7050528526306152\n",
      "[LOG 20200511-10:16:58] epoch: 0, batch: 1784 train-loss: 1.8174303770065308\n",
      "[LOG 20200511-10:16:58] epoch: 0, batch: 1785 train-loss: 1.3193142414093018\n",
      "[LOG 20200511-10:16:58] epoch: 0, batch: 1786 train-loss: 2.0256361961364746\n",
      "[LOG 20200511-10:16:59] epoch: 0, batch: 1787 train-loss: 1.8389058113098145\n",
      "[LOG 20200511-10:16:59] epoch: 0, batch: 1788 train-loss: 1.12674880027771\n",
      "[LOG 20200511-10:16:59] epoch: 0, batch: 1789 train-loss: 2.0044217109680176\n",
      "[LOG 20200511-10:16:59] epoch: 0, batch: 1790 train-loss: 1.61844801902771\n",
      "[LOG 20200511-10:16:59] epoch: 0, batch: 1791 train-loss: 1.4572622776031494\n",
      "[LOG 20200511-10:16:59] epoch: 0, batch: 1792 train-loss: 2.3467822074890137\n",
      "[LOG 20200511-10:16:59] epoch: 0, batch: 1793 train-loss: 2.115440607070923\n",
      "[LOG 20200511-10:16:59] epoch: 0, batch: 1794 train-loss: 1.464379072189331\n",
      "[LOG 20200511-10:17:00] epoch: 0, batch: 1795 train-loss: 2.4084932804107666\n",
      "[LOG 20200511-10:17:00] epoch: 0, batch: 1796 train-loss: 2.3900043964385986\n",
      "[LOG 20200511-10:17:00] epoch: 0, batch: 1797 train-loss: 2.0600967407226562\n",
      "[LOG 20200511-10:17:00] epoch: 0, batch: 1798 train-loss: 1.6298619508743286\n",
      "[LOG 20200511-10:17:00] epoch: 0, batch: 1799 train-loss: 2.5611412525177\n",
      "[LOG 20200511-10:17:00] epoch: 0, batch: 1800 train-loss: 1.539998173713684\n",
      "[LOG 20200511-10:17:00] epoch: 0, batch: 1801 train-loss: 1.3017919063568115\n",
      "[LOG 20200511-10:17:01] epoch: 0, batch: 1802 train-loss: 1.045328140258789\n",
      "[LOG 20200511-10:17:01] epoch: 0, batch: 1803 train-loss: 2.2193570137023926\n",
      "[LOG 20200511-10:17:01] epoch: 0, batch: 1804 train-loss: 1.5449790954589844\n",
      "[LOG 20200511-10:17:01] epoch: 0, batch: 1805 train-loss: 1.8830389976501465\n",
      "[LOG 20200511-10:17:01] epoch: 0, batch: 1806 train-loss: 1.8159102201461792\n",
      "[LOG 20200511-10:17:01] epoch: 0, batch: 1807 train-loss: 1.2591103315353394\n",
      "[LOG 20200511-10:17:01] epoch: 0, batch: 1808 train-loss: 1.9367550611495972\n",
      "[LOG 20200511-10:17:01] epoch: 0, batch: 1809 train-loss: 1.8038320541381836\n",
      "[LOG 20200511-10:17:02] epoch: 0, batch: 1810 train-loss: 1.4614392518997192\n",
      "[LOG 20200511-10:17:02] epoch: 0, batch: 1811 train-loss: 1.3954037427902222\n",
      "[LOG 20200511-10:17:02] epoch: 0, batch: 1812 train-loss: 2.1883580684661865\n",
      "[LOG 20200511-10:17:02] epoch: 0, batch: 1813 train-loss: 2.2864797115325928\n",
      "[LOG 20200511-10:17:02] epoch: 0, batch: 1814 train-loss: 1.4001727104187012\n",
      "[LOG 20200511-10:17:02] epoch: 0, batch: 1815 train-loss: 1.269906997680664\n",
      "[LOG 20200511-10:17:02] epoch: 0, batch: 1816 train-loss: 1.2669776678085327\n",
      "[LOG 20200511-10:17:02] epoch: 0, batch: 1817 train-loss: 1.1988575458526611\n",
      "[LOG 20200511-10:17:03] epoch: 0, batch: 1818 train-loss: 1.397092342376709\n",
      "[LOG 20200511-10:17:03] epoch: 0, batch: 1819 train-loss: 1.7956398725509644\n",
      "[LOG 20200511-10:17:03] epoch: 0, batch: 1820 train-loss: 1.7931842803955078\n",
      "[LOG 20200511-10:17:03] epoch: 0, batch: 1821 train-loss: 1.710574746131897\n",
      "[LOG 20200511-10:17:03] epoch: 0, batch: 1822 train-loss: 2.4285173416137695\n",
      "[LOG 20200511-10:17:03] epoch: 0, batch: 1823 train-loss: 1.8641557693481445\n",
      "[LOG 20200511-10:17:03] epoch: 0, batch: 1824 train-loss: 1.6809641122817993\n",
      "[LOG 20200511-10:17:04] epoch: 0, batch: 1825 train-loss: 1.2660012245178223\n",
      "[LOG 20200511-10:17:04] epoch: 0, batch: 1826 train-loss: 1.802995204925537\n",
      "[LOG 20200511-10:17:04] epoch: 0, batch: 1827 train-loss: 2.3291380405426025\n",
      "[LOG 20200511-10:17:04] epoch: 0, batch: 1828 train-loss: 2.195375919342041\n",
      "[LOG 20200511-10:17:04] epoch: 0, batch: 1829 train-loss: 1.246934175491333\n",
      "[LOG 20200511-10:17:04] epoch: 0, batch: 1830 train-loss: 1.0183173418045044\n",
      "[LOG 20200511-10:17:04] epoch: 0, batch: 1831 train-loss: 1.243017315864563\n",
      "[LOG 20200511-10:17:05] epoch: 0, batch: 1832 train-loss: 1.529140830039978\n",
      "[LOG 20200511-10:17:05] epoch: 0, batch: 1833 train-loss: 1.3129278421401978\n",
      "[LOG 20200511-10:17:05] epoch: 0, batch: 1834 train-loss: 1.0660512447357178\n",
      "[LOG 20200511-10:17:05] epoch: 0, batch: 1835 train-loss: 1.6582767963409424\n",
      "[LOG 20200511-10:17:05] epoch: 0, batch: 1836 train-loss: 0.9924108386039734\n",
      "[LOG 20200511-10:17:05] epoch: 0, batch: 1837 train-loss: 1.265758991241455\n",
      "[LOG 20200511-10:17:05] epoch: 0, batch: 1838 train-loss: 1.834028720855713\n",
      "[LOG 20200511-10:17:06] epoch: 0, batch: 1839 train-loss: 1.2958544492721558\n",
      "[LOG 20200511-10:17:06] epoch: 0, batch: 1840 train-loss: 1.9907853603363037\n",
      "[LOG 20200511-10:17:06] epoch: 0, batch: 1841 train-loss: 1.3695716857910156\n",
      "[LOG 20200511-10:17:06] epoch: 0, batch: 1842 train-loss: 1.4264204502105713\n",
      "[LOG 20200511-10:17:06] epoch: 0, batch: 1843 train-loss: 0.8878961801528931\n",
      "[LOG 20200511-10:17:06] epoch: 0, batch: 1844 train-loss: 1.7500358819961548\n",
      "[LOG 20200511-10:17:06] epoch: 0, batch: 1845 train-loss: 1.036106824874878\n",
      "[LOG 20200511-10:17:06] epoch: 0, batch: 1846 train-loss: 1.255847692489624\n",
      "[LOG 20200511-10:17:07] epoch: 0, batch: 1847 train-loss: 1.9615387916564941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:17:07] epoch: 0, batch: 1848 train-loss: 1.8787987232208252\n",
      "[LOG 20200511-10:17:07] epoch: 0, batch: 1849 train-loss: 1.2240009307861328\n",
      "[LOG 20200511-10:17:07] epoch: 0, batch: 1850 train-loss: 1.5397813320159912\n",
      "[LOG 20200511-10:17:07] epoch: 0, batch: 1851 train-loss: 1.3315171003341675\n",
      "[LOG 20200511-10:17:07] epoch: 0, batch: 1852 train-loss: 2.3534724712371826\n",
      "[LOG 20200511-10:17:07] epoch: 0, batch: 1853 train-loss: 1.4951045513153076\n",
      "[LOG 20200511-10:17:08] epoch: 0, batch: 1854 train-loss: 2.3957948684692383\n",
      "[LOG 20200511-10:17:08] epoch: 0, batch: 1855 train-loss: 1.7512198686599731\n",
      "[LOG 20200511-10:17:08] epoch: 0, batch: 1856 train-loss: 1.8591774702072144\n",
      "[LOG 20200511-10:17:08] epoch: 0, batch: 1857 train-loss: 1.9008017778396606\n",
      "[LOG 20200511-10:17:08] epoch: 0, batch: 1858 train-loss: 1.534407615661621\n",
      "[LOG 20200511-10:17:08] epoch: 0, batch: 1859 train-loss: 1.385382056236267\n",
      "[LOG 20200511-10:17:08] epoch: 0, batch: 1860 train-loss: 1.4664844274520874\n",
      "[LOG 20200511-10:17:09] epoch: 0, batch: 1861 train-loss: 0.6484389305114746\n",
      "[LOG 20200511-10:17:09] epoch: 0, batch: 1862 train-loss: 1.032123327255249\n",
      "[LOG 20200511-10:17:09] epoch: 0, batch: 1863 train-loss: 1.9108127355575562\n",
      "[LOG 20200511-10:17:09] epoch: 0, batch: 1864 train-loss: 1.2559317350387573\n",
      "[LOG 20200511-10:17:09] epoch: 0, batch: 1865 train-loss: 1.5110905170440674\n",
      "[LOG 20200511-10:17:09] epoch: 0, batch: 1866 train-loss: 1.4830634593963623\n",
      "[LOG 20200511-10:17:09] epoch: 0, batch: 1867 train-loss: 0.668620228767395\n",
      "[LOG 20200511-10:17:10] epoch: 0, batch: 1868 train-loss: 1.0330477952957153\n",
      "[LOG 20200511-10:17:10] epoch: 0, batch: 1869 train-loss: 1.4376355409622192\n",
      "[LOG 20200511-10:17:10] epoch: 0, batch: 1870 train-loss: 2.0724620819091797\n",
      "[LOG 20200511-10:17:10] epoch: 0, batch: 1871 train-loss: 0.8461825847625732\n",
      "[LOG 20200511-10:17:10] epoch: 0, batch: 1872 train-loss: 1.3661974668502808\n",
      "[LOG 20200511-10:17:10] epoch: 0, batch: 1873 train-loss: 1.943127155303955\n",
      "[LOG 20200511-10:17:10] epoch: 0, batch: 1874 train-loss: 1.8808176517486572\n",
      "[LOG 20200511-10:17:10] epoch: 0, batch: 1875 train-loss: 2.4167609214782715\n",
      "[LOG 20200511-10:17:11] epoch: 0, batch: 1876 train-loss: 1.0638172626495361\n",
      "[LOG 20200511-10:17:11] epoch: 0, batch: 1877 train-loss: 1.7099041938781738\n",
      "[LOG 20200511-10:17:11] epoch: 0, batch: 1878 train-loss: 1.1304168701171875\n",
      "[LOG 20200511-10:17:11] epoch: 0, batch: 1879 train-loss: 1.426377773284912\n",
      "[LOG 20200511-10:17:11] epoch: 0, batch: 1880 train-loss: 1.6830171346664429\n",
      "[LOG 20200511-10:17:11] epoch: 0, batch: 1881 train-loss: 1.3801649808883667\n",
      "[LOG 20200511-10:17:11] epoch: 0, batch: 1882 train-loss: 0.9855101108551025\n",
      "[LOG 20200511-10:17:12] epoch: 0, batch: 1883 train-loss: 0.8998956084251404\n",
      "[LOG 20200511-10:17:12] epoch: 0, batch: 1884 train-loss: 1.2242684364318848\n",
      "[LOG 20200511-10:17:12] epoch: 0, batch: 1885 train-loss: 1.784724235534668\n",
      "[LOG 20200511-10:17:12] epoch: 0, batch: 1886 train-loss: 1.475268840789795\n",
      "[LOG 20200511-10:17:12] epoch: 0, batch: 1887 train-loss: 1.7606041431427002\n",
      "[LOG 20200511-10:17:12] epoch: 0, batch: 1888 train-loss: 2.255704402923584\n",
      "[LOG 20200511-10:17:12] epoch: 0, batch: 1889 train-loss: 1.8331960439682007\n",
      "[LOG 20200511-10:17:12] epoch: 0, batch: 1890 train-loss: 1.5446064472198486\n",
      "[LOG 20200511-10:17:13] epoch: 0, batch: 1891 train-loss: 1.7923694849014282\n",
      "[LOG 20200511-10:17:13] epoch: 0, batch: 1892 train-loss: 2.249814987182617\n",
      "[LOG 20200511-10:17:13] epoch: 0, batch: 1893 train-loss: 2.3145861625671387\n",
      "[LOG 20200511-10:17:13] epoch: 0, batch: 1894 train-loss: 2.3342676162719727\n",
      "[LOG 20200511-10:17:13] epoch: 0, batch: 1895 train-loss: 1.023376226425171\n",
      "[LOG 20200511-10:17:13] epoch: 0, batch: 1896 train-loss: 1.3839508295059204\n",
      "[LOG 20200511-10:17:13] epoch: 0, batch: 1897 train-loss: 1.899050235748291\n",
      "[LOG 20200511-10:17:13] epoch: 0, batch: 1898 train-loss: 1.0488803386688232\n",
      "[LOG 20200511-10:17:14] epoch: 0, batch: 1899 train-loss: 3.1985669136047363\n",
      "[LOG 20200511-10:17:14] epoch: 0, batch: 1900 train-loss: 1.318367600440979\n",
      "[LOG 20200511-10:17:14] epoch: 0, batch: 1901 train-loss: 1.807828664779663\n",
      "[LOG 20200511-10:17:14] epoch: 0, batch: 1902 train-loss: 1.5495450496673584\n",
      "[LOG 20200511-10:17:14] epoch: 0, batch: 1903 train-loss: 2.095207929611206\n",
      "[LOG 20200511-10:17:14] epoch: 0, batch: 1904 train-loss: 1.8996617794036865\n",
      "[LOG 20200511-10:17:14] epoch: 0, batch: 1905 train-loss: 1.4593112468719482\n",
      "[LOG 20200511-10:17:14] epoch: 0, batch: 1906 train-loss: 1.693000078201294\n",
      "[LOG 20200511-10:17:15] epoch: 0, batch: 1907 train-loss: 1.6397602558135986\n",
      "[LOG 20200511-10:17:15] epoch: 0, batch: 1908 train-loss: 1.4971034526824951\n",
      "[LOG 20200511-10:17:15] epoch: 0, batch: 1909 train-loss: 1.0825080871582031\n",
      "[LOG 20200511-10:17:15] epoch: 0, batch: 1910 train-loss: 2.082873582839966\n",
      "[LOG 20200511-10:17:15] epoch: 0, batch: 1911 train-loss: 1.4977269172668457\n",
      "[LOG 20200511-10:17:15] epoch: 0, batch: 1912 train-loss: 1.7248284816741943\n",
      "[LOG 20200511-10:17:15] epoch: 0, batch: 1913 train-loss: 2.375474214553833\n",
      "[LOG 20200511-10:17:15] epoch: 0, batch: 1914 train-loss: 1.5939406156539917\n",
      "[LOG 20200511-10:17:16] epoch: 0, batch: 1915 train-loss: 1.8250341415405273\n",
      "[LOG 20200511-10:17:16] epoch: 0, batch: 1916 train-loss: 1.0647706985473633\n",
      "[LOG 20200511-10:17:16] epoch: 0, batch: 1917 train-loss: 1.5082929134368896\n",
      "[LOG 20200511-10:17:16] epoch: 0, batch: 1918 train-loss: 2.8347506523132324\n",
      "[LOG 20200511-10:17:16] epoch: 0, batch: 1919 train-loss: 0.8238555192947388\n",
      "[LOG 20200511-10:17:16] epoch: 0, batch: 1920 train-loss: 2.096846103668213\n",
      "[LOG 20200511-10:17:16] epoch: 0, batch: 1921 train-loss: 2.1646156311035156\n",
      "[LOG 20200511-10:17:17] epoch: 0, batch: 1922 train-loss: 1.1810216903686523\n",
      "[LOG 20200511-10:17:17] epoch: 0, batch: 1923 train-loss: 2.0717756748199463\n",
      "[LOG 20200511-10:17:17] epoch: 0, batch: 1924 train-loss: 1.8729690313339233\n",
      "[LOG 20200511-10:17:17] epoch: 0, batch: 1925 train-loss: 2.068413496017456\n",
      "[LOG 20200511-10:17:17] epoch: 0, batch: 1926 train-loss: 1.8439679145812988\n",
      "[LOG 20200511-10:17:17] epoch: 0, batch: 1927 train-loss: 1.3686778545379639\n",
      "[LOG 20200511-10:17:17] epoch: 0, batch: 1928 train-loss: 1.679678201675415\n",
      "[LOG 20200511-10:17:17] epoch: 0, batch: 1929 train-loss: 1.640761375427246\n",
      "[LOG 20200511-10:17:18] epoch: 0, batch: 1930 train-loss: 1.283431887626648\n",
      "[LOG 20200511-10:17:18] epoch: 0, batch: 1931 train-loss: 1.5567188262939453\n",
      "[LOG 20200511-10:17:18] epoch: 0, batch: 1932 train-loss: 1.8944445848464966\n",
      "[LOG 20200511-10:17:18] epoch: 0, batch: 1933 train-loss: 1.8597674369812012\n",
      "[LOG 20200511-10:17:18] epoch: 0, batch: 1934 train-loss: 1.7331786155700684\n",
      "[LOG 20200511-10:17:18] epoch: 0, batch: 1935 train-loss: 1.5485482215881348\n",
      "[LOG 20200511-10:17:18] epoch: 0, batch: 1936 train-loss: 1.7717623710632324\n",
      "[LOG 20200511-10:17:18] epoch: 0, batch: 1937 train-loss: 1.8774040937423706\n",
      "[LOG 20200511-10:17:19] epoch: 0, batch: 1938 train-loss: 0.9375770092010498\n",
      "[LOG 20200511-10:17:19] epoch: 0, batch: 1939 train-loss: 1.3535528182983398\n",
      "[LOG 20200511-10:17:19] epoch: 0, batch: 1940 train-loss: 1.6432228088378906\n",
      "[LOG 20200511-10:17:19] epoch: 0, batch: 1941 train-loss: 2.5741963386535645\n",
      "[LOG 20200511-10:17:19] epoch: 0, batch: 1942 train-loss: 1.4043606519699097\n",
      "[LOG 20200511-10:17:19] epoch: 0, batch: 1943 train-loss: 1.8973710536956787\n",
      "[LOG 20200511-10:17:19] epoch: 0, batch: 1944 train-loss: 1.5599422454833984\n",
      "[LOG 20200511-10:17:19] epoch: 0, batch: 1945 train-loss: 1.176063060760498\n",
      "[LOG 20200511-10:17:20] epoch: 0, batch: 1946 train-loss: 1.086909532546997\n",
      "[LOG 20200511-10:17:20] epoch: 0, batch: 1947 train-loss: 1.2217719554901123\n",
      "[LOG 20200511-10:17:20] epoch: 0, batch: 1948 train-loss: 1.3933833837509155\n",
      "[LOG 20200511-10:17:20] epoch: 0, batch: 1949 train-loss: 1.9387845993041992\n",
      "[LOG 20200511-10:17:20] epoch: 0, batch: 1950 train-loss: 1.6104704141616821\n",
      "[LOG 20200511-10:17:20] epoch: 0, batch: 1951 train-loss: 1.8160427808761597\n",
      "[LOG 20200511-10:17:20] epoch: 0, batch: 1952 train-loss: 1.8760172128677368\n",
      "[LOG 20200511-10:17:20] epoch: 0, batch: 1953 train-loss: 1.7906464338302612\n",
      "[LOG 20200511-10:17:21] epoch: 0, batch: 1954 train-loss: 1.0243823528289795\n",
      "[LOG 20200511-10:17:21] epoch: 0, batch: 1955 train-loss: 1.0668625831604004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:17:21] epoch: 0, batch: 1956 train-loss: 2.2608067989349365\n",
      "[LOG 20200511-10:17:21] epoch: 0, batch: 1957 train-loss: 1.5241174697875977\n",
      "[LOG 20200511-10:17:21] epoch: 0, batch: 1958 train-loss: 1.8159226179122925\n",
      "[LOG 20200511-10:17:21] epoch: 0, batch: 1959 train-loss: 1.5232770442962646\n",
      "[LOG 20200511-10:17:21] epoch: 0, batch: 1960 train-loss: 1.6055313348770142\n",
      "[LOG 20200511-10:17:22] epoch: 0, batch: 1961 train-loss: 1.3218833208084106\n",
      "[LOG 20200511-10:17:22] epoch: 0, batch: 1962 train-loss: 1.2324262857437134\n",
      "[LOG 20200511-10:17:22] epoch: 0, batch: 1963 train-loss: 1.1628338098526\n",
      "[LOG 20200511-10:17:22] epoch: 0, batch: 1964 train-loss: 1.4309027194976807\n",
      "[LOG 20200511-10:17:22] epoch: 0, batch: 1965 train-loss: 2.268134117126465\n",
      "[LOG 20200511-10:17:22] epoch: 0, batch: 1966 train-loss: 1.7862859964370728\n",
      "[LOG 20200511-10:17:22] epoch: 0, batch: 1967 train-loss: 1.9395253658294678\n",
      "[LOG 20200511-10:17:22] epoch: 0, batch: 1968 train-loss: 1.054913878440857\n",
      "[LOG 20200511-10:17:22] epoch: 0, batch: 1969 train-loss: 2.101311206817627\n",
      "[LOG 20200511-10:17:23] epoch: 0, batch: 1970 train-loss: 1.440943717956543\n",
      "[LOG 20200511-10:17:23] epoch: 0, batch: 1971 train-loss: 1.2677602767944336\n",
      "[LOG 20200511-10:17:23] epoch: 0, batch: 1972 train-loss: 1.6864842176437378\n",
      "[LOG 20200511-10:17:23] epoch: 0, batch: 1973 train-loss: 1.6570048332214355\n",
      "[LOG 20200511-10:17:23] epoch: 0, batch: 1974 train-loss: 1.6802573204040527\n",
      "[LOG 20200511-10:17:23] epoch: 0, batch: 1975 train-loss: 1.6145211458206177\n",
      "[LOG 20200511-10:17:23] epoch: 0, batch: 1976 train-loss: 2.0105817317962646\n",
      "[LOG 20200511-10:17:23] epoch: 0, batch: 1977 train-loss: 1.660772681236267\n",
      "[LOG 20200511-10:17:23] epoch: 0, batch: 1978 train-loss: 1.7279529571533203\n",
      "[LOG 20200511-10:17:24] epoch: 0, batch: 1979 train-loss: 1.7589776515960693\n",
      "[LOG 20200511-10:17:24] epoch: 0, batch: 1980 train-loss: 1.595139741897583\n",
      "[LOG 20200511-10:17:24] epoch: 0, batch: 1981 train-loss: 1.2528746128082275\n",
      "[LOG 20200511-10:17:24] epoch: 0, batch: 1982 train-loss: 1.9203474521636963\n",
      "[LOG 20200511-10:17:24] epoch: 0, batch: 1983 train-loss: 1.596505880355835\n",
      "[LOG 20200511-10:17:24] epoch: 0, batch: 1984 train-loss: 2.3775362968444824\n",
      "[LOG 20200511-10:17:24] epoch: 0, batch: 1985 train-loss: 0.839550256729126\n",
      "[LOG 20200511-10:17:24] epoch: 0, batch: 1986 train-loss: 1.5858246088027954\n",
      "[LOG 20200511-10:17:25] epoch: 0, batch: 1987 train-loss: 1.5117239952087402\n",
      "[LOG 20200511-10:17:25] epoch: 0, batch: 1988 train-loss: 1.7136965990066528\n",
      "[LOG 20200511-10:17:25] epoch: 0, batch: 1989 train-loss: 1.9357678890228271\n",
      "[LOG 20200511-10:17:25] epoch: 0, batch: 1990 train-loss: 1.0014569759368896\n",
      "[LOG 20200511-10:17:25] epoch: 0, batch: 1991 train-loss: 1.4655652046203613\n",
      "[LOG 20200511-10:17:25] epoch: 0, batch: 1992 train-loss: 2.453610897064209\n",
      "[LOG 20200511-10:17:25] epoch: 0, batch: 1993 train-loss: 2.2328617572784424\n",
      "[LOG 20200511-10:17:25] epoch: 0, batch: 1994 train-loss: 1.1628508567810059\n",
      "[LOG 20200511-10:17:25] epoch: 0, batch: 1995 train-loss: 0.7960410714149475\n",
      "[LOG 20200511-10:17:26] epoch: 0, batch: 1996 train-loss: 1.0636886358261108\n",
      "[LOG 20200511-10:17:26] epoch: 0, batch: 1997 train-loss: 1.0935680866241455\n",
      "[LOG 20200511-10:17:26] epoch: 0, batch: 1998 train-loss: 2.375770092010498\n",
      "[LOG 20200511-10:17:26] epoch: 0, batch: 1999 train-loss: 1.879773736000061\n",
      "[LOG 20200511-10:17:26] epoch: 0, batch: 2000 train-loss: 1.3813506364822388\n",
      "[LOG 20200511-10:17:26] epoch: 0, batch: 2001 train-loss: 1.7265310287475586\n",
      "[LOG 20200511-10:17:26] epoch: 0, batch: 2002 train-loss: 0.952173113822937\n",
      "[LOG 20200511-10:17:26] epoch: 0, batch: 2003 train-loss: 1.00142502784729\n",
      "[LOG 20200511-10:17:27] epoch: 0, batch: 2004 train-loss: 0.6777236461639404\n",
      "[LOG 20200511-10:17:27] epoch: 0, batch: 2005 train-loss: 2.232698678970337\n",
      "[LOG 20200511-10:17:27] epoch: 0, batch: 2006 train-loss: 1.9300427436828613\n",
      "[LOG 20200511-10:17:27] epoch: 0, batch: 2007 train-loss: 1.525679349899292\n",
      "[LOG 20200511-10:17:27] epoch: 0, batch: 2008 train-loss: 1.4036518335342407\n",
      "[LOG 20200511-10:17:27] epoch: 0, batch: 2009 train-loss: 1.3763465881347656\n",
      "[LOG 20200511-10:17:27] epoch: 0, batch: 2010 train-loss: 1.3049094676971436\n",
      "[LOG 20200511-10:17:27] epoch: 0, batch: 2011 train-loss: 1.2715356349945068\n",
      "[LOG 20200511-10:17:28] epoch: 0, batch: 2012 train-loss: 1.0666147470474243\n",
      "[LOG 20200511-10:17:28] epoch: 0, batch: 2013 train-loss: 1.561096429824829\n",
      "[LOG 20200511-10:17:28] epoch: 0, batch: 2014 train-loss: 1.7210248708724976\n",
      "[LOG 20200511-10:17:28] epoch: 0, batch: 2015 train-loss: 0.9879505634307861\n",
      "[LOG 20200511-10:17:28] epoch: 0, batch: 2016 train-loss: 1.6162573099136353\n",
      "[LOG 20200511-10:17:28] epoch: 0, batch: 2017 train-loss: 1.9149420261383057\n",
      "[LOG 20200511-10:17:28] epoch: 0, batch: 2018 train-loss: 1.974807620048523\n",
      "[LOG 20200511-10:17:28] epoch: 0, batch: 2019 train-loss: 1.8159565925598145\n",
      "[LOG 20200511-10:17:28] epoch: 0, batch: 2020 train-loss: 1.322003722190857\n",
      "[LOG 20200511-10:17:29] epoch: 0, batch: 2021 train-loss: 2.1445109844207764\n",
      "[LOG 20200511-10:17:29] epoch: 0, batch: 2022 train-loss: 0.9361086487770081\n",
      "[LOG 20200511-10:17:29] epoch: 0, batch: 2023 train-loss: 2.2405104637145996\n",
      "[LOG 20200511-10:17:29] epoch: 0, batch: 2024 train-loss: 1.992851734161377\n",
      "[LOG 20200511-10:17:29] epoch: 0, batch: 2025 train-loss: 1.1851915121078491\n",
      "[LOG 20200511-10:17:29] epoch: 0, batch: 2026 train-loss: 2.249708652496338\n",
      "[LOG 20200511-10:17:29] epoch: 0, batch: 2027 train-loss: 2.0574779510498047\n",
      "[LOG 20200511-10:17:29] epoch: 0, batch: 2028 train-loss: 1.4617364406585693\n",
      "[LOG 20200511-10:17:29] epoch: 0, batch: 2029 train-loss: 1.655725359916687\n",
      "[LOG 20200511-10:17:30] epoch: 0, batch: 2030 train-loss: 1.0645076036453247\n",
      "[LOG 20200511-10:17:30] epoch: 0, batch: 2031 train-loss: 1.3587480783462524\n",
      "[LOG 20200511-10:17:30] epoch: 0, batch: 2032 train-loss: 2.7387495040893555\n",
      "[LOG 20200511-10:17:30] epoch: 0, batch: 2033 train-loss: 1.6573153734207153\n",
      "[LOG 20200511-10:17:30] epoch: 0, batch: 2034 train-loss: 1.1868480443954468\n",
      "[LOG 20200511-10:17:30] epoch: 0, batch: 2035 train-loss: 1.673166036605835\n",
      "[LOG 20200511-10:17:30] epoch: 0, batch: 2036 train-loss: 1.023443579673767\n",
      "[LOG 20200511-10:17:31] epoch: 0, batch: 2037 train-loss: 1.5918495655059814\n",
      "[LOG 20200511-10:17:31] epoch: 0, batch: 2038 train-loss: 1.7638261318206787\n",
      "[LOG 20200511-10:17:31] epoch: 0, batch: 2039 train-loss: 1.8752832412719727\n",
      "[LOG 20200511-10:17:31] epoch: 0, batch: 2040 train-loss: 2.04341459274292\n",
      "[LOG 20200511-10:17:31] epoch: 0, batch: 2041 train-loss: 1.534950613975525\n",
      "[LOG 20200511-10:17:31] epoch: 0, batch: 2042 train-loss: 2.3594303131103516\n",
      "[LOG 20200511-10:17:31] epoch: 0, batch: 2043 train-loss: 0.8329821228981018\n",
      "[LOG 20200511-10:17:31] epoch: 0, batch: 2044 train-loss: 1.8571882247924805\n",
      "[LOG 20200511-10:17:31] epoch: 0, batch: 2045 train-loss: 2.343456506729126\n",
      "[LOG 20200511-10:17:32] epoch: 0, batch: 2046 train-loss: 2.0805652141571045\n",
      "[LOG 20200511-10:17:32] epoch: 0, batch: 2047 train-loss: 2.5270283222198486\n",
      "[LOG 20200511-10:17:32] epoch: 0, batch: 2048 train-loss: 1.3669958114624023\n",
      "[LOG 20200511-10:17:32] epoch: 0, batch: 2049 train-loss: 2.2798376083374023\n",
      "[LOG 20200511-10:17:32] epoch: 0, batch: 2050 train-loss: 1.1926929950714111\n",
      "[LOG 20200511-10:17:32] epoch: 0, batch: 2051 train-loss: 0.8068886995315552\n",
      "[LOG 20200511-10:17:32] epoch: 0, batch: 2052 train-loss: 1.6025218963623047\n",
      "[LOG 20200511-10:17:32] epoch: 0, batch: 2053 train-loss: 2.0407633781433105\n",
      "[LOG 20200511-10:17:33] epoch: 0, batch: 2054 train-loss: 1.8641778230667114\n",
      "[LOG 20200511-10:17:33] epoch: 0, batch: 2055 train-loss: 1.728498935699463\n",
      "[LOG 20200511-10:17:33] epoch: 0, batch: 2056 train-loss: 1.8163954019546509\n",
      "[LOG 20200511-10:17:33] epoch: 0, batch: 2057 train-loss: 1.177574634552002\n",
      "[LOG 20200511-10:17:33] epoch: 0, batch: 2058 train-loss: 3.642179012298584\n",
      "[LOG 20200511-10:17:33] epoch: 0, batch: 2059 train-loss: 2.5359840393066406\n",
      "[LOG 20200511-10:17:33] epoch: 0, batch: 2060 train-loss: 1.1437115669250488\n",
      "[LOG 20200511-10:17:33] epoch: 0, batch: 2061 train-loss: 1.0749765634536743\n",
      "[LOG 20200511-10:17:33] epoch: 0, batch: 2062 train-loss: 1.4337629079818726\n",
      "[LOG 20200511-10:17:34] epoch: 0, batch: 2063 train-loss: 1.2050410509109497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:17:34] epoch: 0, batch: 2064 train-loss: 2.0451831817626953\n",
      "[LOG 20200511-10:17:34] epoch: 0, batch: 2065 train-loss: 0.8949069380760193\n",
      "[LOG 20200511-10:17:34] epoch: 0, batch: 2066 train-loss: 1.1423821449279785\n",
      "[LOG 20200511-10:17:34] epoch: 0, batch: 2067 train-loss: 1.6071857213974\n",
      "[LOG 20200511-10:17:34] epoch: 0, batch: 2068 train-loss: 1.9844160079956055\n",
      "[LOG 20200511-10:17:34] epoch: 0, batch: 2069 train-loss: 1.3710503578186035\n",
      "[LOG 20200511-10:17:34] epoch: 0, batch: 2070 train-loss: 2.2841696739196777\n",
      "[LOG 20200511-10:17:35] epoch: 0, batch: 2071 train-loss: 2.5081920623779297\n",
      "[LOG 20200511-10:17:35] epoch: 0, batch: 2072 train-loss: 1.6698193550109863\n",
      "[LOG 20200511-10:17:35] epoch: 0, batch: 2073 train-loss: 0.8784394264221191\n",
      "[LOG 20200511-10:17:35] epoch: 0, batch: 2074 train-loss: 1.342753529548645\n",
      "[LOG 20200511-10:17:35] epoch: 0, batch: 2075 train-loss: 1.5497313737869263\n",
      "[LOG 20200511-10:17:35] epoch: 0, batch: 2076 train-loss: 1.106052041053772\n",
      "[LOG 20200511-10:17:35] epoch: 0, batch: 2077 train-loss: 1.040465235710144\n",
      "[LOG 20200511-10:17:35] epoch: 0, batch: 2078 train-loss: 1.6333379745483398\n",
      "[LOG 20200511-10:17:36] epoch: 0, batch: 2079 train-loss: 1.350451946258545\n",
      "[LOG 20200511-10:17:36] epoch: 0, batch: 2080 train-loss: 1.9604918956756592\n",
      "[LOG 20200511-10:17:36] epoch: 0, batch: 2081 train-loss: 2.4668068885803223\n",
      "[LOG 20200511-10:17:36] epoch: 0, batch: 2082 train-loss: 2.666300058364868\n",
      "[LOG 20200511-10:17:36] epoch: 0, batch: 2083 train-loss: 1.0169692039489746\n",
      "[LOG 20200511-10:17:36] epoch: 0, batch: 2084 train-loss: 1.7929306030273438\n",
      "[LOG 20200511-10:17:36] epoch: 0, batch: 2085 train-loss: 1.8810828924179077\n",
      "[LOG 20200511-10:17:36] epoch: 0, batch: 2086 train-loss: 0.9015262126922607\n",
      "[LOG 20200511-10:17:36] epoch: 0, batch: 2087 train-loss: 2.197402000427246\n",
      "[LOG 20200511-10:17:37] epoch: 0, batch: 2088 train-loss: 0.9886646866798401\n",
      "[LOG 20200511-10:17:37] epoch: 0, batch: 2089 train-loss: 1.366817831993103\n",
      "[LOG 20200511-10:17:37] epoch: 0, batch: 2090 train-loss: 2.226163625717163\n",
      "[LOG 20200511-10:17:37] epoch: 0, batch: 2091 train-loss: 1.4780453443527222\n",
      "[LOG 20200511-10:17:37] epoch: 0, batch: 2092 train-loss: 1.8464940786361694\n",
      "[LOG 20200511-10:17:37] epoch: 0, batch: 2093 train-loss: 2.179791212081909\n",
      "[LOG 20200511-10:17:37] epoch: 0, batch: 2094 train-loss: 1.0965311527252197\n",
      "[LOG 20200511-10:17:37] epoch: 0, batch: 2095 train-loss: 1.4829539060592651\n",
      "[LOG 20200511-10:17:38] epoch: 0, batch: 2096 train-loss: 0.9487290978431702\n",
      "[LOG 20200511-10:17:38] epoch: 0, batch: 2097 train-loss: 2.1345136165618896\n",
      "[LOG 20200511-10:17:38] epoch: 0, batch: 2098 train-loss: 1.2866923809051514\n",
      "[LOG 20200511-10:17:38] epoch: 0, batch: 2099 train-loss: 1.1506845951080322\n",
      "[LOG 20200511-10:17:38] epoch: 0, batch: 2100 train-loss: 1.4941538572311401\n",
      "[LOG 20200511-10:17:38] epoch: 0, batch: 2101 train-loss: 1.491590976715088\n",
      "[LOG 20200511-10:17:38] epoch: 0, batch: 2102 train-loss: 1.8344534635543823\n",
      "[LOG 20200511-10:17:38] epoch: 0, batch: 2103 train-loss: 1.7127184867858887\n",
      "[LOG 20200511-10:17:38] epoch: 0, batch: 2104 train-loss: 1.438765048980713\n",
      "[LOG 20200511-10:17:39] epoch: 0, batch: 2105 train-loss: 0.9167386889457703\n",
      "[LOG 20200511-10:17:39] epoch: 0, batch: 2106 train-loss: 2.068876028060913\n",
      "[LOG 20200511-10:17:39] epoch: 0, batch: 2107 train-loss: 1.833763837814331\n",
      "[LOG 20200511-10:17:39] epoch: 0, batch: 2108 train-loss: 1.987901210784912\n",
      "[LOG 20200511-10:17:39] epoch: 0, batch: 2109 train-loss: 1.1000025272369385\n",
      "[LOG 20200511-10:17:39] epoch: 0, batch: 2110 train-loss: 1.8274747133255005\n",
      "[LOG 20200511-10:17:39] epoch: 0, batch: 2111 train-loss: 1.9110559225082397\n",
      "[LOG 20200511-10:17:39] epoch: 0, batch: 2112 train-loss: 1.0542383193969727\n",
      "[LOG 20200511-10:17:39] epoch: 0, batch: 2113 train-loss: 1.4930013418197632\n",
      "[LOG 20200511-10:17:40] epoch: 0, batch: 2114 train-loss: 2.027803421020508\n",
      "[LOG 20200511-10:17:40] epoch: 0, batch: 2115 train-loss: 2.356053352355957\n",
      "[LOG 20200511-10:17:40] epoch: 0, batch: 2116 train-loss: 2.6135759353637695\n",
      "[LOG 20200511-10:17:40] epoch: 0, batch: 2117 train-loss: 2.090118408203125\n",
      "[LOG 20200511-10:17:40] epoch: 0, batch: 2118 train-loss: 1.5034602880477905\n",
      "[LOG 20200511-10:17:40] epoch: 0, batch: 2119 train-loss: 0.7946648001670837\n",
      "[LOG 20200511-10:17:40] epoch: 0, batch: 2120 train-loss: 0.9869461059570312\n",
      "[LOG 20200511-10:17:40] epoch: 0, batch: 2121 train-loss: 1.1422290802001953\n",
      "[LOG 20200511-10:17:40] epoch: 0, batch: 2122 train-loss: 1.4561491012573242\n",
      "[LOG 20200511-10:17:41] epoch: 0, batch: 2123 train-loss: 2.133965253829956\n",
      "[LOG 20200511-10:17:41] epoch: 0, batch: 2124 train-loss: 1.6187591552734375\n",
      "[LOG 20200511-10:17:41] epoch: 0, batch: 2125 train-loss: 2.0528087615966797\n",
      "[LOG 20200511-10:17:41] epoch: 0, batch: 2126 train-loss: 1.2827973365783691\n",
      "[LOG 20200511-10:17:41] epoch: 0, batch: 2127 train-loss: 1.2901082038879395\n",
      "[LOG 20200511-10:17:41] epoch: 0, batch: 2128 train-loss: 1.3470593690872192\n",
      "[LOG 20200511-10:17:41] epoch: 0, batch: 2129 train-loss: 2.2122981548309326\n",
      "[LOG 20200511-10:17:42] epoch: 0, batch: 2130 train-loss: 1.7614188194274902\n",
      "[LOG 20200511-10:17:42] epoch: 0, batch: 2131 train-loss: 1.6764460802078247\n",
      "[LOG 20200511-10:17:42] epoch: 0, batch: 2132 train-loss: 1.3432586193084717\n",
      "[LOG 20200511-10:17:42] epoch: 0, batch: 2133 train-loss: 1.8333550691604614\n",
      "[LOG 20200511-10:17:42] epoch: 0, batch: 2134 train-loss: 1.2680530548095703\n",
      "[LOG 20200511-10:17:42] epoch: 0, batch: 2135 train-loss: 1.6558458805084229\n",
      "[LOG 20200511-10:17:42] epoch: 0, batch: 2136 train-loss: 1.1899771690368652\n",
      "[LOG 20200511-10:17:42] epoch: 0, batch: 2137 train-loss: 1.8496029376983643\n",
      "[LOG 20200511-10:17:42] epoch: 0, batch: 2138 train-loss: 2.5198123455047607\n",
      "[LOG 20200511-10:17:43] epoch: 0, batch: 2139 train-loss: 2.668409824371338\n",
      "[LOG 20200511-10:17:43] epoch: 0, batch: 2140 train-loss: 2.4132068157196045\n",
      "[LOG 20200511-10:17:43] epoch: 0, batch: 2141 train-loss: 0.8777020573616028\n",
      "[LOG 20200511-10:17:43] epoch: 0, batch: 2142 train-loss: 1.7345356941223145\n",
      "[LOG 20200511-10:17:43] epoch: 0, batch: 2143 train-loss: 1.4352669715881348\n",
      "[LOG 20200511-10:17:43] epoch: 0, batch: 2144 train-loss: 1.4737317562103271\n",
      "[LOG 20200511-10:17:43] epoch: 0, batch: 2145 train-loss: 1.4863996505737305\n",
      "[LOG 20200511-10:17:43] epoch: 0, batch: 2146 train-loss: 0.9218427538871765\n",
      "[LOG 20200511-10:17:44] epoch: 0, batch: 2147 train-loss: 1.3892762660980225\n",
      "[LOG 20200511-10:17:44] epoch: 0, batch: 2148 train-loss: 2.2754828929901123\n",
      "[LOG 20200511-10:17:44] epoch: 0, batch: 2149 train-loss: 0.5225681662559509\n",
      "[LOG 20200511-10:17:44] epoch: 0, batch: 2150 train-loss: 1.1207525730133057\n",
      "[LOG 20200511-10:17:44] epoch: 0, batch: 2151 train-loss: 2.376248836517334\n",
      "[LOG 20200511-10:17:44] epoch: 0, batch: 2152 train-loss: 1.2940469980239868\n",
      "[LOG 20200511-10:17:44] epoch: 0, batch: 2153 train-loss: 1.5405488014221191\n",
      "[LOG 20200511-10:17:44] epoch: 0, batch: 2154 train-loss: 1.4615042209625244\n",
      "[LOG 20200511-10:17:44] epoch: 0, batch: 2155 train-loss: 1.8417900800704956\n",
      "[LOG 20200511-10:17:45] epoch: 0, batch: 2156 train-loss: 2.0960402488708496\n",
      "[LOG 20200511-10:17:45] epoch: 0, batch: 2157 train-loss: 1.2660565376281738\n",
      "[LOG 20200511-10:17:45] epoch: 0, batch: 2158 train-loss: 1.7996845245361328\n",
      "[LOG 20200511-10:17:45] epoch: 0, batch: 2159 train-loss: 0.9066367149353027\n",
      "[LOG 20200511-10:17:45] epoch: 0, batch: 2160 train-loss: 1.008090615272522\n",
      "[LOG 20200511-10:17:45] epoch: 0, batch: 2161 train-loss: 1.219611644744873\n",
      "[LOG 20200511-10:17:45] epoch: 0, batch: 2162 train-loss: 2.496730327606201\n",
      "[LOG 20200511-10:17:45] epoch: 0, batch: 2163 train-loss: 1.9484896659851074\n",
      "[LOG 20200511-10:17:46] epoch: 0, batch: 2164 train-loss: 2.487658739089966\n",
      "[LOG 20200511-10:17:46] epoch: 0, batch: 2165 train-loss: 1.5084935426712036\n",
      "[LOG 20200511-10:17:46] epoch: 0, batch: 2166 train-loss: 1.10252845287323\n",
      "[LOG 20200511-10:17:46] epoch: 0, batch: 2167 train-loss: 1.7066932916641235\n",
      "[LOG 20200511-10:17:46] epoch: 0, batch: 2168 train-loss: 1.713420033454895\n",
      "[LOG 20200511-10:17:46] epoch: 0, batch: 2169 train-loss: 1.6328871250152588\n",
      "[LOG 20200511-10:17:46] epoch: 0, batch: 2170 train-loss: 2.238457202911377\n",
      "[LOG 20200511-10:17:46] epoch: 0, batch: 2171 train-loss: 1.310431718826294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:17:46] epoch: 0, batch: 2172 train-loss: 2.5020828247070312\n",
      "[LOG 20200511-10:17:47] epoch: 0, batch: 2173 train-loss: 1.187734603881836\n",
      "[LOG 20200511-10:17:47] epoch: 0, batch: 2174 train-loss: 1.3865833282470703\n",
      "[LOG 20200511-10:17:47] epoch: 0, batch: 2175 train-loss: 1.4856590032577515\n",
      "[LOG 20200511-10:17:47] epoch: 0, batch: 2176 train-loss: 2.289884328842163\n",
      "[LOG 20200511-10:17:47] epoch: 0, batch: 2177 train-loss: 1.3059558868408203\n",
      "[LOG 20200511-10:17:47] epoch: 0, batch: 2178 train-loss: 1.3936913013458252\n",
      "[LOG 20200511-10:17:47] epoch: 0, batch: 2179 train-loss: 2.286996603012085\n",
      "[LOG 20200511-10:17:47] epoch: 0, batch: 2180 train-loss: 0.9704777002334595\n",
      "[LOG 20200511-10:17:47] epoch: 0, batch: 2181 train-loss: 2.971290111541748\n",
      "[LOG 20200511-10:17:48] epoch: 0, batch: 2182 train-loss: 1.1361163854599\n",
      "[LOG 20200511-10:17:48] epoch: 0, batch: 2183 train-loss: 1.614443063735962\n",
      "[LOG 20200511-10:17:48] epoch: 0, batch: 2184 train-loss: 1.6212153434753418\n",
      "[LOG 20200511-10:17:48] epoch: 0, batch: 2185 train-loss: 1.7153306007385254\n",
      "[LOG 20200511-10:17:48] epoch: 0, batch: 2186 train-loss: 1.4426870346069336\n",
      "[LOG 20200511-10:17:48] epoch: 0, batch: 2187 train-loss: 0.9934862852096558\n",
      "[LOG 20200511-10:17:48] epoch: 0, batch: 2188 train-loss: 1.2321743965148926\n",
      "[LOG 20200511-10:17:48] epoch: 0, batch: 2189 train-loss: 1.340569257736206\n",
      "[LOG 20200511-10:17:49] epoch: 0, batch: 2190 train-loss: 1.6286811828613281\n",
      "[LOG 20200511-10:17:49] epoch: 0, batch: 2191 train-loss: 2.366556167602539\n",
      "[LOG 20200511-10:17:49] epoch: 0, batch: 2192 train-loss: 1.6222020387649536\n",
      "[LOG 20200511-10:17:49] epoch: 0, batch: 2193 train-loss: 0.9325572848320007\n",
      "[LOG 20200511-10:17:49] epoch: 0, batch: 2194 train-loss: 2.777433156967163\n",
      "[LOG 20200511-10:17:49] epoch: 0, batch: 2195 train-loss: 1.701570987701416\n",
      "[LOG 20200511-10:17:49] epoch: 0, batch: 2196 train-loss: 1.1968398094177246\n",
      "[LOG 20200511-10:17:49] epoch: 0, batch: 2197 train-loss: 1.1114957332611084\n",
      "[LOG 20200511-10:17:50] epoch: 0, batch: 2198 train-loss: 1.5872597694396973\n",
      "[LOG 20200511-10:17:50] epoch: 0, batch: 2199 train-loss: 1.226725459098816\n",
      "[LOG 20200511-10:17:50] epoch: 0, batch: 2200 train-loss: 2.3446907997131348\n",
      "[LOG 20200511-10:17:50] epoch: 0, batch: 2201 train-loss: 1.4156074523925781\n",
      "[LOG 20200511-10:17:50] epoch: 0, batch: 2202 train-loss: 1.983193039894104\n",
      "[LOG 20200511-10:17:50] epoch: 0, batch: 2203 train-loss: 1.6887699365615845\n",
      "[LOG 20200511-10:17:50] epoch: 0, batch: 2204 train-loss: 2.054121732711792\n",
      "[LOG 20200511-10:17:50] epoch: 0, batch: 2205 train-loss: 1.5327708721160889\n",
      "[LOG 20200511-10:17:51] epoch: 0, batch: 2206 train-loss: 1.9411712884902954\n",
      "[LOG 20200511-10:17:51] epoch: 0, batch: 2207 train-loss: 1.5427730083465576\n",
      "[LOG 20200511-10:17:51] epoch: 0, batch: 2208 train-loss: 2.178776979446411\n",
      "[LOG 20200511-10:17:51] epoch: 0, batch: 2209 train-loss: 1.9585342407226562\n",
      "[LOG 20200511-10:17:51] epoch: 0, batch: 2210 train-loss: 0.9569835662841797\n",
      "[LOG 20200511-10:17:51] epoch: 0, batch: 2211 train-loss: 1.4608198404312134\n",
      "[LOG 20200511-10:17:51] epoch: 0, batch: 2212 train-loss: 1.7270375490188599\n",
      "[LOG 20200511-10:17:51] epoch: 0, batch: 2213 train-loss: 1.3251971006393433\n",
      "[LOG 20200511-10:17:52] epoch: 0, batch: 2214 train-loss: 1.3713175058364868\n",
      "[LOG 20200511-10:17:52] epoch: 0, batch: 2215 train-loss: 1.5838221311569214\n",
      "[LOG 20200511-10:17:52] epoch: 0, batch: 2216 train-loss: 1.7238975763320923\n",
      "[LOG 20200511-10:17:52] epoch: 0, batch: 2217 train-loss: 2.339223623275757\n",
      "[LOG 20200511-10:17:52] epoch: 0, batch: 2218 train-loss: 1.9050426483154297\n",
      "[LOG 20200511-10:17:52] epoch: 0, batch: 2219 train-loss: 1.4352798461914062\n",
      "[LOG 20200511-10:17:52] epoch: 0, batch: 2220 train-loss: 1.54305100440979\n",
      "[LOG 20200511-10:17:52] epoch: 0, batch: 2221 train-loss: 2.388108730316162\n",
      "[LOG 20200511-10:17:53] epoch: 0, batch: 2222 train-loss: 3.0147571563720703\n",
      "[LOG 20200511-10:17:53] epoch: 0, batch: 2223 train-loss: 2.7139275074005127\n",
      "[LOG 20200511-10:17:53] epoch: 0, batch: 2224 train-loss: 1.5158765316009521\n",
      "[LOG 20200511-10:17:53] epoch: 0, batch: 2225 train-loss: 2.2343626022338867\n",
      "[LOG 20200511-10:17:53] epoch: 0, batch: 2226 train-loss: 0.9333789944648743\n",
      "[LOG 20200511-10:17:53] epoch: 0, batch: 2227 train-loss: 1.9956525564193726\n",
      "[LOG 20200511-10:17:53] epoch: 0, batch: 2228 train-loss: 1.8555710315704346\n",
      "[LOG 20200511-10:17:53] epoch: 0, batch: 2229 train-loss: 1.0141253471374512\n",
      "[LOG 20200511-10:17:53] epoch: 0, batch: 2230 train-loss: 2.071742057800293\n",
      "[LOG 20200511-10:17:54] epoch: 0, batch: 2231 train-loss: 1.8831301927566528\n",
      "[LOG 20200511-10:17:54] epoch: 0, batch: 2232 train-loss: 0.9432539939880371\n",
      "[LOG 20200511-10:17:54] epoch: 0, batch: 2233 train-loss: 1.218847632408142\n",
      "[LOG 20200511-10:17:54] epoch: 0, batch: 2234 train-loss: 1.29156494140625\n",
      "[LOG 20200511-10:17:54] epoch: 0, batch: 2235 train-loss: 1.5696096420288086\n",
      "[LOG 20200511-10:17:54] epoch: 0, batch: 2236 train-loss: 1.2417590618133545\n",
      "[LOG 20200511-10:17:54] epoch: 0, batch: 2237 train-loss: 2.111285924911499\n",
      "[LOG 20200511-10:17:54] epoch: 0, batch: 2238 train-loss: 1.1380985975265503\n",
      "[LOG 20200511-10:17:55] epoch: 0, batch: 2239 train-loss: 2.1168980598449707\n",
      "[LOG 20200511-10:17:55] epoch: 0, batch: 2240 train-loss: 2.12396240234375\n",
      "[LOG 20200511-10:17:55] epoch: 0, batch: 2241 train-loss: 1.8757497072219849\n",
      "[LOG 20200511-10:17:55] epoch: 0, batch: 2242 train-loss: 1.4471523761749268\n",
      "[LOG 20200511-10:17:55] epoch: 0, batch: 2243 train-loss: 1.0337610244750977\n",
      "[LOG 20200511-10:17:55] epoch: 0, batch: 2244 train-loss: 1.9019687175750732\n",
      "[LOG 20200511-10:17:55] epoch: 0, batch: 2245 train-loss: 1.9870080947875977\n",
      "[LOG 20200511-10:17:55] epoch: 0, batch: 2246 train-loss: 1.2053406238555908\n",
      "[LOG 20200511-10:17:56] epoch: 0, batch: 2247 train-loss: 1.1095060110092163\n",
      "[LOG 20200511-10:17:56] epoch: 0, batch: 2248 train-loss: 1.671858310699463\n",
      "[LOG 20200511-10:17:56] epoch: 0, batch: 2249 train-loss: 1.5347462892532349\n",
      "[LOG 20200511-10:17:56] epoch: 0, batch: 2250 train-loss: 3.1105456352233887\n",
      "[LOG 20200511-10:17:56] epoch: 0, batch: 2251 train-loss: 1.5313103199005127\n",
      "[LOG 20200511-10:17:56] epoch: 0, batch: 2252 train-loss: 1.835106611251831\n",
      "[LOG 20200511-10:17:56] epoch: 0, batch: 2253 train-loss: 1.6331627368927002\n",
      "[LOG 20200511-10:17:56] epoch: 0, batch: 2254 train-loss: 1.6111966371536255\n",
      "[LOG 20200511-10:17:57] epoch: 0, batch: 2255 train-loss: 1.3121973276138306\n",
      "[LOG 20200511-10:17:57] epoch: 0, batch: 2256 train-loss: 2.999350070953369\n",
      "[LOG 20200511-10:17:57] epoch: 0, batch: 2257 train-loss: 0.9185624122619629\n",
      "[LOG 20200511-10:17:57] epoch: 0, batch: 2258 train-loss: 1.535740613937378\n",
      "[LOG 20200511-10:17:57] epoch: 0, batch: 2259 train-loss: 0.4711661636829376\n",
      "[LOG 20200511-10:17:57] epoch: 0, batch: 2260 train-loss: 1.1458637714385986\n",
      "[LOG 20200511-10:17:57] epoch: 0, batch: 2261 train-loss: 2.280421495437622\n",
      "[LOG 20200511-10:17:57] epoch: 0, batch: 2262 train-loss: 3.2588162422180176\n",
      "[LOG 20200511-10:17:57] epoch: 0, batch: 2263 train-loss: 1.8471243381500244\n",
      "[LOG 20200511-10:17:58] epoch: 0, batch: 2264 train-loss: 1.6540511846542358\n",
      "[LOG 20200511-10:17:58] epoch: 0, batch: 2265 train-loss: 0.9122755527496338\n",
      "[LOG 20200511-10:17:58] epoch: 0, batch: 2266 train-loss: 1.2923182249069214\n",
      "[LOG 20200511-10:17:58] epoch: 0, batch: 2267 train-loss: 1.317508578300476\n",
      "[LOG 20200511-10:17:58] epoch: 0, batch: 2268 train-loss: 1.6320030689239502\n",
      "[LOG 20200511-10:17:58] epoch: 0, batch: 2269 train-loss: 1.4361830949783325\n",
      "[LOG 20200511-10:17:58] epoch: 0, batch: 2270 train-loss: 2.194427013397217\n",
      "[LOG 20200511-10:17:58] epoch: 0, batch: 2271 train-loss: 1.320633888244629\n",
      "[LOG 20200511-10:17:59] epoch: 0, batch: 2272 train-loss: 0.9828803539276123\n",
      "[LOG 20200511-10:17:59] epoch: 0, batch: 2273 train-loss: 1.084483027458191\n",
      "[LOG 20200511-10:17:59] epoch: 0, batch: 2274 train-loss: 2.1836349964141846\n",
      "[LOG 20200511-10:17:59] epoch: 0, batch: 2275 train-loss: 1.3745815753936768\n",
      "[LOG 20200511-10:17:59] epoch: 0, batch: 2276 train-loss: 2.179600954055786\n",
      "[LOG 20200511-10:17:59] epoch: 0, batch: 2277 train-loss: 1.6522984504699707\n",
      "[LOG 20200511-10:17:59] epoch: 0, batch: 2278 train-loss: 1.2561116218566895\n",
      "[LOG 20200511-10:17:59] epoch: 0, batch: 2279 train-loss: 1.7036526203155518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:17:59] epoch: 0, batch: 2280 train-loss: 1.498608946800232\n",
      "[LOG 20200511-10:18:00] epoch: 0, batch: 2281 train-loss: 1.933569312095642\n",
      "[LOG 20200511-10:18:00] epoch: 0, batch: 2282 train-loss: 1.6173901557922363\n",
      "[LOG 20200511-10:18:00] epoch: 0, batch: 2283 train-loss: 1.653520107269287\n",
      "[LOG 20200511-10:18:00] epoch: 0, batch: 2284 train-loss: 1.316666603088379\n",
      "[LOG 20200511-10:18:00] epoch: 0, batch: 2285 train-loss: 3.2425730228424072\n",
      "[LOG 20200511-10:18:00] epoch: 0, batch: 2286 train-loss: 1.7226359844207764\n",
      "[LOG 20200511-10:18:00] epoch: 0, batch: 2287 train-loss: 1.7199444770812988\n",
      "[LOG 20200511-10:18:00] epoch: 0, batch: 2288 train-loss: 1.3397308588027954\n",
      "[LOG 20200511-10:18:01] epoch: 0, batch: 2289 train-loss: 2.4927921295166016\n",
      "[LOG 20200511-10:18:01] epoch: 0, batch: 2290 train-loss: 1.6303621530532837\n",
      "[LOG 20200511-10:18:01] epoch: 0, batch: 2291 train-loss: 0.8486689329147339\n",
      "[LOG 20200511-10:18:01] epoch: 0, batch: 2292 train-loss: 1.0741510391235352\n",
      "[LOG 20200511-10:18:01] epoch: 0, batch: 2293 train-loss: 2.142043113708496\n",
      "[LOG 20200511-10:18:01] epoch: 0, batch: 2294 train-loss: 1.5925679206848145\n",
      "[LOG 20200511-10:18:01] epoch: 0, batch: 2295 train-loss: 1.6581186056137085\n",
      "[LOG 20200511-10:18:01] epoch: 0, batch: 2296 train-loss: 2.207848310470581\n",
      "[LOG 20200511-10:18:01] epoch: 0, batch: 2297 train-loss: 2.0880520343780518\n",
      "[LOG 20200511-10:18:02] epoch: 0, batch: 2298 train-loss: 1.6150779724121094\n",
      "[LOG 20200511-10:18:02] epoch: 0, batch: 2299 train-loss: 0.9782410860061646\n",
      "[LOG 20200511-10:18:02] epoch: 0, batch: 2300 train-loss: 1.1362555027008057\n",
      "[LOG 20200511-10:18:02] epoch: 0, batch: 2301 train-loss: 1.809105396270752\n",
      "[LOG 20200511-10:18:02] epoch: 0, batch: 2302 train-loss: 1.4236892461776733\n",
      "[LOG 20200511-10:18:02] epoch: 0, batch: 2303 train-loss: 0.9728050231933594\n",
      "[LOG 20200511-10:18:02] epoch: 0, batch: 2304 train-loss: 0.9289153218269348\n",
      "[LOG 20200511-10:18:02] epoch: 0, batch: 2305 train-loss: 1.2335100173950195\n",
      "[LOG 20200511-10:18:02] epoch: 0, batch: 2306 train-loss: 1.1113429069519043\n",
      "[LOG 20200511-10:18:03] epoch: 0, batch: 2307 train-loss: 1.5586122274398804\n",
      "[LOG 20200511-10:18:03] epoch: 0, batch: 2308 train-loss: 0.9080609083175659\n",
      "[LOG 20200511-10:18:03] epoch: 0, batch: 2309 train-loss: 1.8918206691741943\n",
      "[LOG 20200511-10:18:03] epoch: 0, batch: 2310 train-loss: 1.6823521852493286\n",
      "[LOG 20200511-10:18:03] epoch: 0, batch: 2311 train-loss: 1.4093817472457886\n",
      "[LOG 20200511-10:18:03] epoch: 0, batch: 2312 train-loss: 1.871516227722168\n",
      "[LOG 20200511-10:18:03] epoch: 0, batch: 2313 train-loss: 2.747732400894165\n",
      "[LOG 20200511-10:18:03] epoch: 0, batch: 2314 train-loss: 1.0804072618484497\n",
      "[LOG 20200511-10:18:03] epoch: 0, batch: 2315 train-loss: 1.8632521629333496\n",
      "[LOG 20200511-10:18:04] epoch: 0, batch: 2316 train-loss: 1.3219029903411865\n",
      "[LOG 20200511-10:18:04] epoch: 0, batch: 2317 train-loss: 1.5130491256713867\n",
      "[LOG 20200511-10:18:04] epoch: 0, batch: 2318 train-loss: 1.1243629455566406\n",
      "[LOG 20200511-10:18:04] epoch: 0, batch: 2319 train-loss: 2.056094169616699\n",
      "[LOG 20200511-10:18:04] epoch: 0, batch: 2320 train-loss: 1.666424036026001\n",
      "[LOG 20200511-10:18:04] epoch: 0, batch: 2321 train-loss: 2.372041940689087\n",
      "[LOG 20200511-10:18:04] epoch: 0, batch: 2322 train-loss: 1.4721137285232544\n",
      "[LOG 20200511-10:18:04] epoch: 0, batch: 2323 train-loss: 1.9579532146453857\n",
      "[LOG 20200511-10:18:05] epoch: 0, batch: 2324 train-loss: 0.8446488976478577\n",
      "[LOG 20200511-10:18:05] epoch: 0, batch: 2325 train-loss: 1.6841011047363281\n",
      "[LOG 20200511-10:18:05] epoch: 0, batch: 2326 train-loss: 1.1687443256378174\n",
      "[LOG 20200511-10:18:05] epoch: 0, batch: 2327 train-loss: 1.45316743850708\n",
      "[LOG 20200511-10:18:05] epoch: 0, batch: 2328 train-loss: 1.0008102655410767\n",
      "[LOG 20200511-10:18:05] epoch: 0, batch: 2329 train-loss: 1.577993631362915\n",
      "[LOG 20200511-10:18:05] epoch: 0, batch: 2330 train-loss: 0.8365150690078735\n",
      "[LOG 20200511-10:18:05] epoch: 0, batch: 2331 train-loss: 0.9685832262039185\n",
      "[LOG 20200511-10:18:06] epoch: 0, batch: 2332 train-loss: 0.8617309927940369\n",
      "[LOG 20200511-10:18:06] epoch: 0, batch: 2333 train-loss: 1.076022982597351\n",
      "[LOG 20200511-10:18:06] epoch: 0, batch: 2334 train-loss: 1.76413094997406\n",
      "[LOG 20200511-10:18:06] epoch: 0, batch: 2335 train-loss: 1.435849905014038\n",
      "[LOG 20200511-10:18:06] epoch: 0, batch: 2336 train-loss: 2.188969373703003\n",
      "[LOG 20200511-10:18:06] epoch: 0, batch: 2337 train-loss: 1.7836076021194458\n",
      "[LOG 20200511-10:18:06] epoch: 0, batch: 2338 train-loss: 1.4008715152740479\n",
      "[LOG 20200511-10:18:06] epoch: 0, batch: 2339 train-loss: 1.5547125339508057\n",
      "[LOG 20200511-10:18:06] epoch: 0, batch: 2340 train-loss: 1.8712292909622192\n",
      "[LOG 20200511-10:18:07] epoch: 0, batch: 2341 train-loss: 1.3652595281600952\n",
      "[LOG 20200511-10:18:07] epoch: 0, batch: 2342 train-loss: 0.9892695546150208\n",
      "[LOG 20200511-10:18:07] epoch: 0, batch: 2343 train-loss: 1.2752320766448975\n",
      "[LOG 20200511-10:18:07] epoch: 0, batch: 2344 train-loss: 1.1689616441726685\n",
      "[LOG 20200511-10:18:07] epoch: 0, batch: 2345 train-loss: 2.3278796672821045\n",
      "[LOG 20200511-10:18:07] epoch: 0, batch: 2346 train-loss: 2.5660834312438965\n",
      "[LOG 20200511-10:18:07] epoch: 0, batch: 2347 train-loss: 2.01336407661438\n",
      "[LOG 20200511-10:18:07] epoch: 0, batch: 2348 train-loss: 1.7284153699874878\n",
      "[LOG 20200511-10:18:08] epoch: 0, batch: 2349 train-loss: 1.437485933303833\n",
      "[LOG 20200511-10:18:08] epoch: 0, batch: 2350 train-loss: 1.6981751918792725\n",
      "[LOG 20200511-10:18:08] epoch: 0, batch: 2351 train-loss: 1.7757482528686523\n",
      "[LOG 20200511-10:18:08] epoch: 0, batch: 2352 train-loss: 2.7005887031555176\n",
      "[LOG 20200511-10:18:08] epoch: 0, batch: 2353 train-loss: 1.528414011001587\n",
      "[LOG 20200511-10:18:08] epoch: 0, batch: 2354 train-loss: 2.102328300476074\n",
      "[LOG 20200511-10:18:08] epoch: 0, batch: 2355 train-loss: 1.5728498697280884\n",
      "[LOG 20200511-10:18:08] epoch: 0, batch: 2356 train-loss: 1.7270785570144653\n",
      "[LOG 20200511-10:18:09] epoch: 0, batch: 2357 train-loss: 1.3218806982040405\n",
      "[LOG 20200511-10:18:09] epoch: 0, batch: 2358 train-loss: 1.4328283071517944\n",
      "[LOG 20200511-10:18:09] epoch: 0, batch: 2359 train-loss: 1.972930908203125\n",
      "[LOG 20200511-10:18:09] epoch: 0, batch: 2360 train-loss: 1.9087190628051758\n",
      "[LOG 20200511-10:18:09] epoch: 0, batch: 2361 train-loss: 1.0970159769058228\n",
      "[LOG 20200511-10:18:09] epoch: 0, batch: 2362 train-loss: 1.3939032554626465\n",
      "[LOG 20200511-10:18:09] epoch: 0, batch: 2363 train-loss: 1.4165594577789307\n",
      "[LOG 20200511-10:18:09] epoch: 0, batch: 2364 train-loss: 1.1892783641815186\n",
      "[LOG 20200511-10:18:09] epoch: 0, batch: 2365 train-loss: 1.3397458791732788\n",
      "[LOG 20200511-10:18:10] epoch: 0, batch: 2366 train-loss: 1.2702500820159912\n",
      "[LOG 20200511-10:18:10] epoch: 0, batch: 2367 train-loss: 1.6465345621109009\n",
      "[LOG 20200511-10:18:10] epoch: 0, batch: 2368 train-loss: 0.72288578748703\n",
      "[LOG 20200511-10:18:10] epoch: 0, batch: 2369 train-loss: 2.245602607727051\n",
      "[LOG 20200511-10:18:10] epoch: 0, batch: 2370 train-loss: 2.0455167293548584\n",
      "[LOG 20200511-10:18:10] epoch: 0, batch: 2371 train-loss: 2.0343284606933594\n",
      "[LOG 20200511-10:18:10] epoch: 0, batch: 2372 train-loss: 1.5442343950271606\n",
      "[LOG 20200511-10:18:10] epoch: 0, batch: 2373 train-loss: 1.2307689189910889\n",
      "[LOG 20200511-10:18:10] epoch: 0, batch: 2374 train-loss: 1.0899019241333008\n",
      "[LOG 20200511-10:18:11] epoch: 0, batch: 2375 train-loss: 1.4107133150100708\n",
      "[LOG 20200511-10:18:11] epoch: 0, batch: 2376 train-loss: 1.4981491565704346\n",
      "[LOG 20200511-10:18:11] epoch: 0, batch: 2377 train-loss: 1.9525867700576782\n",
      "[LOG 20200511-10:18:11] epoch: 0, batch: 2378 train-loss: 0.6037585735321045\n",
      "[LOG 20200511-10:18:11] epoch: 0, batch: 2379 train-loss: 1.6987762451171875\n",
      "[LOG 20200511-10:18:11] epoch: 0, batch: 2380 train-loss: 1.464556336402893\n",
      "[LOG 20200511-10:18:11] epoch: 0, batch: 2381 train-loss: 1.549664855003357\n",
      "[LOG 20200511-10:18:11] epoch: 0, batch: 2382 train-loss: 2.0229363441467285\n",
      "[LOG 20200511-10:18:11] epoch: 0, batch: 2383 train-loss: 2.7044906616210938\n",
      "[LOG 20200511-10:18:12] epoch: 0, batch: 2384 train-loss: 1.6556596755981445\n",
      "[LOG 20200511-10:18:12] epoch: 0, batch: 2385 train-loss: 1.792468547821045\n",
      "[LOG 20200511-10:18:12] epoch: 0, batch: 2386 train-loss: 1.8092541694641113\n",
      "[LOG 20200511-10:18:12] epoch: 0, batch: 2387 train-loss: 2.1609582901000977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:18:12] epoch: 0, batch: 2388 train-loss: 0.853173553943634\n",
      "[LOG 20200511-10:18:12] epoch: 0, batch: 2389 train-loss: 1.7728261947631836\n",
      "[LOG 20200511-10:18:12] epoch: 0, batch: 2390 train-loss: 0.4271618723869324\n",
      "[LOG 20200511-10:18:12] epoch: 0, batch: 2391 train-loss: 1.3991529941558838\n",
      "[LOG 20200511-10:18:12] epoch: 0, batch: 2392 train-loss: 1.4284476041793823\n",
      "[LOG 20200511-10:18:13] epoch: 0, batch: 2393 train-loss: 1.6710498332977295\n",
      "[LOG 20200511-10:18:13] epoch: 0, batch: 2394 train-loss: 0.7176219820976257\n",
      "[LOG 20200511-10:18:13] epoch: 0, batch: 2395 train-loss: 1.2056694030761719\n",
      "[LOG 20200511-10:18:13] epoch: 0, batch: 2396 train-loss: 2.7290258407592773\n",
      "[LOG 20200511-10:18:13] epoch: 0, batch: 2397 train-loss: 0.5761203765869141\n",
      "[LOG 20200511-10:18:13] epoch: 0, batch: 2398 train-loss: 2.0119669437408447\n",
      "[LOG 20200511-10:18:13] epoch: 0, batch: 2399 train-loss: 1.5912187099456787\n",
      "[LOG 20200511-10:18:13] epoch: 0, batch: 2400 train-loss: 1.3594111204147339\n",
      "[LOG 20200511-10:18:13] epoch: 0, batch: 2401 train-loss: 1.0307981967926025\n",
      "[LOG 20200511-10:18:14] epoch: 0, batch: 2402 train-loss: 1.4076988697052002\n",
      "[LOG 20200511-10:18:14] epoch: 0, batch: 2403 train-loss: 1.7728289365768433\n",
      "[LOG 20200511-10:18:14] epoch: 0, batch: 2404 train-loss: 1.0618228912353516\n",
      "[LOG 20200511-10:18:14] epoch: 0, batch: 2405 train-loss: 2.406473398208618\n",
      "[LOG 20200511-10:18:14] epoch: 0, batch: 2406 train-loss: 1.0766524076461792\n",
      "[LOG 20200511-10:18:14] epoch: 0, batch: 2407 train-loss: 0.8341749310493469\n",
      "[LOG 20200511-10:18:14] epoch: 0, batch: 2408 train-loss: 1.695950984954834\n",
      "[LOG 20200511-10:18:14] epoch: 0, batch: 2409 train-loss: 2.2318618297576904\n",
      "[LOG 20200511-10:18:14] epoch: 0, batch: 2410 train-loss: 0.9423316717147827\n",
      "[LOG 20200511-10:18:14] epoch: 0, batch: 2411 train-loss: 2.0074691772460938\n",
      "[LOG 20200511-10:18:15] epoch: 0, batch: 2412 train-loss: 0.8924669027328491\n",
      "[LOG 20200511-10:18:15] epoch: 0, batch: 2413 train-loss: 1.66892671585083\n",
      "[LOG 20200511-10:18:15] epoch: 0, batch: 2414 train-loss: 1.6707607507705688\n",
      "[LOG 20200511-10:18:15] epoch: 0, batch: 2415 train-loss: 1.1846377849578857\n",
      "[LOG 20200511-10:18:15] epoch: 0, batch: 2416 train-loss: 2.5934078693389893\n",
      "[LOG 20200511-10:18:15] epoch: 0, batch: 2417 train-loss: 1.6904135942459106\n",
      "[LOG 20200511-10:18:15] epoch: 0, batch: 2418 train-loss: 1.3766417503356934\n",
      "[LOG 20200511-10:18:15] epoch: 0, batch: 2419 train-loss: 2.02567458152771\n",
      "[LOG 20200511-10:18:15] epoch: 0, batch: 2420 train-loss: 1.808340072631836\n",
      "[LOG 20200511-10:18:16] epoch: 0, batch: 2421 train-loss: 1.2771766185760498\n",
      "[LOG 20200511-10:18:16] epoch: 0, batch: 2422 train-loss: 1.9010865688323975\n",
      "[LOG 20200511-10:18:16] epoch: 0, batch: 2423 train-loss: 2.0964150428771973\n",
      "[LOG 20200511-10:18:16] epoch: 0, batch: 2424 train-loss: 2.3345351219177246\n",
      "[LOG 20200511-10:18:16] epoch: 0, batch: 2425 train-loss: 1.4491170644760132\n",
      "[LOG 20200511-10:18:16] epoch: 0, batch: 2426 train-loss: 1.5197808742523193\n",
      "[LOG 20200511-10:18:16] epoch: 0, batch: 2427 train-loss: 1.099522590637207\n",
      "[LOG 20200511-10:18:16] epoch: 0, batch: 2428 train-loss: 1.9085681438446045\n",
      "[LOG 20200511-10:18:17] epoch: 0, batch: 2429 train-loss: 1.937385082244873\n",
      "[LOG 20200511-10:18:17] epoch: 0, batch: 2430 train-loss: 1.4852051734924316\n",
      "[LOG 20200511-10:18:17] epoch: 0, batch: 2431 train-loss: 1.983236312866211\n",
      "[LOG 20200511-10:18:17] epoch: 0, batch: 2432 train-loss: 1.5119528770446777\n",
      "[LOG 20200511-10:18:17] epoch: 0, batch: 2433 train-loss: 1.8099915981292725\n",
      "[LOG 20200511-10:18:17] epoch: 0, batch: 2434 train-loss: 2.169703483581543\n",
      "[LOG 20200511-10:18:17] epoch: 0, batch: 2435 train-loss: 1.122730016708374\n",
      "[LOG 20200511-10:18:17] epoch: 0, batch: 2436 train-loss: 1.5704262256622314\n",
      "[LOG 20200511-10:18:18] epoch: 0, batch: 2437 train-loss: 2.109964370727539\n",
      "[LOG 20200511-10:18:18] epoch: 0, batch: 2438 train-loss: 1.0794236660003662\n",
      "[LOG 20200511-10:18:18] epoch: 0, batch: 2439 train-loss: 1.4674535989761353\n",
      "[LOG 20200511-10:18:18] epoch: 0, batch: 2440 train-loss: 1.7586699724197388\n",
      "[LOG 20200511-10:18:18] epoch: 0, batch: 2441 train-loss: 1.4725861549377441\n",
      "[LOG 20200511-10:18:18] epoch: 0, batch: 2442 train-loss: 2.2757720947265625\n",
      "[LOG 20200511-10:18:18] epoch: 0, batch: 2443 train-loss: 2.6280581951141357\n",
      "[LOG 20200511-10:18:18] epoch: 0, batch: 2444 train-loss: 1.6197086572647095\n",
      "[LOG 20200511-10:18:18] epoch: 0, batch: 2445 train-loss: 1.1014446020126343\n",
      "[LOG 20200511-10:18:19] epoch: 0, batch: 2446 train-loss: 1.257947564125061\n",
      "[LOG 20200511-10:18:19] epoch: 0, batch: 2447 train-loss: 1.10609769821167\n",
      "[LOG 20200511-10:18:19] epoch: 0, batch: 2448 train-loss: 1.2166907787322998\n",
      "[LOG 20200511-10:18:19] epoch: 0, batch: 2449 train-loss: 1.8452855348587036\n",
      "[LOG 20200511-10:18:19] epoch: 0, batch: 2450 train-loss: 1.817151427268982\n",
      "[LOG 20200511-10:18:19] epoch: 0, batch: 2451 train-loss: 1.7157225608825684\n",
      "[LOG 20200511-10:18:19] epoch: 0, batch: 2452 train-loss: 1.774156928062439\n",
      "[LOG 20200511-10:18:19] epoch: 0, batch: 2453 train-loss: 2.271650791168213\n",
      "[LOG 20200511-10:18:20] epoch: 0, batch: 2454 train-loss: 1.8437049388885498\n",
      "[LOG 20200511-10:18:20] epoch: 0, batch: 2455 train-loss: 1.4477354288101196\n",
      "[LOG 20200511-10:18:20] epoch: 0, batch: 2456 train-loss: 1.5595624446868896\n",
      "[LOG 20200511-10:18:20] epoch: 0, batch: 2457 train-loss: 1.3729441165924072\n",
      "[LOG 20200511-10:18:20] epoch: 0, batch: 2458 train-loss: 2.207705497741699\n",
      "[LOG 20200511-10:18:20] epoch: 0, batch: 2459 train-loss: 1.391518235206604\n",
      "[LOG 20200511-10:18:20] epoch: 0, batch: 2460 train-loss: 1.936477541923523\n",
      "[LOG 20200511-10:18:20] epoch: 0, batch: 2461 train-loss: 1.9020054340362549\n",
      "[LOG 20200511-10:18:21] epoch: 0, batch: 2462 train-loss: 1.2278199195861816\n",
      "[LOG 20200511-10:18:21] epoch: 0, batch: 2463 train-loss: 0.9787017703056335\n",
      "[LOG 20200511-10:18:21] epoch: 0, batch: 2464 train-loss: 1.984616994857788\n",
      "[LOG 20200511-10:18:21] epoch: 0, batch: 2465 train-loss: 3.274669647216797\n",
      "[LOG 20200511-10:18:21] epoch: 0, batch: 2466 train-loss: 1.947756052017212\n",
      "[LOG 20200511-10:18:21] epoch: 0, batch: 2467 train-loss: 1.892981767654419\n",
      "[LOG 20200511-10:18:21] epoch: 0, batch: 2468 train-loss: 1.5842901468276978\n",
      "[LOG 20200511-10:18:21] epoch: 0, batch: 2469 train-loss: 1.6132233142852783\n",
      "[LOG 20200511-10:18:21] epoch: 0, batch: 2470 train-loss: 1.227512240409851\n",
      "[LOG 20200511-10:18:22] epoch: 0, batch: 2471 train-loss: 2.972435235977173\n",
      "[LOG 20200511-10:18:22] epoch: 0, batch: 2472 train-loss: 1.6576584577560425\n",
      "[LOG 20200511-10:18:22] epoch: 0, batch: 2473 train-loss: 1.9522836208343506\n",
      "[LOG 20200511-10:18:22] epoch: 0, batch: 2474 train-loss: 2.2530362606048584\n",
      "[LOG 20200511-10:18:22] epoch: 0, batch: 2475 train-loss: 1.0650078058242798\n",
      "[LOG 20200511-10:18:22] epoch: 0, batch: 2476 train-loss: 1.80612313747406\n",
      "[LOG 20200511-10:18:22] epoch: 0, batch: 2477 train-loss: 0.720644474029541\n",
      "[LOG 20200511-10:18:22] epoch: 0, batch: 2478 train-loss: 2.5801424980163574\n",
      "[LOG 20200511-10:18:22] epoch: 0, batch: 2479 train-loss: 3.2478556632995605\n",
      "[LOG 20200511-10:18:23] epoch: 0, batch: 2480 train-loss: 1.2211830615997314\n",
      "[LOG 20200511-10:18:23] epoch: 0, batch: 2481 train-loss: 1.0724917650222778\n",
      "[LOG 20200511-10:18:23] epoch: 0, batch: 2482 train-loss: 2.1058952808380127\n",
      "[LOG 20200511-10:18:23] epoch: 0, batch: 2483 train-loss: 1.3898587226867676\n",
      "[LOG 20200511-10:18:23] epoch: 0, batch: 2484 train-loss: 1.7773511409759521\n",
      "[LOG 20200511-10:18:23] epoch: 0, batch: 2485 train-loss: 1.2557569742202759\n",
      "[LOG 20200511-10:18:23] epoch: 0, batch: 2486 train-loss: 1.0600866079330444\n",
      "[LOG 20200511-10:18:23] epoch: 0, batch: 2487 train-loss: 2.020174980163574\n",
      "[LOG 20200511-10:18:24] epoch: 0, batch: 2488 train-loss: 1.4197683334350586\n",
      "[LOG 20200511-10:18:24] epoch: 0, batch: 2489 train-loss: 1.4005143642425537\n",
      "[LOG 20200511-10:18:24] epoch: 0, batch: 2490 train-loss: 1.6996967792510986\n",
      "[LOG 20200511-10:18:24] epoch: 0, batch: 2491 train-loss: 1.2486441135406494\n",
      "[LOG 20200511-10:18:24] epoch: 0, batch: 2492 train-loss: 1.4640469551086426\n",
      "[LOG 20200511-10:18:24] epoch: 0, batch: 2493 train-loss: 1.523136854171753\n",
      "[LOG 20200511-10:18:24] epoch: 0, batch: 2494 train-loss: 2.124890089035034\n",
      "[LOG 20200511-10:18:24] epoch: 0, batch: 2495 train-loss: 1.1116169691085815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:18:24] epoch: 0, batch: 2496 train-loss: 1.4756364822387695\n",
      "[LOG 20200511-10:18:25] epoch: 0, batch: 2497 train-loss: 1.3357263803482056\n",
      "[LOG 20200511-10:18:25] epoch: 0, batch: 2498 train-loss: 1.9305555820465088\n",
      "[LOG 20200511-10:18:25] epoch: 0, batch: 2499 train-loss: 1.9241054058074951\n",
      "[LOG 20200511-10:18:25] epoch: 0, batch: 2500 train-loss: 1.1238799095153809\n",
      "[LOG 20200511-10:18:25] epoch: 0, batch: 2501 train-loss: 1.0024017095565796\n",
      "[LOG 20200511-10:18:25] epoch: 0, batch: 2502 train-loss: 1.0264769792556763\n",
      "[LOG 20200511-10:18:25] epoch: 0, batch: 2503 train-loss: 1.9132435321807861\n",
      "[LOG 20200511-10:18:25] epoch: 0, batch: 2504 train-loss: 1.9558407068252563\n",
      "[LOG 20200511-10:18:25] epoch: 0, batch: 2505 train-loss: 1.6257659196853638\n",
      "[LOG 20200511-10:18:26] epoch: 0, batch: 2506 train-loss: 0.8100277185440063\n",
      "[LOG 20200511-10:18:26] epoch: 0, batch: 2507 train-loss: 1.7089810371398926\n",
      "[LOG 20200511-10:18:26] epoch: 0, batch: 2508 train-loss: 1.7842085361480713\n",
      "[LOG 20200511-10:18:26] epoch: 0, batch: 2509 train-loss: 1.912304401397705\n",
      "[LOG 20200511-10:18:26] epoch: 0, batch: 2510 train-loss: 1.7888898849487305\n",
      "[LOG 20200511-10:18:26] epoch: 0, batch: 2511 train-loss: 2.66955828666687\n",
      "[LOG 20200511-10:18:26] epoch: 0, batch: 2512 train-loss: 1.1485657691955566\n",
      "[LOG 20200511-10:18:26] epoch: 0, batch: 2513 train-loss: 1.252584457397461\n",
      "[LOG 20200511-10:18:26] epoch: 0, batch: 2514 train-loss: 2.6316113471984863\n",
      "[LOG 20200511-10:18:27] epoch: 0, batch: 2515 train-loss: 1.0685646533966064\n",
      "[LOG 20200511-10:18:27] epoch: 0, batch: 2516 train-loss: 1.319779396057129\n",
      "[LOG 20200511-10:18:27] epoch: 0, batch: 2517 train-loss: 1.200327754020691\n",
      "[LOG 20200511-10:18:27] epoch: 0, batch: 2518 train-loss: 0.8532832264900208\n",
      "[LOG 20200511-10:18:27] epoch: 0, batch: 2519 train-loss: 1.890324592590332\n",
      "[LOG 20200511-10:18:27] epoch: 0, batch: 2520 train-loss: 1.8580245971679688\n",
      "[LOG 20200511-10:18:27] epoch: 0, batch: 2521 train-loss: 0.8888903856277466\n",
      "[LOG 20200511-10:18:27] epoch: 0, batch: 2522 train-loss: 1.4285823106765747\n",
      "[LOG 20200511-10:18:28] epoch: 0, batch: 2523 train-loss: 0.7193576097488403\n",
      "[LOG 20200511-10:18:28] epoch: 0, batch: 2524 train-loss: 1.4211161136627197\n",
      "[LOG 20200511-10:18:28] epoch: 0, batch: 2525 train-loss: 1.2877106666564941\n",
      "[LOG 20200511-10:18:28] epoch: 0, batch: 2526 train-loss: 1.8151131868362427\n",
      "[LOG 20200511-10:18:28] epoch: 0, batch: 2527 train-loss: 1.3255313634872437\n",
      "[LOG 20200511-10:18:28] epoch: 0, batch: 2528 train-loss: 1.2392921447753906\n",
      "[LOG 20200511-10:18:28] epoch: 0, batch: 2529 train-loss: 1.0039589405059814\n",
      "[LOG 20200511-10:18:28] epoch: 0, batch: 2530 train-loss: 2.080064535140991\n",
      "[LOG 20200511-10:18:28] epoch: 0, batch: 2531 train-loss: 1.6998486518859863\n",
      "[LOG 20200511-10:18:29] epoch: 0, batch: 2532 train-loss: 1.3184142112731934\n",
      "[LOG 20200511-10:18:29] epoch: 0, batch: 2533 train-loss: 1.5155848264694214\n",
      "[LOG 20200511-10:18:29] epoch: 0, batch: 2534 train-loss: 1.4130094051361084\n",
      "[LOG 20200511-10:18:29] epoch: 0, batch: 2535 train-loss: 2.4488914012908936\n",
      "[LOG 20200511-10:18:29] epoch: 0, batch: 2536 train-loss: 1.244412899017334\n",
      "[LOG 20200511-10:18:29] epoch: 0, batch: 2537 train-loss: 1.5251500606536865\n",
      "[LOG 20200511-10:18:29] epoch: 0, batch: 2538 train-loss: 2.3004634380340576\n",
      "[LOG 20200511-10:18:29] epoch: 0, batch: 2539 train-loss: 1.8572087287902832\n",
      "[LOG 20200511-10:18:30] epoch: 0, batch: 2540 train-loss: 0.9282788038253784\n",
      "[LOG 20200511-10:18:30] epoch: 0, batch: 2541 train-loss: 1.0412505865097046\n",
      "[LOG 20200511-10:18:30] epoch: 0, batch: 2542 train-loss: 1.2875616550445557\n",
      "[LOG 20200511-10:18:30] epoch: 0, batch: 2543 train-loss: 1.0916742086410522\n",
      "[LOG 20200511-10:18:30] epoch: 0, batch: 2544 train-loss: 1.4633753299713135\n",
      "[LOG 20200511-10:18:30] epoch: 0, batch: 2545 train-loss: 1.7434015274047852\n",
      "[LOG 20200511-10:18:30] epoch: 0, batch: 2546 train-loss: 1.5998321771621704\n",
      "[LOG 20200511-10:18:30] epoch: 0, batch: 2547 train-loss: 1.9266055822372437\n",
      "[LOG 20200511-10:18:30] epoch: 0, batch: 2548 train-loss: 1.3566498756408691\n",
      "[LOG 20200511-10:18:31] epoch: 0, batch: 2549 train-loss: 1.5363354682922363\n",
      "[LOG 20200511-10:18:31] epoch: 0, batch: 2550 train-loss: 1.8477439880371094\n",
      "[LOG 20200511-10:18:31] epoch: 0, batch: 2551 train-loss: 3.164417028427124\n",
      "[LOG 20200511-10:18:31] epoch: 0, batch: 2552 train-loss: 2.4361720085144043\n",
      "[LOG 20200511-10:18:31] epoch: 0, batch: 2553 train-loss: 2.2973484992980957\n",
      "[LOG 20200511-10:18:31] epoch: 0, batch: 2554 train-loss: 2.0994086265563965\n",
      "[LOG 20200511-10:18:31] epoch: 0, batch: 2555 train-loss: 1.5081881284713745\n",
      "[LOG 20200511-10:18:31] epoch: 0, batch: 2556 train-loss: 1.7776739597320557\n",
      "[LOG 20200511-10:18:31] epoch: 0, batch: 2557 train-loss: 1.4956562519073486\n",
      "[LOG 20200511-10:18:32] epoch: 0, batch: 2558 train-loss: 1.4363067150115967\n",
      "[LOG 20200511-10:18:32] epoch: 0, batch: 2559 train-loss: 1.9963701963424683\n",
      "[LOG 20200511-10:18:32] epoch: 0, batch: 2560 train-loss: 1.6213442087173462\n",
      "[LOG 20200511-10:18:32] epoch: 0, batch: 2561 train-loss: 1.6667871475219727\n",
      "[LOG 20200511-10:18:32] epoch: 0, batch: 2562 train-loss: 1.3622047901153564\n",
      "[LOG 20200511-10:18:32] epoch: 0, batch: 2563 train-loss: 2.2477641105651855\n",
      "[LOG 20200511-10:18:32] epoch: 0, batch: 2564 train-loss: 1.1335363388061523\n",
      "[LOG 20200511-10:18:32] epoch: 0, batch: 2565 train-loss: 1.7115280628204346\n",
      "[LOG 20200511-10:18:32] epoch: 0, batch: 2566 train-loss: 1.7908607721328735\n",
      "[LOG 20200511-10:18:32] epoch: 0, batch: 2567 train-loss: 1.1491321325302124\n",
      "[LOG 20200511-10:18:33] epoch: 0, batch: 2568 train-loss: 1.6343331336975098\n",
      "[LOG 20200511-10:18:33] epoch: 0, batch: 2569 train-loss: 1.2176578044891357\n",
      "[LOG 20200511-10:18:33] epoch: 0, batch: 2570 train-loss: 1.0877301692962646\n",
      "[LOG 20200511-10:18:33] epoch: 0, batch: 2571 train-loss: 1.4213898181915283\n",
      "[LOG 20200511-10:18:33] epoch: 0, batch: 2572 train-loss: 1.5446832180023193\n",
      "[LOG 20200511-10:18:33] epoch: 0, batch: 2573 train-loss: 2.128267765045166\n",
      "[LOG 20200511-10:18:33] epoch: 0, batch: 2574 train-loss: 1.9863882064819336\n",
      "[LOG 20200511-10:18:33] epoch: 0, batch: 2575 train-loss: 1.348049521446228\n",
      "[LOG 20200511-10:18:33] epoch: 0, batch: 2576 train-loss: 1.3755396604537964\n",
      "[LOG 20200511-10:18:34] epoch: 0, batch: 2577 train-loss: 1.586317777633667\n",
      "[LOG 20200511-10:18:34] epoch: 0, batch: 2578 train-loss: 1.191617488861084\n",
      "[LOG 20200511-10:18:34] epoch: 0, batch: 2579 train-loss: 1.9210067987442017\n",
      "[LOG 20200511-10:18:34] epoch: 0, batch: 2580 train-loss: 2.201836109161377\n",
      "[LOG 20200511-10:18:34] epoch: 0, batch: 2581 train-loss: 1.6929051876068115\n",
      "[LOG 20200511-10:18:34] epoch: 0, batch: 2582 train-loss: 1.4777733087539673\n",
      "[LOG 20200511-10:18:34] epoch: 0, batch: 2583 train-loss: 1.1889244318008423\n",
      "[LOG 20200511-10:18:34] epoch: 0, batch: 2584 train-loss: 2.4722836017608643\n",
      "[LOG 20200511-10:18:35] epoch: 0, batch: 2585 train-loss: 1.513386607170105\n",
      "[LOG 20200511-10:18:35] epoch: 0, batch: 2586 train-loss: 1.3876733779907227\n",
      "[LOG 20200511-10:18:35] epoch: 0, batch: 2587 train-loss: 1.9742622375488281\n",
      "[LOG 20200511-10:18:35] epoch: 0, batch: 2588 train-loss: 2.25064754486084\n",
      "[LOG 20200511-10:18:35] epoch: 0, batch: 2589 train-loss: 1.0869574546813965\n",
      "[LOG 20200511-10:18:35] epoch: 0, batch: 2590 train-loss: 1.4453401565551758\n",
      "[LOG 20200511-10:18:35] epoch: 0, batch: 2591 train-loss: 1.528271198272705\n",
      "[LOG 20200511-10:18:35] epoch: 0, batch: 2592 train-loss: 1.2255712747573853\n",
      "[LOG 20200511-10:18:35] epoch: 0, batch: 2593 train-loss: 1.529218316078186\n",
      "[LOG 20200511-10:18:36] epoch: 0, batch: 2594 train-loss: 0.7400993704795837\n",
      "[LOG 20200511-10:18:36] epoch: 0, batch: 2595 train-loss: 1.3758983612060547\n",
      "[LOG 20200511-10:18:36] epoch: 0, batch: 2596 train-loss: 1.9157198667526245\n",
      "[LOG 20200511-10:18:36] epoch: 0, batch: 2597 train-loss: 1.22273588180542\n",
      "[LOG 20200511-10:18:36] epoch: 0, batch: 2598 train-loss: 1.4756983518600464\n",
      "[LOG 20200511-10:18:36] epoch: 0, batch: 2599 train-loss: 0.9532352685928345\n",
      "[LOG 20200511-10:18:36] epoch: 0, batch: 2600 train-loss: 1.0954853296279907\n",
      "[LOG 20200511-10:18:36] epoch: 0, batch: 2601 train-loss: 0.8877471685409546\n",
      "[LOG 20200511-10:18:36] epoch: 0, batch: 2602 train-loss: 1.7955933809280396\n",
      "[LOG 20200511-10:18:37] epoch: 0, batch: 2603 train-loss: 1.2898536920547485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:18:37] epoch: 0, batch: 2604 train-loss: 1.2637081146240234\n",
      "[LOG 20200511-10:18:37] epoch: 0, batch: 2605 train-loss: 2.6069750785827637\n",
      "[LOG 20200511-10:18:37] epoch: 0, batch: 2606 train-loss: 1.367401361465454\n",
      "[LOG 20200511-10:18:37] epoch: 0, batch: 2607 train-loss: 1.6296586990356445\n",
      "[LOG 20200511-10:18:37] epoch: 0, batch: 2608 train-loss: 1.9235656261444092\n",
      "[LOG 20200511-10:18:37] epoch: 0, batch: 2609 train-loss: 1.2847322225570679\n",
      "[LOG 20200511-10:18:37] epoch: 0, batch: 2610 train-loss: 1.304450273513794\n",
      "[LOG 20200511-10:18:37] epoch: 0, batch: 2611 train-loss: 1.786399483680725\n",
      "[LOG 20200511-10:18:38] epoch: 0, batch: 2612 train-loss: 1.7669458389282227\n",
      "[LOG 20200511-10:18:38] epoch: 0, batch: 2613 train-loss: 1.4291155338287354\n",
      "[LOG 20200511-10:18:38] epoch: 0, batch: 2614 train-loss: 1.2592860460281372\n",
      "[LOG 20200511-10:18:38] epoch: 0, batch: 2615 train-loss: 1.9086744785308838\n",
      "[LOG 20200511-10:18:38] epoch: 0, batch: 2616 train-loss: 1.168128490447998\n",
      "[LOG 20200511-10:18:38] epoch: 0, batch: 2617 train-loss: 2.218992233276367\n",
      "[LOG 20200511-10:18:38] epoch: 0, batch: 2618 train-loss: 1.8031210899353027\n",
      "[LOG 20200511-10:18:38] epoch: 0, batch: 2619 train-loss: 1.5982249975204468\n",
      "[LOG 20200511-10:18:38] epoch: 0, batch: 2620 train-loss: 1.627537727355957\n",
      "[LOG 20200511-10:18:39] epoch: 0, batch: 2621 train-loss: 1.0522480010986328\n",
      "[LOG 20200511-10:18:39] epoch: 0, batch: 2622 train-loss: 1.1314691305160522\n",
      "[LOG 20200511-10:18:39] epoch: 0, batch: 2623 train-loss: 2.2675790786743164\n",
      "[LOG 20200511-10:18:39] epoch: 0, batch: 2624 train-loss: 0.9428144693374634\n",
      "[LOG 20200511-10:18:39] epoch: 0, batch: 2625 train-loss: 1.200441837310791\n",
      "[LOG 20200511-10:18:39] epoch: 0, batch: 2626 train-loss: 1.3264133930206299\n",
      "[LOG 20200511-10:18:39] epoch: 0, batch: 2627 train-loss: 1.9341453313827515\n",
      "[LOG 20200511-10:18:39] epoch: 0, batch: 2628 train-loss: 1.5921032428741455\n",
      "[LOG 20200511-10:18:39] epoch: 0, batch: 2629 train-loss: 2.8022522926330566\n",
      "[LOG 20200511-10:18:40] epoch: 0, batch: 2630 train-loss: 2.1203885078430176\n",
      "[LOG 20200511-10:18:40] epoch: 0, batch: 2631 train-loss: 1.9628536701202393\n",
      "[LOG 20200511-10:18:40] epoch: 0, batch: 2632 train-loss: 1.9120867252349854\n",
      "[LOG 20200511-10:18:40] epoch: 0, batch: 2633 train-loss: 0.8278583288192749\n",
      "[LOG 20200511-10:18:40] epoch: 0, batch: 2634 train-loss: 2.005141258239746\n",
      "[LOG 20200511-10:18:40] epoch: 0, batch: 2635 train-loss: 1.2960124015808105\n",
      "[LOG 20200511-10:18:40] epoch: 0, batch: 2636 train-loss: 2.46531343460083\n",
      "[LOG 20200511-10:18:40] epoch: 0, batch: 2637 train-loss: 2.20209002494812\n",
      "[LOG 20200511-10:18:41] epoch: 0, batch: 2638 train-loss: 1.964848279953003\n",
      "[LOG 20200511-10:18:41] epoch: 0, batch: 2639 train-loss: 1.2613613605499268\n",
      "[LOG 20200511-10:18:41] epoch: 0, batch: 2640 train-loss: 1.4186224937438965\n",
      "[LOG 20200511-10:18:41] epoch: 0, batch: 2641 train-loss: 1.652360200881958\n",
      "[LOG 20200511-10:18:41] epoch: 0, batch: 2642 train-loss: 2.71783447265625\n",
      "[LOG 20200511-10:18:41] epoch: 0, batch: 2643 train-loss: 1.7359615564346313\n",
      "[LOG 20200511-10:18:41] epoch: 0, batch: 2644 train-loss: 2.4458470344543457\n",
      "[LOG 20200511-10:18:41] epoch: 0, batch: 2645 train-loss: 1.1674089431762695\n",
      "[LOG 20200511-10:18:42] epoch: 0, batch: 2646 train-loss: 1.304343581199646\n",
      "[LOG 20200511-10:18:42] epoch: 0, batch: 2647 train-loss: 1.7489136457443237\n",
      "[LOG 20200511-10:18:42] epoch: 0, batch: 2648 train-loss: 2.0094027519226074\n",
      "[LOG 20200511-10:18:42] epoch: 0, batch: 2649 train-loss: 1.8091572523117065\n",
      "[LOG 20200511-10:18:42] epoch: 0, batch: 2650 train-loss: 2.5052361488342285\n",
      "[LOG 20200511-10:18:42] epoch: 0, batch: 2651 train-loss: 1.449628233909607\n",
      "[LOG 20200511-10:18:42] epoch: 0, batch: 2652 train-loss: 1.4873671531677246\n",
      "[LOG 20200511-10:18:42] epoch: 0, batch: 2653 train-loss: 1.7128465175628662\n",
      "[LOG 20200511-10:18:43] epoch: 0, batch: 2654 train-loss: 2.030869722366333\n",
      "[LOG 20200511-10:18:43] epoch: 0, batch: 2655 train-loss: 1.4917137622833252\n",
      "[LOG 20200511-10:18:43] epoch: 0, batch: 2656 train-loss: 1.6606582403182983\n",
      "[LOG 20200511-10:18:43] epoch: 0, batch: 2657 train-loss: 2.1286351680755615\n",
      "[LOG 20200511-10:18:43] epoch: 0, batch: 2658 train-loss: 1.1742379665374756\n",
      "[LOG 20200511-10:18:43] epoch: 0, batch: 2659 train-loss: 2.0596346855163574\n",
      "[LOG 20200511-10:18:43] epoch: 0, batch: 2660 train-loss: 0.9447327852249146\n",
      "[LOG 20200511-10:18:43] epoch: 0, batch: 2661 train-loss: 2.587379217147827\n",
      "[LOG 20200511-10:18:44] epoch: 0, batch: 2662 train-loss: 0.9563227891921997\n",
      "[LOG 20200511-10:18:44] epoch: 0, batch: 2663 train-loss: 1.5052201747894287\n",
      "[LOG 20200511-10:18:44] epoch: 0, batch: 2664 train-loss: 1.4400441646575928\n",
      "[LOG 20200511-10:18:44] epoch: 0, batch: 2665 train-loss: 1.1282485723495483\n",
      "[LOG 20200511-10:18:44] epoch: 0, batch: 2666 train-loss: 2.2227602005004883\n",
      "[LOG 20200511-10:18:44] epoch: 0, batch: 2667 train-loss: 1.2651557922363281\n",
      "[LOG 20200511-10:18:44] epoch: 0, batch: 2668 train-loss: 1.9062687158584595\n",
      "[LOG 20200511-10:18:44] epoch: 0, batch: 2669 train-loss: 1.7947568893432617\n",
      "[LOG 20200511-10:18:44] epoch: 0, batch: 2670 train-loss: 1.5899990797042847\n",
      "[LOG 20200511-10:18:45] epoch: 0, batch: 2671 train-loss: 1.452983021736145\n",
      "[LOG 20200511-10:18:45] epoch: 0, batch: 2672 train-loss: 1.5277453660964966\n",
      "[LOG 20200511-10:18:45] epoch: 0, batch: 2673 train-loss: 2.9083902835845947\n",
      "[LOG 20200511-10:18:45] epoch: 0, batch: 2674 train-loss: 1.294266939163208\n",
      "[LOG 20200511-10:18:45] epoch: 0, batch: 2675 train-loss: 0.9296896457672119\n",
      "[LOG 20200511-10:18:45] epoch: 0, batch: 2676 train-loss: 1.2656302452087402\n",
      "[LOG 20200511-10:18:45] epoch: 0, batch: 2677 train-loss: 1.4959311485290527\n",
      "[LOG 20200511-10:18:45] epoch: 0, batch: 2678 train-loss: 1.9134360551834106\n",
      "[LOG 20200511-10:18:45] epoch: 0, batch: 2679 train-loss: 1.7074229717254639\n",
      "[LOG 20200511-10:18:46] epoch: 0, batch: 2680 train-loss: 1.1725770235061646\n",
      "[LOG 20200511-10:18:46] epoch: 0, batch: 2681 train-loss: 0.7294010519981384\n",
      "[LOG 20200511-10:18:46] epoch: 0, batch: 2682 train-loss: 1.86283540725708\n",
      "[LOG 20200511-10:18:46] epoch: 0, batch: 2683 train-loss: 0.9413803815841675\n",
      "[LOG 20200511-10:18:46] epoch: 0, batch: 2684 train-loss: 1.2059675455093384\n",
      "[LOG 20200511-10:18:46] epoch: 0, batch: 2685 train-loss: 1.3231232166290283\n",
      "[LOG 20200511-10:18:46] epoch: 0, batch: 2686 train-loss: 1.5887795686721802\n",
      "[LOG 20200511-10:18:46] epoch: 0, batch: 2687 train-loss: 1.5096237659454346\n",
      "[LOG 20200511-10:18:47] epoch: 0, batch: 2688 train-loss: 0.5375095009803772\n",
      "[LOG 20200511-10:18:47] epoch: 0, batch: 2689 train-loss: 2.427816152572632\n",
      "[LOG 20200511-10:18:47] epoch: 0, batch: 2690 train-loss: 1.5979559421539307\n",
      "[LOG 20200511-10:18:47] epoch: 0, batch: 2691 train-loss: 1.7341783046722412\n",
      "[LOG 20200511-10:18:47] epoch: 0, batch: 2692 train-loss: 1.4614495038986206\n",
      "[LOG 20200511-10:18:47] epoch: 0, batch: 2693 train-loss: 1.4362244606018066\n",
      "[LOG 20200511-10:18:47] epoch: 0, batch: 2694 train-loss: 1.859349012374878\n",
      "[LOG 20200511-10:18:47] epoch: 0, batch: 2695 train-loss: 2.144505500793457\n",
      "[LOG 20200511-10:18:48] epoch: 0, batch: 2696 train-loss: 2.313180923461914\n",
      "[LOG 20200511-10:18:48] epoch: 0, batch: 2697 train-loss: 1.2077522277832031\n",
      "[LOG 20200511-10:18:48] epoch: 0, batch: 2698 train-loss: 1.3195714950561523\n",
      "[LOG 20200511-10:18:48] epoch: 0, batch: 2699 train-loss: 1.3266066312789917\n",
      "[LOG 20200511-10:18:48] epoch: 0, batch: 2700 train-loss: 2.007089138031006\n",
      "[LOG 20200511-10:18:48] epoch: 0, batch: 2701 train-loss: 2.252774953842163\n",
      "[LOG 20200511-10:18:48] epoch: 0, batch: 2702 train-loss: 1.2560616731643677\n",
      "[LOG 20200511-10:18:48] epoch: 0, batch: 2703 train-loss: 0.9624805450439453\n",
      "[LOG 20200511-10:18:49] epoch: 0, batch: 2704 train-loss: 1.6282132863998413\n",
      "[LOG 20200511-10:18:49] epoch: 0, batch: 2705 train-loss: 2.3889718055725098\n",
      "[LOG 20200511-10:18:49] epoch: 0, batch: 2706 train-loss: 2.2947330474853516\n",
      "[LOG 20200511-10:18:49] epoch: 0, batch: 2707 train-loss: 1.7413501739501953\n",
      "[LOG 20200511-10:18:49] epoch: 0, batch: 2708 train-loss: 1.2039687633514404\n",
      "[LOG 20200511-10:18:49] epoch: 0, batch: 2709 train-loss: 1.6112780570983887\n",
      "[LOG 20200511-10:18:49] epoch: 0, batch: 2710 train-loss: 2.3347413539886475\n",
      "[LOG 20200511-10:18:49] epoch: 0, batch: 2711 train-loss: 1.4545202255249023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:18:49] epoch: 0, batch: 2712 train-loss: 1.7318994998931885\n",
      "[LOG 20200511-10:18:50] epoch: 0, batch: 2713 train-loss: 1.4020099639892578\n",
      "[LOG 20200511-10:18:50] epoch: 0, batch: 2714 train-loss: 1.9268642663955688\n",
      "[LOG 20200511-10:18:50] epoch: 0, batch: 2715 train-loss: 2.9548919200897217\n",
      "[LOG 20200511-10:18:50] epoch: 0, batch: 2716 train-loss: 1.0660433769226074\n",
      "[LOG 20200511-10:18:50] epoch: 0, batch: 2717 train-loss: 1.816584825515747\n",
      "[LOG 20200511-10:18:50] epoch: 0, batch: 2718 train-loss: 1.9128257036209106\n",
      "[LOG 20200511-10:18:50] epoch: 0, batch: 2719 train-loss: 1.4649779796600342\n",
      "[LOG 20200511-10:18:50] epoch: 0, batch: 2720 train-loss: 1.423110008239746\n",
      "[LOG 20200511-10:18:51] epoch: 0, batch: 2721 train-loss: 2.087632417678833\n",
      "[LOG 20200511-10:18:51] epoch: 0, batch: 2722 train-loss: 1.5824518203735352\n",
      "[LOG 20200511-10:18:51] epoch: 0, batch: 2723 train-loss: 1.5671530961990356\n",
      "[LOG 20200511-10:18:51] epoch: 0, batch: 2724 train-loss: 1.081619381904602\n",
      "[LOG 20200511-10:18:51] epoch: 0, batch: 2725 train-loss: 1.6633540391921997\n",
      "[LOG 20200511-10:18:51] epoch: 0, batch: 2726 train-loss: 2.0488953590393066\n",
      "[LOG 20200511-10:18:51] epoch: 0, batch: 2727 train-loss: 1.8440109491348267\n",
      "[LOG 20200511-10:18:51] epoch: 0, batch: 2728 train-loss: 1.403065800666809\n",
      "[LOG 20200511-10:18:52] epoch: 0, batch: 2729 train-loss: 1.1631056070327759\n",
      "[LOG 20200511-10:18:52] epoch: 0, batch: 2730 train-loss: 1.1477116346359253\n",
      "[LOG 20200511-10:18:52] epoch: 0, batch: 2731 train-loss: 2.0873537063598633\n",
      "[LOG 20200511-10:18:52] epoch: 0, batch: 2732 train-loss: 1.3872162103652954\n",
      "[LOG 20200511-10:18:52] epoch: 0, batch: 2733 train-loss: 2.2447550296783447\n",
      "[LOG 20200511-10:18:52] epoch: 0, batch: 2734 train-loss: 0.7134931087493896\n",
      "[LOG 20200511-10:18:52] epoch: 0, batch: 2735 train-loss: 1.4973630905151367\n",
      "[LOG 20200511-10:18:52] epoch: 0, batch: 2736 train-loss: 1.4410001039505005\n",
      "[LOG 20200511-10:18:53] epoch: 0, batch: 2737 train-loss: 1.2191311120986938\n",
      "[LOG 20200511-10:18:53] epoch: 0, batch: 2738 train-loss: 1.6911845207214355\n",
      "[LOG 20200511-10:18:53] epoch: 0, batch: 2739 train-loss: 1.2114585638046265\n",
      "[LOG 20200511-10:18:53] epoch: 0, batch: 2740 train-loss: 1.8856840133666992\n",
      "[LOG 20200511-10:18:53] epoch: 0, batch: 2741 train-loss: 1.310746669769287\n",
      "[LOG 20200511-10:18:53] epoch: 0, batch: 2742 train-loss: 1.1696255207061768\n",
      "[LOG 20200511-10:18:53] epoch: 0, batch: 2743 train-loss: 1.5052810907363892\n",
      "[LOG 20200511-10:18:53] epoch: 0, batch: 2744 train-loss: 1.7305593490600586\n",
      "[LOG 20200511-10:18:53] epoch: 0, batch: 2745 train-loss: 1.752258062362671\n",
      "[LOG 20200511-10:18:54] epoch: 0, batch: 2746 train-loss: 2.258052110671997\n",
      "[LOG 20200511-10:18:54] epoch: 0, batch: 2747 train-loss: 1.2792165279388428\n",
      "[LOG 20200511-10:18:54] epoch: 0, batch: 2748 train-loss: 1.6796741485595703\n",
      "[LOG 20200511-10:18:54] epoch: 0, batch: 2749 train-loss: 1.5918357372283936\n",
      "[LOG 20200511-10:18:54] epoch: 0, batch: 2750 train-loss: 1.112136721611023\n",
      "[LOG 20200511-10:18:54] epoch: 0, batch: 2751 train-loss: 1.7248809337615967\n",
      "[LOG 20200511-10:18:54] epoch: 0, batch: 2752 train-loss: 1.6055923700332642\n",
      "[LOG 20200511-10:18:54] epoch: 0, batch: 2753 train-loss: 1.2848949432373047\n",
      "[LOG 20200511-10:18:55] epoch: 0, batch: 2754 train-loss: 1.3319952487945557\n",
      "[LOG 20200511-10:18:55] epoch: 0, batch: 2755 train-loss: 2.7474794387817383\n",
      "[LOG 20200511-10:18:55] epoch: 0, batch: 2756 train-loss: 2.007481336593628\n",
      "[LOG 20200511-10:18:55] epoch: 0, batch: 2757 train-loss: 1.686929702758789\n",
      "[LOG 20200511-10:18:55] epoch: 0, batch: 2758 train-loss: 1.6124186515808105\n",
      "[LOG 20200511-10:18:55] epoch: 0, batch: 2759 train-loss: 1.4245047569274902\n",
      "[LOG 20200511-10:18:55] epoch: 0, batch: 2760 train-loss: 0.872791051864624\n",
      "[LOG 20200511-10:18:55] epoch: 0, batch: 2761 train-loss: 1.7566498517990112\n",
      "[LOG 20200511-10:18:56] epoch: 0, batch: 2762 train-loss: 1.6394538879394531\n",
      "[LOG 20200511-10:18:56] epoch: 0, batch: 2763 train-loss: 1.0905592441558838\n",
      "[LOG 20200511-10:18:56] epoch: 0, batch: 2764 train-loss: 1.0240243673324585\n",
      "[LOG 20200511-10:18:56] epoch: 0, batch: 2765 train-loss: 1.9922981262207031\n",
      "[LOG 20200511-10:18:56] epoch: 0, batch: 2766 train-loss: 1.0211873054504395\n",
      "[LOG 20200511-10:18:56] epoch: 0, batch: 2767 train-loss: 1.1468993425369263\n",
      "[LOG 20200511-10:18:56] epoch: 0, batch: 2768 train-loss: 1.749549388885498\n",
      "[LOG 20200511-10:18:56] epoch: 0, batch: 2769 train-loss: 2.0052361488342285\n",
      "[LOG 20200511-10:18:57] epoch: 0, batch: 2770 train-loss: 1.2711360454559326\n",
      "[LOG 20200511-10:18:57] epoch: 0, batch: 2771 train-loss: 0.8668414354324341\n",
      "[LOG 20200511-10:18:57] epoch: 0, batch: 2772 train-loss: 1.6400612592697144\n",
      "[LOG 20200511-10:18:57] epoch: 0, batch: 2773 train-loss: 1.0648475885391235\n",
      "[LOG 20200511-10:18:57] epoch: 0, batch: 2774 train-loss: 1.7349374294281006\n",
      "[LOG 20200511-10:18:57] epoch: 0, batch: 2775 train-loss: 2.371539831161499\n",
      "[LOG 20200511-10:18:57] epoch: 0, batch: 2776 train-loss: 0.9442300796508789\n",
      "[LOG 20200511-10:18:57] epoch: 0, batch: 2777 train-loss: 3.022127151489258\n",
      "[LOG 20200511-10:18:58] epoch: 0, batch: 2778 train-loss: 1.0395482778549194\n",
      "[LOG 20200511-10:18:58] epoch: 0, batch: 2779 train-loss: 1.5390082597732544\n",
      "[LOG 20200511-10:18:58] epoch: 0, batch: 2780 train-loss: 2.117889881134033\n",
      "[LOG 20200511-10:18:58] epoch: 0, batch: 2781 train-loss: 1.9460437297821045\n",
      "[LOG 20200511-10:18:58] epoch: 0, batch: 2782 train-loss: 1.0195730924606323\n",
      "[LOG 20200511-10:18:58] epoch: 0, batch: 2783 train-loss: 1.4673017263412476\n",
      "[LOG 20200511-10:18:58] epoch: 0, batch: 2784 train-loss: 0.9698138236999512\n",
      "[LOG 20200511-10:18:58] epoch: 0, batch: 2785 train-loss: 1.0936944484710693\n",
      "[LOG 20200511-10:18:59] epoch: 0, batch: 2786 train-loss: 2.823946475982666\n",
      "[LOG 20200511-10:18:59] epoch: 0, batch: 2787 train-loss: 1.1349657773971558\n",
      "[LOG 20200511-10:18:59] epoch: 0, batch: 2788 train-loss: 1.146963119506836\n",
      "[LOG 20200511-10:18:59] epoch: 0, batch: 2789 train-loss: 1.8768285512924194\n",
      "[LOG 20200511-10:18:59] epoch: 0, batch: 2790 train-loss: 1.3935730457305908\n",
      "[LOG 20200511-10:18:59] epoch: 0, batch: 2791 train-loss: 1.7263898849487305\n",
      "[LOG 20200511-10:18:59] epoch: 0, batch: 2792 train-loss: 1.9650524854660034\n",
      "[LOG 20200511-10:18:59] epoch: 0, batch: 2793 train-loss: 0.7752624154090881\n",
      "[LOG 20200511-10:18:59] epoch: 0, batch: 2794 train-loss: 2.0165793895721436\n",
      "[LOG 20200511-10:19:00] epoch: 0, batch: 2795 train-loss: 1.2983300685882568\n",
      "[LOG 20200511-10:19:00] epoch: 0, batch: 2796 train-loss: 1.9310367107391357\n",
      "[LOG 20200511-10:19:00] epoch: 0, batch: 2797 train-loss: 1.0209579467773438\n",
      "[LOG 20200511-10:19:00] epoch: 0, batch: 2798 train-loss: 0.9989389181137085\n",
      "[LOG 20200511-10:19:00] epoch: 0, batch: 2799 train-loss: 1.3353337049484253\n",
      "[LOG 20200511-10:19:00] epoch: 0, batch: 2800 train-loss: 1.4785252809524536\n",
      "[LOG 20200511-10:19:00] epoch: 0, batch: 2801 train-loss: 2.5094046592712402\n",
      "[LOG 20200511-10:19:00] epoch: 0, batch: 2802 train-loss: 1.5998338460922241\n",
      "[LOG 20200511-10:19:01] epoch: 0, batch: 2803 train-loss: 1.407415747642517\n",
      "[LOG 20200511-10:19:01] epoch: 0, batch: 2804 train-loss: 0.8606928586959839\n",
      "[LOG 20200511-10:19:01] epoch: 0, batch: 2805 train-loss: 1.669884204864502\n",
      "[LOG 20200511-10:19:01] epoch: 0, batch: 2806 train-loss: 1.6434762477874756\n",
      "[LOG 20200511-10:19:01] epoch: 0, batch: 2807 train-loss: 1.5994091033935547\n",
      "[LOG 20200511-10:19:01] epoch: 0, batch: 2808 train-loss: 0.8766282200813293\n",
      "[LOG 20200511-10:19:01] epoch: 0, batch: 2809 train-loss: 1.1484278440475464\n",
      "[LOG 20200511-10:19:01] epoch: 0, batch: 2810 train-loss: 1.1552999019622803\n",
      "[LOG 20200511-10:19:02] epoch: 0, batch: 2811 train-loss: 2.3257784843444824\n",
      "[LOG 20200511-10:19:02] epoch: 0, batch: 2812 train-loss: 1.2716736793518066\n",
      "[LOG 20200511-10:19:02] epoch: 0, batch: 2813 train-loss: 1.1310665607452393\n",
      "[LOG 20200511-10:19:02] epoch: 0, batch: 2814 train-loss: 1.1056952476501465\n",
      "[LOG 20200511-10:19:02] epoch: 0, batch: 2815 train-loss: 1.5151175260543823\n",
      "[LOG 20200511-10:19:02] epoch: 0, batch: 2816 train-loss: 1.05087411403656\n",
      "[LOG 20200511-10:19:02] epoch: 0, batch: 2817 train-loss: 1.4032763242721558\n",
      "[LOG 20200511-10:19:02] epoch: 0, batch: 2818 train-loss: 1.541175365447998\n",
      "[LOG 20200511-10:19:02] epoch: 0, batch: 2819 train-loss: 1.8261852264404297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:19:03] epoch: 0, batch: 2820 train-loss: 1.6747713088989258\n",
      "[LOG 20200511-10:19:03] epoch: 0, batch: 2821 train-loss: 0.9212950468063354\n",
      "[LOG 20200511-10:19:03] epoch: 0, batch: 2822 train-loss: 1.0442817211151123\n",
      "[LOG 20200511-10:19:03] epoch: 0, batch: 2823 train-loss: 1.518796443939209\n",
      "[LOG 20200511-10:19:03] epoch: 0, batch: 2824 train-loss: 2.7063403129577637\n",
      "[LOG 20200511-10:19:03] epoch: 0, batch: 2825 train-loss: 2.0239055156707764\n",
      "[LOG 20200511-10:19:03] epoch: 0, batch: 2826 train-loss: 1.2810611724853516\n",
      "[LOG 20200511-10:19:03] epoch: 0, batch: 2827 train-loss: 1.2956724166870117\n",
      "[LOG 20200511-10:19:04] epoch: 0, batch: 2828 train-loss: 0.9468064308166504\n",
      "[LOG 20200511-10:19:04] epoch: 0, batch: 2829 train-loss: 1.32048761844635\n",
      "[LOG 20200511-10:19:04] epoch: 0, batch: 2830 train-loss: 1.3734135627746582\n",
      "[LOG 20200511-10:19:04] epoch: 0, batch: 2831 train-loss: 1.3806102275848389\n",
      "[LOG 20200511-10:19:04] epoch: 0, batch: 2832 train-loss: 1.9230008125305176\n",
      "[LOG 20200511-10:19:04] epoch: 0, batch: 2833 train-loss: 1.44143807888031\n",
      "[LOG 20200511-10:19:04] epoch: 0, batch: 2834 train-loss: 0.9179390072822571\n",
      "[LOG 20200511-10:19:04] epoch: 0, batch: 2835 train-loss: 1.7746318578720093\n",
      "[LOG 20200511-10:19:05] epoch: 0, batch: 2836 train-loss: 0.753929615020752\n",
      "[LOG 20200511-10:19:05] epoch: 0, batch: 2837 train-loss: 1.7863348722457886\n",
      "[LOG 20200511-10:19:05] epoch: 0, batch: 2838 train-loss: 2.1496591567993164\n",
      "[LOG 20200511-10:19:05] epoch: 0, batch: 2839 train-loss: 1.5781198740005493\n",
      "[LOG 20200511-10:19:05] epoch: 0, batch: 2840 train-loss: 1.5908141136169434\n",
      "[LOG 20200511-10:19:05] epoch: 0, batch: 2841 train-loss: 1.4193912744522095\n",
      "[LOG 20200511-10:19:05] epoch: 0, batch: 2842 train-loss: 1.1619075536727905\n",
      "[LOG 20200511-10:19:05] epoch: 0, batch: 2843 train-loss: 1.3725917339324951\n",
      "[LOG 20200511-10:19:05] epoch: 0, batch: 2844 train-loss: 1.3146560192108154\n",
      "[LOG 20200511-10:19:06] epoch: 0, batch: 2845 train-loss: 1.5692315101623535\n",
      "[LOG 20200511-10:19:06] epoch: 0, batch: 2846 train-loss: 0.9883593320846558\n",
      "[LOG 20200511-10:19:06] epoch: 0, batch: 2847 train-loss: 1.2538228034973145\n",
      "[LOG 20200511-10:19:06] epoch: 0, batch: 2848 train-loss: 2.8124568462371826\n",
      "[LOG 20200511-10:19:06] epoch: 0, batch: 2849 train-loss: 1.168480396270752\n",
      "[LOG 20200511-10:19:06] epoch: 0, batch: 2850 train-loss: 1.8000998497009277\n",
      "[LOG 20200511-10:19:06] epoch: 0, batch: 2851 train-loss: 1.4129515886306763\n",
      "[LOG 20200511-10:19:07] epoch: 0, batch: 2852 train-loss: 1.6477352380752563\n",
      "[LOG 20200511-10:19:07] epoch: 0, batch: 2853 train-loss: 1.9561357498168945\n",
      "[LOG 20200511-10:19:07] epoch: 0, batch: 2854 train-loss: 1.1121678352355957\n",
      "[LOG 20200511-10:19:07] epoch: 0, batch: 2855 train-loss: 1.4479650259017944\n",
      "[LOG 20200511-10:19:07] epoch: 0, batch: 2856 train-loss: 1.4317736625671387\n",
      "[LOG 20200511-10:19:07] epoch: 0, batch: 2857 train-loss: 1.6265546083450317\n",
      "[LOG 20200511-10:19:07] epoch: 0, batch: 2858 train-loss: 2.2279372215270996\n",
      "[LOG 20200511-10:19:07] epoch: 0, batch: 2859 train-loss: 2.016592502593994\n",
      "[LOG 20200511-10:19:08] epoch: 0, batch: 2860 train-loss: 2.209027051925659\n",
      "[LOG 20200511-10:19:08] epoch: 0, batch: 2861 train-loss: 1.1159024238586426\n",
      "[LOG 20200511-10:19:08] epoch: 0, batch: 2862 train-loss: 1.5440599918365479\n",
      "[LOG 20200511-10:19:08] epoch: 0, batch: 2863 train-loss: 1.2010467052459717\n",
      "[LOG 20200511-10:19:08] epoch: 0, batch: 2864 train-loss: 2.5843541622161865\n",
      "[LOG 20200511-10:19:08] epoch: 0, batch: 2865 train-loss: 1.213644027709961\n",
      "[LOG 20200511-10:19:08] epoch: 0, batch: 2866 train-loss: 2.505680799484253\n",
      "[LOG 20200511-10:19:08] epoch: 0, batch: 2867 train-loss: 1.7937960624694824\n",
      "[LOG 20200511-10:19:08] epoch: 0, batch: 2868 train-loss: 1.6375517845153809\n",
      "[LOG 20200511-10:19:09] epoch: 0, batch: 2869 train-loss: 2.0899271965026855\n",
      "[LOG 20200511-10:19:09] epoch: 0, batch: 2870 train-loss: 2.5255298614501953\n",
      "[LOG 20200511-10:19:09] epoch: 0, batch: 2871 train-loss: 1.4218146800994873\n",
      "[LOG 20200511-10:19:09] epoch: 0, batch: 2872 train-loss: 1.78389310836792\n",
      "[LOG 20200511-10:19:09] epoch: 0, batch: 2873 train-loss: 2.1997556686401367\n",
      "[LOG 20200511-10:19:09] epoch: 0, batch: 2874 train-loss: 1.482938289642334\n",
      "[LOG 20200511-10:19:09] epoch: 0, batch: 2875 train-loss: 1.0167410373687744\n",
      "[LOG 20200511-10:19:09] epoch: 0, batch: 2876 train-loss: 1.703028678894043\n",
      "[LOG 20200511-10:19:10] epoch: 0, batch: 2877 train-loss: 1.5709319114685059\n",
      "[LOG 20200511-10:19:10] epoch: 0, batch: 2878 train-loss: 1.2072899341583252\n",
      "[LOG 20200511-10:19:10] epoch: 0, batch: 2879 train-loss: 1.7256178855895996\n",
      "[LOG 20200511-10:19:10] epoch: 0, batch: 2880 train-loss: 0.9233452081680298\n",
      "[LOG 20200511-10:19:10] epoch: 0, batch: 2881 train-loss: 2.0487759113311768\n",
      "[LOG 20200511-10:19:10] epoch: 0, batch: 2882 train-loss: 0.9148789048194885\n",
      "[LOG 20200511-10:19:10] epoch: 0, batch: 2883 train-loss: 2.299952745437622\n",
      "[LOG 20200511-10:19:10] epoch: 0, batch: 2884 train-loss: 1.8176689147949219\n",
      "[LOG 20200511-10:19:10] epoch: 0, batch: 2885 train-loss: 2.4096407890319824\n",
      "[LOG 20200511-10:19:11] epoch: 0, batch: 2886 train-loss: 1.3520084619522095\n",
      "[LOG 20200511-10:19:11] epoch: 0, batch: 2887 train-loss: 1.5260822772979736\n",
      "[LOG 20200511-10:19:11] epoch: 0, batch: 2888 train-loss: 2.1409130096435547\n",
      "[LOG 20200511-10:19:11] epoch: 0, batch: 2889 train-loss: 1.592357873916626\n",
      "[LOG 20200511-10:19:11] epoch: 0, batch: 2890 train-loss: 2.023761034011841\n",
      "[LOG 20200511-10:19:11] epoch: 0, batch: 2891 train-loss: 1.2607347965240479\n",
      "[LOG 20200511-10:19:11] epoch: 0, batch: 2892 train-loss: 1.8198480606079102\n",
      "[LOG 20200511-10:19:11] epoch: 0, batch: 2893 train-loss: 2.439676523208618\n",
      "[LOG 20200511-10:19:11] epoch: 0, batch: 2894 train-loss: 1.3627862930297852\n",
      "[LOG 20200511-10:19:12] epoch: 0, batch: 2895 train-loss: 2.2853500843048096\n",
      "[LOG 20200511-10:19:12] epoch: 0, batch: 2896 train-loss: 1.477785587310791\n",
      "[LOG 20200511-10:19:12] epoch: 0, batch: 2897 train-loss: 1.7361106872558594\n",
      "[LOG 20200511-10:19:12] epoch: 0, batch: 2898 train-loss: 0.7224268913269043\n",
      "[LOG 20200511-10:19:12] epoch: 0, batch: 2899 train-loss: 1.4485843181610107\n",
      "[LOG 20200511-10:19:12] epoch: 0, batch: 2900 train-loss: 1.442125916481018\n",
      "[LOG 20200511-10:19:12] epoch: 0, batch: 2901 train-loss: 1.3901078701019287\n",
      "[LOG 20200511-10:19:12] epoch: 0, batch: 2902 train-loss: 0.8337648510932922\n",
      "[LOG 20200511-10:19:12] epoch: 0, batch: 2903 train-loss: 0.8840426206588745\n",
      "[LOG 20200511-10:19:13] epoch: 0, batch: 2904 train-loss: 1.2300941944122314\n",
      "[LOG 20200511-10:19:13] epoch: 0, batch: 2905 train-loss: 1.9691061973571777\n",
      "[LOG 20200511-10:19:13] epoch: 0, batch: 2906 train-loss: 1.38307785987854\n",
      "[LOG 20200511-10:19:13] epoch: 0, batch: 2907 train-loss: 1.3686639070510864\n",
      "[LOG 20200511-10:19:13] epoch: 0, batch: 2908 train-loss: 0.9735976457595825\n",
      "[LOG 20200511-10:19:13] epoch: 0, batch: 2909 train-loss: 1.2567592859268188\n",
      "[LOG 20200511-10:19:13] epoch: 0, batch: 2910 train-loss: 1.0272634029388428\n",
      "[LOG 20200511-10:19:13] epoch: 0, batch: 2911 train-loss: 1.1254585981369019\n",
      "[LOG 20200511-10:19:14] epoch: 0, batch: 2912 train-loss: 2.01731014251709\n",
      "[LOG 20200511-10:19:14] epoch: 0, batch: 2913 train-loss: 1.5651330947875977\n",
      "[LOG 20200511-10:19:14] epoch: 0, batch: 2914 train-loss: 0.9945708513259888\n",
      "[LOG 20200511-10:19:14] epoch: 0, batch: 2915 train-loss: 1.3000209331512451\n",
      "[LOG 20200511-10:19:14] epoch: 0, batch: 2916 train-loss: 1.7436723709106445\n",
      "[LOG 20200511-10:19:14] epoch: 0, batch: 2917 train-loss: 0.9185598492622375\n",
      "[LOG 20200511-10:19:14] epoch: 0, batch: 2918 train-loss: 1.4130052328109741\n",
      "[LOG 20200511-10:19:14] epoch: 0, batch: 2919 train-loss: 2.3696908950805664\n",
      "[LOG 20200511-10:19:15] epoch: 0, batch: 2920 train-loss: 1.7781178951263428\n",
      "[LOG 20200511-10:19:15] epoch: 0, batch: 2921 train-loss: 2.156705141067505\n",
      "[LOG 20200511-10:19:15] epoch: 0, batch: 2922 train-loss: 1.3816986083984375\n",
      "[LOG 20200511-10:19:15] epoch: 0, batch: 2923 train-loss: 1.165600299835205\n",
      "[LOG 20200511-10:19:15] epoch: 0, batch: 2924 train-loss: 1.3037031888961792\n",
      "[LOG 20200511-10:19:15] epoch: 0, batch: 2925 train-loss: 1.7926905155181885\n",
      "[LOG 20200511-10:19:15] epoch: 0, batch: 2926 train-loss: 1.52992582321167\n",
      "[LOG 20200511-10:19:15] epoch: 0, batch: 2927 train-loss: 0.7929732799530029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:19:15] epoch: 0, batch: 2928 train-loss: 0.9335904717445374\n",
      "[LOG 20200511-10:19:16] epoch: 0, batch: 2929 train-loss: 1.647125244140625\n",
      "[LOG 20200511-10:19:16] epoch: 0, batch: 2930 train-loss: 2.023738145828247\n",
      "[LOG 20200511-10:19:16] epoch: 0, batch: 2931 train-loss: 1.2709696292877197\n",
      "[LOG 20200511-10:19:16] epoch: 0, batch: 2932 train-loss: 0.955868124961853\n",
      "[LOG 20200511-10:19:16] epoch: 0, batch: 2933 train-loss: 0.6644101142883301\n",
      "[LOG 20200511-10:19:16] epoch: 0, batch: 2934 train-loss: 0.7395920753479004\n",
      "[LOG 20200511-10:19:16] epoch: 0, batch: 2935 train-loss: 1.631339192390442\n",
      "[LOG 20200511-10:19:16] epoch: 0, batch: 2936 train-loss: 1.0602636337280273\n",
      "[LOG 20200511-10:19:16] epoch: 0, batch: 2937 train-loss: 1.5605533123016357\n",
      "[LOG 20200511-10:19:16] epoch: 0, batch: 2938 train-loss: 1.4179354906082153\n",
      "[LOG 20200511-10:19:17] epoch: 0, batch: 2939 train-loss: 2.0481009483337402\n",
      "[LOG 20200511-10:19:17] epoch: 0, batch: 2940 train-loss: 1.8422547578811646\n",
      "[LOG 20200511-10:19:17] epoch: 0, batch: 2941 train-loss: 1.6584031581878662\n",
      "[LOG 20200511-10:19:17] epoch: 0, batch: 2942 train-loss: 1.614240050315857\n",
      "[LOG 20200511-10:19:17] epoch: 0, batch: 2943 train-loss: 1.9348273277282715\n",
      "[LOG 20200511-10:19:17] epoch: 0, batch: 2944 train-loss: 1.7870697975158691\n",
      "[LOG 20200511-10:19:17] epoch: 0, batch: 2945 train-loss: 1.280902624130249\n",
      "[LOG 20200511-10:19:17] epoch: 0, batch: 2946 train-loss: 1.3639532327651978\n",
      "[LOG 20200511-10:19:17] epoch: 0, batch: 2947 train-loss: 1.969925045967102\n",
      "[LOG 20200511-10:19:18] epoch: 0, batch: 2948 train-loss: 1.1144777536392212\n",
      "[LOG 20200511-10:19:18] epoch: 0, batch: 2949 train-loss: 1.417715072631836\n",
      "[LOG 20200511-10:19:18] epoch: 0, batch: 2950 train-loss: 0.7630549073219299\n",
      "[LOG 20200511-10:19:18] epoch: 0, batch: 2951 train-loss: 1.957130789756775\n",
      "[LOG 20200511-10:19:18] epoch: 0, batch: 2952 train-loss: 1.6186466217041016\n",
      "[LOG 20200511-10:19:18] epoch: 0, batch: 2953 train-loss: 1.4581036567687988\n",
      "[LOG 20200511-10:19:18] epoch: 0, batch: 2954 train-loss: 2.0745327472686768\n",
      "[LOG 20200511-10:19:18] epoch: 0, batch: 2955 train-loss: 1.7416809797286987\n",
      "[LOG 20200511-10:19:18] epoch: 0, batch: 2956 train-loss: 1.167479395866394\n",
      "[LOG 20200511-10:19:19] epoch: 0, batch: 2957 train-loss: 1.7502659559249878\n",
      "[LOG 20200511-10:19:19] epoch: 0, batch: 2958 train-loss: 2.2389020919799805\n",
      "[LOG 20200511-10:19:19] epoch: 0, batch: 2959 train-loss: 0.8818450570106506\n",
      "[LOG 20200511-10:19:19] epoch: 0, batch: 2960 train-loss: 1.2520338296890259\n",
      "[LOG 20200511-10:19:19] epoch: 0, batch: 2961 train-loss: 0.9958505630493164\n",
      "[LOG 20200511-10:19:19] epoch: 0, batch: 2962 train-loss: 1.7460081577301025\n",
      "[LOG 20200511-10:19:19] epoch: 0, batch: 2963 train-loss: 1.8134150505065918\n",
      "[LOG 20200511-10:19:19] epoch: 0, batch: 2964 train-loss: 2.033637285232544\n",
      "[LOG 20200511-10:19:19] epoch: 0, batch: 2965 train-loss: 1.1716917753219604\n",
      "[LOG 20200511-10:19:20] epoch: 0, batch: 2966 train-loss: 1.349621295928955\n",
      "[LOG 20200511-10:19:20] epoch: 0, batch: 2967 train-loss: 1.9500048160552979\n",
      "[LOG 20200511-10:19:20] epoch: 0, batch: 2968 train-loss: 1.1601812839508057\n",
      "[LOG 20200511-10:19:20] epoch: 0, batch: 2969 train-loss: 1.3655688762664795\n",
      "[LOG 20200511-10:19:20] epoch: 0, batch: 2970 train-loss: 1.69645357131958\n",
      "[LOG 20200511-10:19:20] epoch: 0, batch: 2971 train-loss: 2.28930401802063\n",
      "[LOG 20200511-10:19:20] epoch: 0, batch: 2972 train-loss: 1.1322298049926758\n",
      "[LOG 20200511-10:19:20] epoch: 0, batch: 2973 train-loss: 2.4328460693359375\n",
      "[LOG 20200511-10:19:21] epoch: 0, batch: 2974 train-loss: 1.8106814622879028\n",
      "[LOG 20200511-10:19:21] epoch: 0, batch: 2975 train-loss: 2.0974700450897217\n",
      "[LOG 20200511-10:19:21] epoch: 0, batch: 2976 train-loss: 1.6534883975982666\n",
      "[LOG 20200511-10:19:21] epoch: 0, batch: 2977 train-loss: 1.7605682611465454\n",
      "[LOG 20200511-10:19:21] epoch: 0, batch: 2978 train-loss: 2.4603145122528076\n",
      "[LOG 20200511-10:19:21] epoch: 0, batch: 2979 train-loss: 2.5595405101776123\n",
      "[LOG 20200511-10:19:21] epoch: 0, batch: 2980 train-loss: 1.5117436647415161\n",
      "[LOG 20200511-10:19:21] epoch: 0, batch: 2981 train-loss: 1.086721658706665\n",
      "[LOG 20200511-10:19:21] epoch: 0, batch: 2982 train-loss: 2.253795623779297\n",
      "[LOG 20200511-10:19:22] epoch: 0, batch: 2983 train-loss: 1.3343709707260132\n",
      "[LOG 20200511-10:19:22] epoch: 0, batch: 2984 train-loss: 1.908600091934204\n",
      "[LOG 20200511-10:19:22] epoch: 0, batch: 2985 train-loss: 0.9680929780006409\n",
      "[LOG 20200511-10:19:22] epoch: 0, batch: 2986 train-loss: 1.0147160291671753\n",
      "[LOG 20200511-10:19:22] epoch: 0, batch: 2987 train-loss: 1.0427242517471313\n",
      "[LOG 20200511-10:19:22] epoch: 0, batch: 2988 train-loss: 1.6016103029251099\n",
      "[LOG 20200511-10:19:22] epoch: 0, batch: 2989 train-loss: 1.274914026260376\n",
      "[LOG 20200511-10:19:22] epoch: 0, batch: 2990 train-loss: 1.247717022895813\n",
      "[LOG 20200511-10:19:23] epoch: 0, batch: 2991 train-loss: 1.430677890777588\n",
      "[LOG 20200511-10:19:23] epoch: 0, batch: 2992 train-loss: 0.5804674029350281\n",
      "[LOG 20200511-10:19:23] epoch: 0, batch: 2993 train-loss: 1.446802020072937\n",
      "[LOG 20200511-10:19:23] epoch: 0, batch: 2994 train-loss: 0.8298081159591675\n",
      "[LOG 20200511-10:19:23] epoch: 0, batch: 2995 train-loss: 1.077944040298462\n",
      "[LOG 20200511-10:19:23] epoch: 0, batch: 2996 train-loss: 2.2993690967559814\n",
      "[LOG 20200511-10:19:23] epoch: 0, batch: 2997 train-loss: 2.420220136642456\n",
      "[LOG 20200511-10:19:23] epoch: 0, batch: 2998 train-loss: 2.2448368072509766\n",
      "[LOG 20200511-10:19:23] epoch: 0, batch: 2999 train-loss: 0.7378858923912048\n",
      "[LOG 20200511-10:19:24] epoch: 0, batch: 3000 train-loss: 1.3314361572265625\n",
      "[LOG 20200511-10:19:24] epoch: 0, batch: 3001 train-loss: 1.251913070678711\n",
      "[LOG 20200511-10:19:24] epoch: 0, batch: 3002 train-loss: 1.2613613605499268\n",
      "[LOG 20200511-10:19:24] epoch: 0, batch: 3003 train-loss: 1.2907911539077759\n",
      "[LOG 20200511-10:19:24] epoch: 0, batch: 3004 train-loss: 3.0376551151275635\n",
      "[LOG 20200511-10:19:24] epoch: 0, batch: 3005 train-loss: 2.57255482673645\n",
      "[LOG 20200511-10:19:24] epoch: 0, batch: 3006 train-loss: 2.8886609077453613\n",
      "[LOG 20200511-10:19:24] epoch: 0, batch: 3007 train-loss: 2.049441337585449\n",
      "[LOG 20200511-10:19:24] epoch: 0, batch: 3008 train-loss: 1.166837215423584\n",
      "[LOG 20200511-10:19:25] epoch: 0, batch: 3009 train-loss: 1.0078508853912354\n",
      "[LOG 20200511-10:19:25] epoch: 0, batch: 3010 train-loss: 1.260823369026184\n",
      "[LOG 20200511-10:19:25] epoch: 0, batch: 3011 train-loss: 1.323010802268982\n",
      "[LOG 20200511-10:19:25] epoch: 0, batch: 3012 train-loss: 0.7733880877494812\n",
      "[LOG 20200511-10:19:25] epoch: 0, batch: 3013 train-loss: 1.3424099683761597\n",
      "[LOG 20200511-10:19:25] epoch: 0, batch: 3014 train-loss: 1.737313151359558\n",
      "[LOG 20200511-10:19:25] epoch: 0, batch: 3015 train-loss: 1.0474662780761719\n",
      "[LOG 20200511-10:19:25] epoch: 0, batch: 3016 train-loss: 2.45864200592041\n",
      "[LOG 20200511-10:19:26] epoch: 0, batch: 3017 train-loss: 1.4580411911010742\n",
      "[LOG 20200511-10:19:26] epoch: 0, batch: 3018 train-loss: 1.4445595741271973\n",
      "[LOG 20200511-10:19:26] epoch: 0, batch: 3019 train-loss: 1.1003749370574951\n",
      "[LOG 20200511-10:19:26] epoch: 0, batch: 3020 train-loss: 2.015625\n",
      "[LOG 20200511-10:19:26] epoch: 0, batch: 3021 train-loss: 2.1626265048980713\n",
      "[LOG 20200511-10:19:26] epoch: 0, batch: 3022 train-loss: 2.0331339836120605\n",
      "[LOG 20200511-10:19:26] epoch: 0, batch: 3023 train-loss: 0.9508503675460815\n",
      "[LOG 20200511-10:19:26] epoch: 0, batch: 3024 train-loss: 1.6185743808746338\n",
      "[LOG 20200511-10:19:27] epoch: 0, batch: 3025 train-loss: 1.4086925983428955\n",
      "[LOG 20200511-10:19:27] epoch: 0, batch: 3026 train-loss: 1.0342330932617188\n",
      "[LOG 20200511-10:19:27] epoch: 0, batch: 3027 train-loss: 1.4878422021865845\n",
      "[LOG 20200511-10:19:27] epoch: 0, batch: 3028 train-loss: 2.078975200653076\n",
      "[LOG 20200511-10:19:27] epoch: 0, batch: 3029 train-loss: 1.19810950756073\n",
      "[LOG 20200511-10:19:27] epoch: 0, batch: 3030 train-loss: 1.4198384284973145\n",
      "[LOG 20200511-10:19:27] epoch: 0, batch: 3031 train-loss: 1.5578739643096924\n",
      "[LOG 20200511-10:19:27] epoch: 0, batch: 3032 train-loss: 2.234585762023926\n",
      "[LOG 20200511-10:19:27] epoch: 0, batch: 3033 train-loss: 1.88970148563385\n",
      "[LOG 20200511-10:19:28] epoch: 0, batch: 3034 train-loss: 1.5635823011398315\n",
      "[LOG 20200511-10:19:28] epoch: 0, batch: 3035 train-loss: 0.6303958892822266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:19:28] epoch: 0, batch: 3036 train-loss: 2.0270557403564453\n",
      "[LOG 20200511-10:19:28] epoch: 0, batch: 3037 train-loss: 1.0595425367355347\n",
      "[LOG 20200511-10:19:28] epoch: 0, batch: 3038 train-loss: 0.6862509250640869\n",
      "[LOG 20200511-10:19:28] epoch: 0, batch: 3039 train-loss: 1.6423734426498413\n",
      "[LOG 20200511-10:19:28] epoch: 0, batch: 3040 train-loss: 2.0996296405792236\n",
      "[LOG 20200511-10:19:28] epoch: 0, batch: 3041 train-loss: 1.8236347436904907\n",
      "[LOG 20200511-10:19:28] epoch: 0, batch: 3042 train-loss: 2.100250482559204\n",
      "[LOG 20200511-10:19:29] epoch: 0, batch: 3043 train-loss: 0.9187421798706055\n",
      "[LOG 20200511-10:19:29] epoch: 0, batch: 3044 train-loss: 1.5877768993377686\n",
      "[LOG 20200511-10:19:29] epoch: 0, batch: 3045 train-loss: 1.3627171516418457\n",
      "[LOG 20200511-10:19:29] epoch: 0, batch: 3046 train-loss: 0.6481527090072632\n",
      "[LOG 20200511-10:19:29] epoch: 0, batch: 3047 train-loss: 2.7505064010620117\n",
      "[LOG 20200511-10:19:29] epoch: 0, batch: 3048 train-loss: 2.38844633102417\n",
      "[LOG 20200511-10:19:29] epoch: 0, batch: 3049 train-loss: 1.1480038166046143\n",
      "[LOG 20200511-10:19:29] epoch: 0, batch: 3050 train-loss: 1.8665703535079956\n",
      "[LOG 20200511-10:19:30] epoch: 0, batch: 3051 train-loss: 1.173804521560669\n",
      "[LOG 20200511-10:19:30] epoch: 0, batch: 3052 train-loss: 0.8121263980865479\n",
      "[LOG 20200511-10:19:30] epoch: 0, batch: 3053 train-loss: 1.9871779680252075\n",
      "[LOG 20200511-10:19:30] epoch: 0, batch: 3054 train-loss: 1.9072670936584473\n",
      "[LOG 20200511-10:19:30] epoch: 0, batch: 3055 train-loss: 1.3685791492462158\n",
      "[LOG 20200511-10:19:30] epoch: 0, batch: 3056 train-loss: 1.8406633138656616\n",
      "[LOG 20200511-10:19:30] epoch: 0, batch: 3057 train-loss: 1.214125156402588\n",
      "[LOG 20200511-10:19:30] epoch: 0, batch: 3058 train-loss: 0.838575005531311\n",
      "[LOG 20200511-10:19:30] epoch: 0, batch: 3059 train-loss: 1.6824533939361572\n",
      "[LOG 20200511-10:19:30] epoch: 0, batch: 3060 train-loss: 1.532497763633728\n",
      "[LOG 20200511-10:19:31] epoch: 0, batch: 3061 train-loss: 1.6256729364395142\n",
      "[LOG 20200511-10:19:31] epoch: 0, batch: 3062 train-loss: 1.4411954879760742\n",
      "[LOG 20200511-10:19:31] epoch: 0, batch: 3063 train-loss: 2.363492012023926\n",
      "[LOG 20200511-10:19:31] epoch: 0, batch: 3064 train-loss: 0.8686738014221191\n",
      "[LOG 20200511-10:19:31] epoch: 0, batch: 3065 train-loss: 1.1027143001556396\n",
      "[LOG 20200511-10:19:31] epoch: 0, batch: 3066 train-loss: 1.5467132329940796\n",
      "[LOG 20200511-10:19:31] epoch: 0, batch: 3067 train-loss: 1.0853121280670166\n",
      "[LOG 20200511-10:19:31] epoch: 0, batch: 3068 train-loss: 1.357125163078308\n",
      "[LOG 20200511-10:19:31] epoch: 0, batch: 3069 train-loss: 1.006385087966919\n",
      "[LOG 20200511-10:19:32] epoch: 0, batch: 3070 train-loss: 0.8596528172492981\n",
      "[LOG 20200511-10:19:32] epoch: 0, batch: 3071 train-loss: 1.4265249967575073\n",
      "[LOG 20200511-10:19:32] epoch: 0, batch: 3072 train-loss: 1.4835360050201416\n",
      "[LOG 20200511-10:19:32] epoch: 0, batch: 3073 train-loss: 1.2411844730377197\n",
      "[LOG 20200511-10:19:32] epoch: 0, batch: 3074 train-loss: 2.5496203899383545\n",
      "[LOG 20200511-10:19:32] epoch: 0, batch: 3075 train-loss: 2.4511940479278564\n",
      "[LOG 20200511-10:19:32] epoch: 0, batch: 3076 train-loss: 2.1977412700653076\n",
      "[LOG 20200511-10:19:32] epoch: 0, batch: 3077 train-loss: 2.188634157180786\n",
      "[LOG 20200511-10:19:32] epoch: 0, batch: 3078 train-loss: 0.8075248599052429\n",
      "[LOG 20200511-10:19:33] epoch: 0, batch: 3079 train-loss: 0.9412325024604797\n",
      "[LOG 20200511-10:19:33] epoch: 0, batch: 3080 train-loss: 2.5803093910217285\n",
      "[LOG 20200511-10:19:33] epoch: 0, batch: 3081 train-loss: 0.9414076805114746\n",
      "[LOG 20200511-10:19:33] epoch: 0, batch: 3082 train-loss: 1.3008610010147095\n",
      "[LOG 20200511-10:19:33] epoch: 0, batch: 3083 train-loss: 1.31438410282135\n",
      "[LOG 20200511-10:19:33] epoch: 0, batch: 3084 train-loss: 1.8009543418884277\n",
      "[LOG 20200511-10:19:33] epoch: 0, batch: 3085 train-loss: 1.5073856115341187\n",
      "[LOG 20200511-10:19:33] epoch: 0, batch: 3086 train-loss: 0.7784522771835327\n",
      "[LOG 20200511-10:19:33] epoch: 0, batch: 3087 train-loss: 0.8425678014755249\n",
      "[LOG 20200511-10:19:34] epoch: 0, batch: 3088 train-loss: 1.3332011699676514\n",
      "[LOG 20200511-10:19:34] epoch: 0, batch: 3089 train-loss: 1.6317501068115234\n",
      "[LOG 20200511-10:19:34] epoch: 0, batch: 3090 train-loss: 1.5863515138626099\n",
      "[LOG 20200511-10:19:34] epoch: 0, batch: 3091 train-loss: 1.011950969696045\n",
      "[LOG 20200511-10:19:34] epoch: 0, batch: 3092 train-loss: 0.46907564997673035\n",
      "[LOG 20200511-10:19:34] epoch: 0, batch: 3093 train-loss: 1.2180495262145996\n",
      "[LOG 20200511-10:19:34] epoch: 0, batch: 3094 train-loss: 1.802314043045044\n",
      "[LOG 20200511-10:19:34] epoch: 0, batch: 3095 train-loss: 1.5949188470840454\n",
      "[LOG 20200511-10:19:34] epoch: 0, batch: 3096 train-loss: 1.30930495262146\n",
      "[LOG 20200511-10:19:35] epoch: 0, batch: 3097 train-loss: 0.5203518271446228\n",
      "[LOG 20200511-10:19:35] epoch: 0, batch: 3098 train-loss: 0.7533234357833862\n",
      "[LOG 20200511-10:19:35] epoch: 0, batch: 3099 train-loss: 1.0168465375900269\n",
      "[LOG 20200511-10:19:35] epoch: 0, batch: 3100 train-loss: 1.7627049684524536\n",
      "[LOG 20200511-10:19:35] epoch: 0, batch: 3101 train-loss: 2.5690064430236816\n",
      "[LOG 20200511-10:19:35] epoch: 0, batch: 3102 train-loss: 0.8498325347900391\n",
      "[LOG 20200511-10:19:35] epoch: 0, batch: 3103 train-loss: 0.919613242149353\n",
      "[LOG 20200511-10:19:35] epoch: 0, batch: 3104 train-loss: 1.5288679599761963\n",
      "[LOG 20200511-10:19:35] epoch: 0, batch: 3105 train-loss: 1.954848289489746\n",
      "[LOG 20200511-10:19:36] epoch: 0, batch: 3106 train-loss: 2.332404613494873\n",
      "[LOG 20200511-10:19:36] epoch: 0, batch: 3107 train-loss: 2.0285990238189697\n",
      "[LOG 20200511-10:19:36] epoch: 0, batch: 3108 train-loss: 1.0714304447174072\n",
      "[LOG 20200511-10:19:36] epoch: 0, batch: 3109 train-loss: 1.6125301122665405\n",
      "[LOG 20200511-10:19:36] epoch: 0, batch: 3110 train-loss: 1.4795504808425903\n",
      "[LOG 20200511-10:19:36] epoch: 0, batch: 3111 train-loss: 1.0665347576141357\n",
      "[LOG 20200511-10:19:36] epoch: 0, batch: 3112 train-loss: 1.8114013671875\n",
      "[LOG 20200511-10:19:36] epoch: 0, batch: 3113 train-loss: 2.7872121334075928\n",
      "[LOG 20200511-10:19:36] epoch: 0, batch: 3114 train-loss: 1.468139886856079\n",
      "[LOG 20200511-10:19:36] epoch: 0, batch: 3115 train-loss: 1.6902345418930054\n",
      "[LOG 20200511-10:19:37] epoch: 0, batch: 3116 train-loss: 1.2997232675552368\n",
      "[LOG 20200511-10:19:37] epoch: 0, batch: 3117 train-loss: 1.2565230131149292\n",
      "[LOG 20200511-10:19:37] epoch: 0, batch: 3118 train-loss: 1.3519275188446045\n",
      "[LOG 20200511-10:19:37] epoch: 0, batch: 3119 train-loss: 2.0997395515441895\n",
      "[LOG 20200511-10:19:37] epoch: 0, batch: 3120 train-loss: 1.5135904550552368\n",
      "[LOG 20200511-10:19:37] epoch: 0, batch: 3121 train-loss: 2.7478532791137695\n",
      "[LOG 20200511-10:19:37] epoch: 0, batch: 3122 train-loss: 1.2328205108642578\n",
      "[LOG 20200511-10:19:37] epoch: 0, batch: 3123 train-loss: 2.045295476913452\n",
      "[LOG 20200511-10:19:37] epoch: 0, batch: 3124 train-loss: 1.5092978477478027\n",
      "[LOG 20200511-10:19:38] epoch: 0, batch: 3125 train-loss: 1.4065579175949097\n",
      "[LOG 20200511-10:19:38] epoch: 0, batch: 3126 train-loss: 0.6516368985176086\n",
      "[LOG 20200511-10:19:38] epoch: 0, batch: 3127 train-loss: 1.4431092739105225\n",
      "[LOG 20200511-10:19:38] epoch: 0, batch: 3128 train-loss: 1.6266748905181885\n",
      "[LOG 20200511-10:19:38] epoch: 0, batch: 3129 train-loss: 1.439488172531128\n",
      "[LOG 20200511-10:19:38] epoch: 0, batch: 3130 train-loss: 1.1235120296478271\n",
      "[LOG 20200511-10:19:38] epoch: 0, batch: 3131 train-loss: 1.5514638423919678\n",
      "[LOG 20200511-10:19:38] epoch: 0, batch: 3132 train-loss: 1.3924322128295898\n",
      "[LOG 20200511-10:19:38] epoch: 0, batch: 3133 train-loss: 1.3313305377960205\n",
      "[LOG 20200511-10:19:39] epoch: 0, batch: 3134 train-loss: 2.2538628578186035\n",
      "[LOG 20200511-10:19:39] epoch: 0, batch: 3135 train-loss: 0.823174774646759\n",
      "[LOG 20200511-10:19:39] epoch: 0, batch: 3136 train-loss: 1.799268364906311\n",
      "[LOG 20200511-10:19:39] epoch: 0, batch: 3137 train-loss: 1.589511513710022\n",
      "[LOG 20200511-10:19:39] epoch: 0, batch: 3138 train-loss: 2.029947519302368\n",
      "[LOG 20200511-10:19:39] epoch: 0, batch: 3139 train-loss: 1.8690791130065918\n",
      "[LOG 20200511-10:19:39] epoch: 0, batch: 3140 train-loss: 1.8689011335372925\n",
      "[LOG 20200511-10:19:39] epoch: 0, batch: 3141 train-loss: 1.6015291213989258\n",
      "[LOG 20200511-10:19:39] epoch: 0, batch: 3142 train-loss: 1.2424899339675903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:19:40] epoch: 0, batch: 3143 train-loss: 0.42555180191993713\n",
      "[LOG 20200511-10:19:40] epoch: 0, batch: 3144 train-loss: 0.9749017357826233\n",
      "[LOG 20200511-10:19:40] epoch: 0, batch: 3145 train-loss: 1.4157105684280396\n",
      "[LOG 20200511-10:19:40] epoch: 0, batch: 3146 train-loss: 1.3161101341247559\n",
      "[LOG 20200511-10:19:40] epoch: 0, batch: 3147 train-loss: 1.4814954996109009\n",
      "[LOG 20200511-10:19:40] epoch: 0, batch: 3148 train-loss: 1.0076173543930054\n",
      "[LOG 20200511-10:19:40] epoch: 0, batch: 3149 train-loss: 1.4770348072052002\n",
      "[LOG 20200511-10:19:40] epoch: 0, batch: 3150 train-loss: 0.913108229637146\n",
      "[LOG 20200511-10:19:40] epoch: 0, batch: 3151 train-loss: 1.1367696523666382\n",
      "[LOG 20200511-10:19:41] epoch: 0, batch: 3152 train-loss: 1.8601443767547607\n",
      "[LOG 20200511-10:19:41] epoch: 0, batch: 3153 train-loss: 1.103498935699463\n",
      "[LOG 20200511-10:19:41] epoch: 0, batch: 3154 train-loss: 0.29108306765556335\n",
      "[LOG 20200511-10:19:41] epoch: 0, batch: 3155 train-loss: 1.9347074031829834\n",
      "[LOG 20200511-10:19:41] epoch: 0, batch: 3156 train-loss: 2.319653034210205\n",
      "[LOG 20200511-10:19:41] epoch: 0, batch: 3157 train-loss: 0.900897741317749\n",
      "[LOG 20200511-10:19:41] epoch: 0, batch: 3158 train-loss: 2.130051851272583\n",
      "[LOG 20200511-10:19:41] epoch: 0, batch: 3159 train-loss: 1.227131962776184\n",
      "[LOG 20200511-10:19:42] epoch: 0, batch: 3160 train-loss: 1.5978600978851318\n",
      "[LOG 20200511-10:19:42] epoch: 0, batch: 3161 train-loss: 1.4066873788833618\n",
      "[LOG 20200511-10:19:42] epoch: 0, batch: 3162 train-loss: 2.2813827991485596\n",
      "[LOG 20200511-10:19:42] epoch: 0, batch: 3163 train-loss: 1.1216347217559814\n",
      "[LOG 20200511-10:19:42] epoch: 0, batch: 3164 train-loss: 1.2402269840240479\n",
      "[LOG 20200511-10:19:42] epoch: 0, batch: 3165 train-loss: 0.8593083024024963\n",
      "[LOG 20200511-10:19:42] epoch: 0, batch: 3166 train-loss: 1.094504475593567\n",
      "[LOG 20200511-10:19:42] epoch: 0, batch: 3167 train-loss: 1.2643682956695557\n",
      "[LOG 20200511-10:19:43] epoch: 0, batch: 3168 train-loss: 1.483987808227539\n",
      "[LOG 20200511-10:19:43] epoch: 0, batch: 3169 train-loss: 1.376419186592102\n",
      "[LOG 20200511-10:19:43] epoch: 0, batch: 3170 train-loss: 1.2641265392303467\n",
      "[LOG 20200511-10:19:43] epoch: 0, batch: 3171 train-loss: 1.1394381523132324\n",
      "[LOG 20200511-10:19:43] epoch: 0, batch: 3172 train-loss: 1.3369330167770386\n",
      "[LOG 20200511-10:19:43] epoch: 0, batch: 3173 train-loss: 1.6112924814224243\n",
      "[LOG 20200511-10:19:43] epoch: 0, batch: 3174 train-loss: 1.0850862264633179\n",
      "[LOG 20200511-10:19:43] epoch: 0, batch: 3175 train-loss: 1.1642459630966187\n",
      "[LOG 20200511-10:19:43] epoch: 0, batch: 3176 train-loss: 1.4791827201843262\n",
      "[LOG 20200511-10:19:44] epoch: 0, batch: 3177 train-loss: 1.8183696269989014\n",
      "[LOG 20200511-10:19:44] epoch: 0, batch: 3178 train-loss: 0.8356446027755737\n",
      "[LOG 20200511-10:19:44] epoch: 0, batch: 3179 train-loss: 1.308497428894043\n",
      "[LOG 20200511-10:19:44] epoch: 0, batch: 3180 train-loss: 1.8897885084152222\n",
      "[LOG 20200511-10:19:44] epoch: 0, batch: 3181 train-loss: 1.4769173860549927\n",
      "[LOG 20200511-10:19:44] epoch: 0, batch: 3182 train-loss: 1.2120466232299805\n",
      "[LOG 20200511-10:19:44] epoch: 0, batch: 3183 train-loss: 1.6701722145080566\n",
      "[LOG 20200511-10:19:44] epoch: 0, batch: 3184 train-loss: 1.6579132080078125\n",
      "[LOG 20200511-10:19:44] epoch: 0, batch: 3185 train-loss: 2.1854746341705322\n",
      "[LOG 20200511-10:19:45] epoch: 0, batch: 3186 train-loss: 0.6678503751754761\n",
      "[LOG 20200511-10:19:45] epoch: 0, batch: 3187 train-loss: 2.0347747802734375\n",
      "[LOG 20200511-10:19:45] epoch: 0, batch: 3188 train-loss: 1.0236021280288696\n",
      "[LOG 20200511-10:19:45] epoch: 0, batch: 3189 train-loss: 1.0293350219726562\n",
      "[LOG 20200511-10:19:45] epoch: 0, batch: 3190 train-loss: 1.7960550785064697\n",
      "[LOG 20200511-10:19:45] epoch: 0, batch: 3191 train-loss: 1.1042306423187256\n",
      "[LOG 20200511-10:19:45] epoch: 0, batch: 3192 train-loss: 2.3063807487487793\n",
      "[LOG 20200511-10:19:45] epoch: 0, batch: 3193 train-loss: 1.4855306148529053\n",
      "[LOG 20200511-10:19:46] epoch: 0, batch: 3194 train-loss: 1.7347984313964844\n",
      "[LOG 20200511-10:19:46] epoch: 0, batch: 3195 train-loss: 1.0239264965057373\n",
      "[LOG 20200511-10:19:46] epoch: 0, batch: 3196 train-loss: 1.9896419048309326\n",
      "[LOG 20200511-10:19:46] epoch: 0, batch: 3197 train-loss: 1.396440863609314\n",
      "[LOG 20200511-10:19:46] epoch: 0, batch: 3198 train-loss: 2.495673894882202\n",
      "[LOG 20200511-10:19:46] epoch: 0, batch: 3199 train-loss: 1.4363296031951904\n",
      "[LOG 20200511-10:19:46] epoch: 0, batch: 3200 train-loss: 2.704144239425659\n",
      "[LOG 20200511-10:19:46] epoch: 0, batch: 3201 train-loss: 0.4936860501766205\n",
      "[LOG 20200511-10:19:47] epoch: 0, batch: 3202 train-loss: 1.5495920181274414\n",
      "[LOG 20200511-10:19:47] epoch: 0, batch: 3203 train-loss: 1.6535460948944092\n",
      "[LOG 20200511-10:19:47] epoch: 0, batch: 3204 train-loss: 1.1413323879241943\n",
      "[LOG 20200511-10:19:47] epoch: 0, batch: 3205 train-loss: 1.1091458797454834\n",
      "[LOG 20200511-10:19:47] epoch: 0, batch: 3206 train-loss: 0.928209662437439\n",
      "[LOG 20200511-10:19:47] epoch: 0, batch: 3207 train-loss: 1.3142056465148926\n",
      "[LOG 20200511-10:19:47] epoch: 0, batch: 3208 train-loss: 1.593681812286377\n",
      "[LOG 20200511-10:19:47] epoch: 0, batch: 3209 train-loss: 1.69569993019104\n",
      "[LOG 20200511-10:19:47] epoch: 0, batch: 3210 train-loss: 1.8562591075897217\n",
      "[LOG 20200511-10:19:48] epoch: 0, batch: 3211 train-loss: 1.3906127214431763\n",
      "[LOG 20200511-10:19:48] epoch: 0, batch: 3212 train-loss: 2.1504998207092285\n",
      "[LOG 20200511-10:19:48] epoch: 0, batch: 3213 train-loss: 1.4038465023040771\n",
      "[LOG 20200511-10:19:48] epoch: 0, batch: 3214 train-loss: 0.8122603893280029\n",
      "[LOG 20200511-10:19:48] epoch: 0, batch: 3215 train-loss: 1.4456087350845337\n",
      "[LOG 20200511-10:19:48] epoch: 0, batch: 3216 train-loss: 2.275193452835083\n",
      "[LOG 20200511-10:19:48] epoch: 0, batch: 3217 train-loss: 1.4351624250411987\n",
      "[LOG 20200511-10:19:48] epoch: 0, batch: 3218 train-loss: 1.159915804862976\n",
      "[LOG 20200511-10:19:49] epoch: 0, batch: 3219 train-loss: 1.6179873943328857\n",
      "[LOG 20200511-10:19:49] epoch: 0, batch: 3220 train-loss: 0.6155754327774048\n",
      "[LOG 20200511-10:19:49] epoch: 0, batch: 3221 train-loss: 2.0831029415130615\n",
      "[LOG 20200511-10:19:49] epoch: 0, batch: 3222 train-loss: 2.3718671798706055\n",
      "[LOG 20200511-10:19:49] epoch: 0, batch: 3223 train-loss: 1.2932734489440918\n",
      "[LOG 20200511-10:19:49] epoch: 0, batch: 3224 train-loss: 1.1861426830291748\n",
      "[LOG 20200511-10:19:49] epoch: 0, batch: 3225 train-loss: 1.962892770767212\n",
      "[LOG 20200511-10:19:49] epoch: 0, batch: 3226 train-loss: 1.203071117401123\n",
      "[LOG 20200511-10:19:49] epoch: 0, batch: 3227 train-loss: 1.3832674026489258\n",
      "[LOG 20200511-10:19:50] epoch: 0, batch: 3228 train-loss: 0.9111473560333252\n",
      "[LOG 20200511-10:19:50] epoch: 0, batch: 3229 train-loss: 1.1550376415252686\n",
      "[LOG 20200511-10:19:50] epoch: 0, batch: 3230 train-loss: 1.512691617012024\n",
      "[LOG 20200511-10:19:50] epoch: 0, batch: 3231 train-loss: 1.850193977355957\n",
      "[LOG 20200511-10:19:50] epoch: 0, batch: 3232 train-loss: 1.328338384628296\n",
      "[LOG 20200511-10:19:50] epoch: 0, batch: 3233 train-loss: 1.5851013660430908\n",
      "[LOG 20200511-10:19:50] epoch: 0, batch: 3234 train-loss: 1.8800081014633179\n",
      "[LOG 20200511-10:19:50] epoch: 0, batch: 3235 train-loss: 1.2039134502410889\n",
      "[LOG 20200511-10:19:50] epoch: 0, batch: 3236 train-loss: 1.938187837600708\n",
      "[LOG 20200511-10:19:51] epoch: 0, batch: 3237 train-loss: 0.9887040257453918\n",
      "[LOG 20200511-10:19:51] epoch: 0, batch: 3238 train-loss: 2.464608669281006\n",
      "[LOG 20200511-10:19:51] epoch: 0, batch: 3239 train-loss: 1.4417874813079834\n",
      "[LOG 20200511-10:19:51] epoch: 0, batch: 3240 train-loss: 2.1298234462738037\n",
      "[LOG 20200511-10:19:51] epoch: 0, batch: 3241 train-loss: 1.0845155715942383\n",
      "[LOG 20200511-10:19:51] epoch: 0, batch: 3242 train-loss: 1.472728967666626\n",
      "[LOG 20200511-10:19:51] epoch: 0, batch: 3243 train-loss: 1.504446029663086\n",
      "[LOG 20200511-10:19:51] epoch: 0, batch: 3244 train-loss: 1.7458560466766357\n",
      "[LOG 20200511-10:19:51] epoch: 0, batch: 3245 train-loss: 1.6644954681396484\n",
      "[LOG 20200511-10:19:51] epoch: 0, batch: 3246 train-loss: 1.5359692573547363\n",
      "[LOG 20200511-10:19:52] epoch: 0, batch: 3247 train-loss: 1.2084829807281494\n",
      "[LOG 20200511-10:19:52] epoch: 0, batch: 3248 train-loss: 0.9827037453651428\n",
      "[LOG 20200511-10:19:52] epoch: 0, batch: 3249 train-loss: 1.5537415742874146\n",
      "[LOG 20200511-10:19:52] epoch: 0, batch: 3250 train-loss: 1.5604819059371948\n",
      "[LOG 20200511-10:19:52] epoch: 0, batch: 3251 train-loss: 1.1543753147125244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:19:52] epoch: 0, batch: 3252 train-loss: 1.6229467391967773\n",
      "[LOG 20200511-10:19:52] epoch: 0, batch: 3253 train-loss: 2.0955252647399902\n",
      "[LOG 20200511-10:19:52] epoch: 0, batch: 3254 train-loss: 2.722860336303711\n",
      "[LOG 20200511-10:19:52] epoch: 0, batch: 3255 train-loss: 1.0882384777069092\n",
      "[LOG 20200511-10:19:53] epoch: 0, batch: 3256 train-loss: 1.6345876455307007\n",
      "[LOG 20200511-10:19:53] epoch: 0, batch: 3257 train-loss: 0.9252783060073853\n",
      "[LOG 20200511-10:19:53] epoch: 0, batch: 3258 train-loss: 1.3504031896591187\n",
      "[LOG 20200511-10:19:53] epoch: 0, batch: 3259 train-loss: 1.261095404624939\n",
      "[LOG 20200511-10:19:53] epoch: 0, batch: 3260 train-loss: 1.7272498607635498\n",
      "[LOG 20200511-10:19:53] epoch: 0, batch: 3261 train-loss: 2.525167465209961\n",
      "[LOG 20200511-10:19:53] epoch: 0, batch: 3262 train-loss: 1.3121223449707031\n",
      "[LOG 20200511-10:19:53] epoch: 0, batch: 3263 train-loss: 0.6483813524246216\n",
      "[LOG 20200511-10:19:53] epoch: 0, batch: 3264 train-loss: 0.7882158756256104\n",
      "[LOG 20200511-10:19:54] epoch: 0, batch: 3265 train-loss: 1.937312364578247\n",
      "[LOG 20200511-10:19:54] epoch: 0, batch: 3266 train-loss: 2.519498348236084\n",
      "[LOG 20200511-10:19:54] epoch: 0, batch: 3267 train-loss: 1.7864636182785034\n",
      "[LOG 20200511-10:19:54] epoch: 0, batch: 3268 train-loss: 1.7430338859558105\n",
      "[LOG 20200511-10:19:54] epoch: 0, batch: 3269 train-loss: 0.9531482458114624\n",
      "[LOG 20200511-10:19:54] epoch: 0, batch: 3270 train-loss: 1.9765403270721436\n",
      "[LOG 20200511-10:19:54] epoch: 0, batch: 3271 train-loss: 0.9351745843887329\n",
      "[LOG 20200511-10:19:54] epoch: 0, batch: 3272 train-loss: 2.3466830253601074\n",
      "[LOG 20200511-10:19:55] epoch: 0, batch: 3273 train-loss: 2.508578300476074\n",
      "[LOG 20200511-10:19:55] epoch: 0, batch: 3274 train-loss: 1.141352891921997\n",
      "[LOG 20200511-10:19:55] epoch: 0, batch: 3275 train-loss: 0.5126664638519287\n",
      "[LOG 20200511-10:19:55] epoch: 0, batch: 3276 train-loss: 1.3458302021026611\n",
      "[LOG 20200511-10:19:55] epoch: 0, batch: 3277 train-loss: 1.5321849584579468\n",
      "[LOG 20200511-10:19:55] epoch: 0, batch: 3278 train-loss: 0.5365716814994812\n",
      "[LOG 20200511-10:19:55] epoch: 0, batch: 3279 train-loss: 0.9791461229324341\n",
      "[LOG 20200511-10:19:56] epoch: 0, batch: 3280 train-loss: 0.6899557113647461\n",
      "[LOG 20200511-10:19:56] epoch: 0, batch: 3281 train-loss: 1.206052541732788\n",
      "[LOG 20200511-10:19:56] epoch: 0, batch: 3282 train-loss: 1.8793872594833374\n",
      "[LOG 20200511-10:19:56] epoch: 0, batch: 3283 train-loss: 1.758225679397583\n",
      "[LOG 20200511-10:19:56] epoch: 0, batch: 3284 train-loss: 0.9357489347457886\n",
      "[LOG 20200511-10:19:56] epoch: 0, batch: 3285 train-loss: 1.0935651063919067\n",
      "[LOG 20200511-10:19:56] epoch: 0, batch: 3286 train-loss: 1.0475257635116577\n",
      "[LOG 20200511-10:19:57] epoch: 0, batch: 3287 train-loss: 2.2143971920013428\n",
      "[LOG 20200511-10:19:57] epoch: 0, batch: 3288 train-loss: 1.318994164466858\n",
      "[LOG 20200511-10:19:57] epoch: 0, batch: 3289 train-loss: 2.08048152923584\n",
      "[LOG 20200511-10:19:57] epoch: 0, batch: 3290 train-loss: 1.5383937358856201\n",
      "[LOG 20200511-10:19:57] epoch: 0, batch: 3291 train-loss: 1.9205011129379272\n",
      "[LOG 20200511-10:19:57] epoch: 0, batch: 3292 train-loss: 0.9504736065864563\n",
      "[LOG 20200511-10:19:57] epoch: 0, batch: 3293 train-loss: 2.0958406925201416\n",
      "[LOG 20200511-10:19:57] epoch: 0, batch: 3294 train-loss: 2.1209867000579834\n",
      "[LOG 20200511-10:19:58] epoch: 0, batch: 3295 train-loss: 1.9063506126403809\n",
      "[LOG 20200511-10:19:58] epoch: 0, batch: 3296 train-loss: 1.637906789779663\n",
      "[LOG 20200511-10:19:58] epoch: 0, batch: 3297 train-loss: 2.6202430725097656\n",
      "[LOG 20200511-10:19:58] epoch: 0, batch: 3298 train-loss: 1.3172969818115234\n",
      "[LOG 20200511-10:19:58] epoch: 0, batch: 3299 train-loss: 0.6421554088592529\n",
      "[LOG 20200511-10:19:58] epoch: 0, batch: 3300 train-loss: 2.512251853942871\n",
      "[LOG 20200511-10:19:58] epoch: 0, batch: 3301 train-loss: 1.6087489128112793\n",
      "[LOG 20200511-10:19:58] epoch: 0, batch: 3302 train-loss: 1.4902335405349731\n",
      "[LOG 20200511-10:19:59] epoch: 0, batch: 3303 train-loss: 1.316773772239685\n",
      "[LOG 20200511-10:19:59] epoch: 0, batch: 3304 train-loss: 1.549720048904419\n",
      "[LOG 20200511-10:19:59] epoch: 0, batch: 3305 train-loss: 1.8976699113845825\n",
      "[LOG 20200511-10:19:59] epoch: 0, batch: 3306 train-loss: 1.508075475692749\n",
      "[LOG 20200511-10:19:59] epoch: 0, batch: 3307 train-loss: 1.7386091947555542\n",
      "[LOG 20200511-10:19:59] epoch: 0, batch: 3308 train-loss: 1.136830449104309\n",
      "[LOG 20200511-10:19:59] epoch: 0, batch: 3309 train-loss: 1.1752533912658691\n",
      "[LOG 20200511-10:19:59] epoch: 0, batch: 3310 train-loss: 0.9156656265258789\n",
      "[LOG 20200511-10:20:00] epoch: 0, batch: 3311 train-loss: 1.4780651330947876\n",
      "[LOG 20200511-10:20:00] epoch: 0, batch: 3312 train-loss: 1.8783137798309326\n",
      "[LOG 20200511-10:20:00] epoch: 0, batch: 3313 train-loss: 0.8873478174209595\n",
      "[LOG 20200511-10:20:00] epoch: 0, batch: 3314 train-loss: 0.8514050841331482\n",
      "[LOG 20200511-10:20:00] epoch: 0, batch: 3315 train-loss: 0.957740068435669\n",
      "[LOG 20200511-10:20:00] epoch: 0, batch: 3316 train-loss: 1.2868921756744385\n",
      "[LOG 20200511-10:20:00] epoch: 0, batch: 3317 train-loss: 1.2041593790054321\n",
      "[LOG 20200511-10:20:00] epoch: 0, batch: 3318 train-loss: 1.9117847681045532\n",
      "[LOG 20200511-10:20:01] epoch: 0, batch: 3319 train-loss: 1.6997519731521606\n",
      "[LOG 20200511-10:20:01] epoch: 0, batch: 3320 train-loss: 2.1655101776123047\n",
      "[LOG 20200511-10:20:01] epoch: 0, batch: 3321 train-loss: 1.747126817703247\n",
      "[LOG 20200511-10:20:01] epoch: 0, batch: 3322 train-loss: 2.5682497024536133\n",
      "[LOG 20200511-10:20:01] epoch: 0, batch: 3323 train-loss: 1.3213090896606445\n",
      "[LOG 20200511-10:20:01] epoch: 0, batch: 3324 train-loss: 2.9129433631896973\n",
      "[LOG 20200511-10:20:01] epoch: 0, batch: 3325 train-loss: 1.4357632398605347\n",
      "[LOG 20200511-10:20:02] epoch: 0, batch: 3326 train-loss: 1.630000352859497\n",
      "[LOG 20200511-10:20:02] epoch: 0, batch: 3327 train-loss: 1.8323314189910889\n",
      "[LOG 20200511-10:20:02] epoch: 0, batch: 3328 train-loss: 2.151353359222412\n",
      "[LOG 20200511-10:20:02] epoch: 0, batch: 3329 train-loss: 1.059096336364746\n",
      "[LOG 20200511-10:20:02] epoch: 0, batch: 3330 train-loss: 1.9142670631408691\n",
      "[LOG 20200511-10:20:02] epoch: 0, batch: 3331 train-loss: 1.0088019371032715\n",
      "[LOG 20200511-10:20:02] epoch: 0, batch: 3332 train-loss: 2.4619388580322266\n",
      "[LOG 20200511-10:20:02] epoch: 0, batch: 3333 train-loss: 1.8872137069702148\n",
      "[LOG 20200511-10:20:03] epoch: 0, batch: 3334 train-loss: 1.6846102476119995\n",
      "[LOG 20200511-10:20:03] epoch: 0, batch: 3335 train-loss: 1.0296430587768555\n",
      "[LOG 20200511-10:20:03] epoch: 0, batch: 3336 train-loss: 1.0815589427947998\n",
      "[LOG 20200511-10:20:03] epoch: 0, batch: 3337 train-loss: 1.9699881076812744\n",
      "[LOG 20200511-10:20:03] epoch: 0, batch: 3338 train-loss: 1.6374001502990723\n",
      "[LOG 20200511-10:20:03] epoch: 0, batch: 3339 train-loss: 1.2440056800842285\n",
      "[LOG 20200511-10:20:03] epoch: 0, batch: 3340 train-loss: 2.8377294540405273\n",
      "[LOG 20200511-10:20:03] epoch: 0, batch: 3341 train-loss: 1.8312597274780273\n",
      "[LOG 20200511-10:20:03] epoch: 0, batch: 3342 train-loss: 2.028599739074707\n",
      "[LOG 20200511-10:20:04] epoch: 0, batch: 3343 train-loss: 1.6057748794555664\n",
      "[LOG 20200511-10:20:04] epoch: 0, batch: 3344 train-loss: 1.5157647132873535\n",
      "[LOG 20200511-10:20:04] epoch: 0, batch: 3345 train-loss: 1.491415023803711\n",
      "[LOG 20200511-10:20:04] epoch: 0, batch: 3346 train-loss: 1.5853685140609741\n",
      "[LOG 20200511-10:20:04] epoch: 0, batch: 3347 train-loss: 2.0290889739990234\n",
      "[LOG 20200511-10:20:04] epoch: 0, batch: 3348 train-loss: 1.8403576612472534\n",
      "[LOG 20200511-10:20:04] epoch: 0, batch: 3349 train-loss: 1.7674407958984375\n",
      "[LOG 20200511-10:20:04] epoch: 0, batch: 3350 train-loss: 1.315861701965332\n",
      "[LOG 20200511-10:20:05] epoch: 0, batch: 3351 train-loss: 1.0336134433746338\n",
      "[LOG 20200511-10:20:05] epoch: 0, batch: 3352 train-loss: 1.488191843032837\n",
      "[LOG 20200511-10:20:05] epoch: 0, batch: 3353 train-loss: 0.5935029983520508\n",
      "[LOG 20200511-10:20:05] epoch: 0, batch: 3354 train-loss: 1.3177392482757568\n",
      "[LOG 20200511-10:20:05] epoch: 0, batch: 3355 train-loss: 1.5181560516357422\n",
      "[LOG 20200511-10:20:05] epoch: 0, batch: 3356 train-loss: 1.0665476322174072\n",
      "[LOG 20200511-10:20:05] epoch: 0, batch: 3357 train-loss: 1.433119535446167\n",
      "[LOG 20200511-10:20:05] epoch: 0, batch: 3358 train-loss: 1.9790747165679932\n",
      "[LOG 20200511-10:20:06] epoch: 0, batch: 3359 train-loss: 1.1814546585083008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:20:06] epoch: 0, batch: 3360 train-loss: 0.8250045776367188\n",
      "[LOG 20200511-10:20:06] epoch: 0, batch: 3361 train-loss: 1.4454734325408936\n",
      "[LOG 20200511-10:20:06] epoch: 0, batch: 3362 train-loss: 0.7060684561729431\n",
      "[LOG 20200511-10:20:06] epoch: 0, batch: 3363 train-loss: 1.4820115566253662\n",
      "[LOG 20200511-10:20:06] epoch: 0, batch: 3364 train-loss: 1.546743392944336\n",
      "[LOG 20200511-10:20:06] epoch: 0, batch: 3365 train-loss: 1.4423496723175049\n",
      "[LOG 20200511-10:20:06] epoch: 0, batch: 3366 train-loss: 1.0077406167984009\n",
      "[LOG 20200511-10:20:06] epoch: 0, batch: 3367 train-loss: 1.1089601516723633\n",
      "[LOG 20200511-10:20:07] epoch: 0, batch: 3368 train-loss: 0.857621967792511\n",
      "[LOG 20200511-10:20:07] epoch: 0, batch: 3369 train-loss: 1.5459710359573364\n",
      "[LOG 20200511-10:20:07] epoch: 0, batch: 3370 train-loss: 1.9429963827133179\n",
      "[LOG 20200511-10:20:07] epoch: 0, batch: 3371 train-loss: 2.038966178894043\n",
      "[LOG 20200511-10:20:07] epoch: 0, batch: 3372 train-loss: 1.062977910041809\n",
      "[LOG 20200511-10:20:07] epoch: 0, batch: 3373 train-loss: 1.9315685033798218\n",
      "[LOG 20200511-10:20:07] epoch: 0, batch: 3374 train-loss: 0.6907517313957214\n",
      "[LOG 20200511-10:20:07] epoch: 0, batch: 3375 train-loss: 2.589529514312744\n",
      "[LOG 20200511-10:20:08] epoch: 0, batch: 3376 train-loss: 1.9891481399536133\n",
      "[LOG 20200511-10:20:08] epoch: 0, batch: 3377 train-loss: 1.9591468572616577\n",
      "[LOG 20200511-10:20:08] epoch: 0, batch: 3378 train-loss: 1.2083033323287964\n",
      "[LOG 20200511-10:20:08] epoch: 0, batch: 3379 train-loss: 1.1230107545852661\n",
      "[LOG 20200511-10:20:08] epoch: 0, batch: 3380 train-loss: 0.9236794710159302\n",
      "[LOG 20200511-10:20:08] epoch: 0, batch: 3381 train-loss: 1.3962602615356445\n",
      "[LOG 20200511-10:20:08] epoch: 0, batch: 3382 train-loss: 1.313542366027832\n",
      "[LOG 20200511-10:20:08] epoch: 0, batch: 3383 train-loss: 1.0733448266983032\n",
      "[LOG 20200511-10:20:09] epoch: 0, batch: 3384 train-loss: 2.454953193664551\n",
      "[LOG 20200511-10:20:09] epoch: 0, batch: 3385 train-loss: 1.885204553604126\n",
      "[LOG 20200511-10:20:09] epoch: 0, batch: 3386 train-loss: 1.6534773111343384\n",
      "[LOG 20200511-10:20:09] epoch: 0, batch: 3387 train-loss: 1.4242979288101196\n",
      "[LOG 20200511-10:20:09] epoch: 0, batch: 3388 train-loss: 0.8180752992630005\n",
      "[LOG 20200511-10:20:09] epoch: 0, batch: 3389 train-loss: 1.2147810459136963\n",
      "[LOG 20200511-10:20:09] epoch: 0, batch: 3390 train-loss: 1.4230066537857056\n",
      "[LOG 20200511-10:20:09] epoch: 0, batch: 3391 train-loss: 1.8640284538269043\n",
      "[LOG 20200511-10:20:10] epoch: 0, batch: 3392 train-loss: 1.2110745906829834\n",
      "[LOG 20200511-10:20:10] epoch: 0, batch: 3393 train-loss: 1.5044922828674316\n",
      "[LOG 20200511-10:20:10] epoch: 0, batch: 3394 train-loss: 1.7361136674880981\n",
      "[LOG 20200511-10:20:10] epoch: 0, batch: 3395 train-loss: 1.4351950883865356\n",
      "[LOG 20200511-10:20:10] epoch: 0, batch: 3396 train-loss: 1.9650412797927856\n",
      "[LOG 20200511-10:20:10] epoch: 0, batch: 3397 train-loss: 2.268321990966797\n",
      "[LOG 20200511-10:20:10] epoch: 0, batch: 3398 train-loss: 2.093055009841919\n",
      "[LOG 20200511-10:20:10] epoch: 0, batch: 3399 train-loss: 1.8733288049697876\n",
      "[LOG 20200511-10:20:11] epoch: 0, batch: 3400 train-loss: 1.1804225444793701\n",
      "[LOG 20200511-10:20:11] epoch: 0, batch: 3401 train-loss: 1.5574586391448975\n",
      "[LOG 20200511-10:20:11] epoch: 0, batch: 3402 train-loss: 0.8956773281097412\n",
      "[LOG 20200511-10:20:11] epoch: 0, batch: 3403 train-loss: 1.378220796585083\n",
      "[LOG 20200511-10:20:11] epoch: 0, batch: 3404 train-loss: 1.7341585159301758\n",
      "[LOG 20200511-10:20:11] epoch: 0, batch: 3405 train-loss: 1.430347204208374\n",
      "[LOG 20200511-10:20:11] epoch: 0, batch: 3406 train-loss: 1.3672062158584595\n",
      "[LOG 20200511-10:20:12] epoch: 0, batch: 3407 train-loss: 1.2365235090255737\n",
      "[LOG 20200511-10:20:12] epoch: 0, batch: 3408 train-loss: 1.3179945945739746\n",
      "[LOG 20200511-10:20:12] epoch: 0, batch: 3409 train-loss: 2.147632122039795\n",
      "[LOG 20200511-10:20:12] epoch: 0, batch: 3410 train-loss: 1.2493619918823242\n",
      "[LOG 20200511-10:20:12] epoch: 0, batch: 3411 train-loss: 1.7496622800827026\n",
      "[LOG 20200511-10:20:12] epoch: 0, batch: 3412 train-loss: 1.1303142309188843\n",
      "[LOG 20200511-10:20:12] epoch: 0, batch: 3413 train-loss: 1.0533208847045898\n",
      "[LOG 20200511-10:20:12] epoch: 0, batch: 3414 train-loss: 1.1874502897262573\n",
      "[LOG 20200511-10:20:13] epoch: 0, batch: 3415 train-loss: 0.47174689173698425\n",
      "[LOG 20200511-10:20:13] epoch: 0, batch: 3416 train-loss: 0.711672842502594\n",
      "[LOG 20200511-10:20:13] epoch: 0, batch: 3417 train-loss: 2.3928842544555664\n",
      "[LOG 20200511-10:20:13] epoch: 0, batch: 3418 train-loss: 1.3631118535995483\n",
      "[LOG 20200511-10:20:13] epoch: 0, batch: 3419 train-loss: 2.0053176879882812\n",
      "[LOG 20200511-10:20:13] epoch: 0, batch: 3420 train-loss: 2.4830946922302246\n",
      "[LOG 20200511-10:20:13] epoch: 0, batch: 3421 train-loss: 1.3897291421890259\n",
      "[LOG 20200511-10:20:13] epoch: 0, batch: 3422 train-loss: 2.1447243690490723\n",
      "[LOG 20200511-10:20:13] epoch: 0, batch: 3423 train-loss: 0.9811714291572571\n",
      "[LOG 20200511-10:20:14] epoch: 0, batch: 3424 train-loss: 1.8809404373168945\n",
      "[LOG 20200511-10:20:14] epoch: 0, batch: 3425 train-loss: 1.558803915977478\n",
      "[LOG 20200511-10:20:14] epoch: 0, batch: 3426 train-loss: 1.452308177947998\n",
      "[LOG 20200511-10:20:14] epoch: 0, batch: 3427 train-loss: 0.6930700540542603\n",
      "[LOG 20200511-10:20:14] epoch: 0, batch: 3428 train-loss: 1.442806601524353\n",
      "[LOG 20200511-10:20:14] epoch: 0, batch: 3429 train-loss: 0.8191685676574707\n",
      "[LOG 20200511-10:20:14] epoch: 0, batch: 3430 train-loss: 0.9159387350082397\n",
      "[LOG 20200511-10:20:14] epoch: 0, batch: 3431 train-loss: 0.7969593405723572\n",
      "[LOG 20200511-10:20:15] epoch: 0, batch: 3432 train-loss: 1.401973009109497\n",
      "[LOG 20200511-10:20:15] epoch: 0, batch: 3433 train-loss: 2.3953254222869873\n",
      "[LOG 20200511-10:20:15] epoch: 0, batch: 3434 train-loss: 1.9819347858428955\n",
      "[LOG 20200511-10:20:15] epoch: 0, batch: 3435 train-loss: 1.1255409717559814\n",
      "[LOG 20200511-10:20:15] epoch: 0, batch: 3436 train-loss: 1.7547192573547363\n",
      "[LOG 20200511-10:20:15] epoch: 0, batch: 3437 train-loss: 1.1932954788208008\n",
      "[LOG 20200511-10:20:15] epoch: 0, batch: 3438 train-loss: 1.0806617736816406\n",
      "[LOG 20200511-10:20:15] epoch: 0, batch: 3439 train-loss: 1.4400361776351929\n",
      "[LOG 20200511-10:20:15] epoch: 0, batch: 3440 train-loss: 1.6978108882904053\n",
      "[LOG 20200511-10:20:16] epoch: 0, batch: 3441 train-loss: 2.2454183101654053\n",
      "[LOG 20200511-10:20:16] epoch: 0, batch: 3442 train-loss: 1.0221529006958008\n",
      "[LOG 20200511-10:20:16] epoch: 0, batch: 3443 train-loss: 1.3604097366333008\n",
      "[LOG 20200511-10:20:16] epoch: 0, batch: 3444 train-loss: 1.7940592765808105\n",
      "[LOG 20200511-10:20:16] epoch: 0, batch: 3445 train-loss: 1.5208016633987427\n",
      "[LOG 20200511-10:20:16] epoch: 0, batch: 3446 train-loss: 1.6613996028900146\n",
      "[LOG 20200511-10:20:16] epoch: 0, batch: 3447 train-loss: 1.1634852886199951\n",
      "[LOG 20200511-10:20:16] epoch: 0, batch: 3448 train-loss: 1.2394020557403564\n",
      "[LOG 20200511-10:20:17] epoch: 0, batch: 3449 train-loss: 1.6506718397140503\n",
      "[LOG 20200511-10:20:17] epoch: 0, batch: 3450 train-loss: 1.1584442853927612\n",
      "[LOG 20200511-10:20:17] epoch: 0, batch: 3451 train-loss: 1.359785556793213\n",
      "[LOG 20200511-10:20:17] epoch: 0, batch: 3452 train-loss: 1.3576934337615967\n",
      "[LOG 20200511-10:20:17] epoch: 0, batch: 3453 train-loss: 1.4987674951553345\n",
      "[LOG 20200511-10:20:17] epoch: 0, batch: 3454 train-loss: 1.9600930213928223\n",
      "[LOG 20200511-10:20:17] epoch: 0, batch: 3455 train-loss: 0.7615491151809692\n",
      "[LOG 20200511-10:20:17] epoch: 0, batch: 3456 train-loss: 2.0249898433685303\n",
      "[LOG 20200511-10:20:17] epoch: 0, batch: 3457 train-loss: 1.7591140270233154\n",
      "[LOG 20200511-10:20:18] epoch: 0, batch: 3458 train-loss: 1.3691256046295166\n",
      "[LOG 20200511-10:20:18] epoch: 0, batch: 3459 train-loss: 1.1480344533920288\n",
      "[LOG 20200511-10:20:18] epoch: 0, batch: 3460 train-loss: 2.2784228324890137\n",
      "[LOG 20200511-10:20:18] epoch: 0, batch: 3461 train-loss: 1.3563752174377441\n",
      "[LOG 20200511-10:20:18] epoch: 0, batch: 3462 train-loss: 2.249643325805664\n",
      "[LOG 20200511-10:20:18] epoch: 0, batch: 3463 train-loss: 1.2266615629196167\n",
      "[LOG 20200511-10:20:18] epoch: 0, batch: 3464 train-loss: 1.7161955833435059\n",
      "[LOG 20200511-10:20:19] epoch: 0, batch: 3465 train-loss: 1.6149024963378906\n",
      "[LOG 20200511-10:20:19] epoch: 0, batch: 3466 train-loss: 1.4844626188278198\n",
      "[LOG 20200511-10:20:19] epoch: 0, batch: 3467 train-loss: 1.1055610179901123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:20:19] epoch: 0, batch: 3468 train-loss: 1.4122201204299927\n",
      "[LOG 20200511-10:20:19] epoch: 0, batch: 3469 train-loss: 1.9920462369918823\n",
      "[LOG 20200511-10:20:19] epoch: 0, batch: 3470 train-loss: 1.1729812622070312\n",
      "[LOG 20200511-10:20:19] epoch: 0, batch: 3471 train-loss: 1.5077697038650513\n",
      "[LOG 20200511-10:20:19] epoch: 0, batch: 3472 train-loss: 1.2558473348617554\n",
      "[LOG 20200511-10:20:19] epoch: 0, batch: 3473 train-loss: 0.7126234173774719\n",
      "[LOG 20200511-10:20:20] epoch: 0, batch: 3474 train-loss: 1.0162639617919922\n",
      "[LOG 20200511-10:20:20] epoch: 0, batch: 3475 train-loss: 1.2255685329437256\n",
      "[LOG 20200511-10:20:20] epoch: 0, batch: 3476 train-loss: 1.0244982242584229\n",
      "[LOG 20200511-10:20:20] epoch: 0, batch: 3477 train-loss: 1.1726877689361572\n",
      "[LOG 20200511-10:20:20] epoch: 0, batch: 3478 train-loss: 0.8637012243270874\n",
      "[LOG 20200511-10:20:20] epoch: 0, batch: 3479 train-loss: 1.126375675201416\n",
      "[LOG 20200511-10:20:20] epoch: 0, batch: 3480 train-loss: 1.066545844078064\n",
      "[LOG 20200511-10:20:20] epoch: 0, batch: 3481 train-loss: 1.802785873413086\n",
      "[LOG 20200511-10:20:20] epoch: 0, batch: 3482 train-loss: 1.7651344537734985\n",
      "[LOG 20200511-10:20:21] epoch: 0, batch: 3483 train-loss: 1.7471375465393066\n",
      "[LOG 20200511-10:20:21] epoch: 0, batch: 3484 train-loss: 0.9008073210716248\n",
      "[LOG 20200511-10:20:21] epoch: 0, batch: 3485 train-loss: 1.6068570613861084\n",
      "[LOG 20200511-10:20:21] epoch: 0, batch: 3486 train-loss: 1.4465007781982422\n",
      "[LOG 20200511-10:20:21] epoch: 0, batch: 3487 train-loss: 1.8642940521240234\n",
      "[LOG 20200511-10:20:21] epoch: 0, batch: 3488 train-loss: 0.567836344242096\n",
      "[LOG 20200511-10:20:21] epoch: 0, batch: 3489 train-loss: 2.399831771850586\n",
      "[LOG 20200511-10:20:21] epoch: 0, batch: 3490 train-loss: 1.4531636238098145\n",
      "[LOG 20200511-10:20:22] epoch: 0, batch: 3491 train-loss: 1.0367845296859741\n",
      "[LOG 20200511-10:20:22] epoch: 0, batch: 3492 train-loss: 1.750937819480896\n",
      "[LOG 20200511-10:20:22] epoch: 0, batch: 3493 train-loss: 2.3240742683410645\n",
      "[LOG 20200511-10:20:22] epoch: 0, batch: 3494 train-loss: 1.507054328918457\n",
      "[LOG 20200511-10:20:22] epoch: 0, batch: 3495 train-loss: 2.2167117595672607\n",
      "[LOG 20200511-10:20:22] epoch: 0, batch: 3496 train-loss: 1.788353443145752\n",
      "[LOG 20200511-10:20:22] epoch: 0, batch: 3497 train-loss: 1.2580280303955078\n",
      "[LOG 20200511-10:20:22] epoch: 0, batch: 3498 train-loss: 1.359191656112671\n",
      "[LOG 20200511-10:20:22] epoch: 0, batch: 3499 train-loss: 1.1885998249053955\n",
      "[LOG 20200511-10:20:23] epoch: 0, batch: 3500 train-loss: 0.8566324710845947\n",
      "[LOG 20200511-10:20:23] epoch: 0, batch: 3501 train-loss: 2.455996513366699\n",
      "[LOG 20200511-10:20:23] epoch: 0, batch: 3502 train-loss: 1.052834391593933\n",
      "[LOG 20200511-10:20:23] epoch: 0, batch: 3503 train-loss: 0.8403747081756592\n",
      "[LOG 20200511-10:20:23] epoch: 0, batch: 3504 train-loss: 1.3077960014343262\n",
      "[LOG 20200511-10:20:23] epoch: 0, batch: 3505 train-loss: 1.9478044509887695\n",
      "[LOG 20200511-10:20:23] epoch: 0, batch: 3506 train-loss: 1.2476563453674316\n",
      "[LOG 20200511-10:20:23] epoch: 0, batch: 3507 train-loss: 1.1148144006729126\n",
      "[LOG 20200511-10:20:23] epoch: 0, batch: 3508 train-loss: 1.0269041061401367\n",
      "[LOG 20200511-10:20:24] epoch: 0, batch: 3509 train-loss: 0.9114561080932617\n",
      "[LOG 20200511-10:20:24] epoch: 0, batch: 3510 train-loss: 2.6620380878448486\n",
      "[LOG 20200511-10:20:24] epoch: 0, batch: 3511 train-loss: 2.2099742889404297\n",
      "[LOG 20200511-10:20:24] epoch: 0, batch: 3512 train-loss: 0.6581230163574219\n",
      "[LOG 20200511-10:20:24] epoch: 0, batch: 3513 train-loss: 2.541168451309204\n",
      "[LOG 20200511-10:20:24] epoch: 0, batch: 3514 train-loss: 1.2800993919372559\n",
      "[LOG 20200511-10:20:24] epoch: 0, batch: 3515 train-loss: 1.8779473304748535\n",
      "[LOG 20200511-10:20:24] epoch: 0, batch: 3516 train-loss: 1.5835561752319336\n",
      "[LOG 20200511-10:20:24] epoch: 0, batch: 3517 train-loss: 1.3683032989501953\n",
      "[LOG 20200511-10:20:25] epoch: 0, batch: 3518 train-loss: 1.2993677854537964\n",
      "[LOG 20200511-10:20:25] epoch: 0, batch: 3519 train-loss: 1.6095789670944214\n",
      "[LOG 20200511-10:20:25] epoch: 0, batch: 3520 train-loss: 1.0416820049285889\n",
      "[LOG 20200511-10:20:25] epoch: 0, batch: 3521 train-loss: 0.6862418055534363\n",
      "[LOG 20200511-10:20:25] epoch: 0, batch: 3522 train-loss: 1.376544713973999\n",
      "[LOG 20200511-10:20:25] epoch: 0, batch: 3523 train-loss: 1.2787086963653564\n",
      "[LOG 20200511-10:20:25] epoch: 0, batch: 3524 train-loss: 0.8649001121520996\n",
      "[LOG 20200511-10:20:25] epoch: 0, batch: 3525 train-loss: 2.075012445449829\n",
      "[LOG 20200511-10:20:25] epoch: 0, batch: 3526 train-loss: 0.7588846683502197\n",
      "[LOG 20200511-10:20:26] epoch: 0, batch: 3527 train-loss: 1.724485993385315\n",
      "[LOG 20200511-10:20:26] epoch: 0, batch: 3528 train-loss: 0.974404513835907\n",
      "[LOG 20200511-10:20:26] epoch: 0, batch: 3529 train-loss: 2.0049490928649902\n",
      "[LOG 20200511-10:20:26] epoch: 0, batch: 3530 train-loss: 1.4925439357757568\n",
      "[LOG 20200511-10:20:26] epoch: 0, batch: 3531 train-loss: 1.3031005859375\n",
      "[LOG 20200511-10:20:26] epoch: 0, batch: 3532 train-loss: 1.228086233139038\n",
      "[LOG 20200511-10:20:26] epoch: 0, batch: 3533 train-loss: 0.9528470039367676\n",
      "[LOG 20200511-10:20:26] epoch: 0, batch: 3534 train-loss: 1.2896100282669067\n",
      "[LOG 20200511-10:20:26] epoch: 0, batch: 3535 train-loss: 1.4595786333084106\n",
      "[LOG 20200511-10:20:27] epoch: 0, batch: 3536 train-loss: 1.039676308631897\n",
      "[LOG 20200511-10:20:27] epoch: 0, batch: 3537 train-loss: 1.0826358795166016\n",
      "[LOG 20200511-10:20:27] epoch: 0, batch: 3538 train-loss: 0.36530399322509766\n",
      "[LOG 20200511-10:20:27] epoch: 0, batch: 3539 train-loss: 1.4809985160827637\n",
      "[LOG 20200511-10:20:27] epoch: 0, batch: 3540 train-loss: 1.6454594135284424\n",
      "[LOG 20200511-10:20:27] epoch: 0, batch: 3541 train-loss: 1.6264954805374146\n",
      "[LOG 20200511-10:20:27] epoch: 0, batch: 3542 train-loss: 1.4118797779083252\n",
      "[LOG 20200511-10:20:27] epoch: 0, batch: 3543 train-loss: 1.2946393489837646\n",
      "[LOG 20200511-10:20:27] epoch: 0, batch: 3544 train-loss: 2.129849910736084\n",
      "[LOG 20200511-10:20:28] epoch: 0, batch: 3545 train-loss: 0.7716813087463379\n",
      "[LOG 20200511-10:20:28] epoch: 0, batch: 3546 train-loss: 0.881911039352417\n",
      "[LOG 20200511-10:20:28] epoch: 0, batch: 3547 train-loss: 1.2223705053329468\n",
      "[LOG 20200511-10:20:28] epoch: 0, batch: 3548 train-loss: 1.9048094749450684\n",
      "[LOG 20200511-10:20:28] epoch: 0, batch: 3549 train-loss: 1.8059990406036377\n",
      "[LOG 20200511-10:20:28] epoch: 0, batch: 3550 train-loss: 2.2526116371154785\n",
      "[LOG 20200511-10:20:28] epoch: 0, batch: 3551 train-loss: 1.1983118057250977\n",
      "[LOG 20200511-10:20:28] epoch: 0, batch: 3552 train-loss: 1.644740343093872\n",
      "[LOG 20200511-10:20:28] epoch: 0, batch: 3553 train-loss: 1.754262089729309\n",
      "[LOG 20200511-10:20:29] epoch: 0, batch: 3554 train-loss: 0.8366166949272156\n",
      "[LOG 20200511-10:20:29] epoch: 0, batch: 3555 train-loss: 2.1295690536499023\n",
      "[LOG 20200511-10:20:29] epoch: 0, batch: 3556 train-loss: 1.654175877571106\n",
      "[LOG 20200511-10:20:29] epoch: 0, batch: 3557 train-loss: 1.2502789497375488\n",
      "[LOG 20200511-10:20:29] epoch: 0, batch: 3558 train-loss: 1.5449597835540771\n",
      "[LOG 20200511-10:20:29] epoch: 0, batch: 3559 train-loss: 2.1312685012817383\n",
      "[LOG 20200511-10:20:29] epoch: 0, batch: 3560 train-loss: 1.8941247463226318\n",
      "[LOG 20200511-10:20:29] epoch: 0, batch: 3561 train-loss: 1.957487940788269\n",
      "[LOG 20200511-10:20:30] epoch: 0, batch: 3562 train-loss: 0.9151618480682373\n",
      "[LOG 20200511-10:20:30] epoch: 0, batch: 3563 train-loss: 1.4492805004119873\n",
      "[LOG 20200511-10:20:30] epoch: 0, batch: 3564 train-loss: 1.7018181085586548\n",
      "[LOG 20200511-10:20:30] epoch: 0, batch: 3565 train-loss: 1.2652359008789062\n",
      "[LOG 20200511-10:20:30] epoch: 0, batch: 3566 train-loss: 1.6119475364685059\n",
      "[LOG 20200511-10:20:30] epoch: 0, batch: 3567 train-loss: 1.7877880334854126\n",
      "[LOG 20200511-10:20:30] epoch: 0, batch: 3568 train-loss: 0.7090749740600586\n",
      "[LOG 20200511-10:20:30] epoch: 0, batch: 3569 train-loss: 1.3210774660110474\n",
      "[LOG 20200511-10:20:31] epoch: 0, batch: 3570 train-loss: 1.0420982837677002\n",
      "[LOG 20200511-10:20:31] epoch: 0, batch: 3571 train-loss: 1.2656364440917969\n",
      "[LOG 20200511-10:20:31] epoch: 0, batch: 3572 train-loss: 0.8942350745201111\n",
      "[LOG 20200511-10:20:31] epoch: 0, batch: 3573 train-loss: 1.9983155727386475\n",
      "[LOG 20200511-10:20:31] epoch: 0, batch: 3574 train-loss: 1.6240004301071167\n",
      "[LOG 20200511-10:20:31] epoch: 0, batch: 3575 train-loss: 2.4561209678649902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:20:31] epoch: 0, batch: 3576 train-loss: 1.3839269876480103\n",
      "[LOG 20200511-10:20:31] epoch: 0, batch: 3577 train-loss: 2.010807514190674\n",
      "[LOG 20200511-10:20:31] epoch: 0, batch: 3578 train-loss: 0.716179609298706\n",
      "[LOG 20200511-10:20:32] epoch: 0, batch: 3579 train-loss: 2.350276470184326\n",
      "[LOG 20200511-10:20:32] epoch: 0, batch: 3580 train-loss: 0.7776621580123901\n",
      "[LOG 20200511-10:20:32] epoch: 0, batch: 3581 train-loss: 0.9906747341156006\n",
      "[LOG 20200511-10:20:32] epoch: 0, batch: 3582 train-loss: 1.2622222900390625\n",
      "[LOG 20200511-10:20:32] epoch: 0, batch: 3583 train-loss: 1.8510322570800781\n",
      "[LOG 20200511-10:20:32] epoch: 0, batch: 3584 train-loss: 1.4691603183746338\n",
      "[LOG 20200511-10:20:32] epoch: 0, batch: 3585 train-loss: 1.6601288318634033\n",
      "[LOG 20200511-10:20:32] epoch: 0, batch: 3586 train-loss: 1.0380213260650635\n",
      "[LOG 20200511-10:20:32] epoch: 0, batch: 3587 train-loss: 0.8941514492034912\n",
      "[LOG 20200511-10:20:33] epoch: 0, batch: 3588 train-loss: 1.7017120122909546\n",
      "[LOG 20200511-10:20:33] epoch: 0, batch: 3589 train-loss: 1.599815011024475\n",
      "[LOG 20200511-10:20:33] epoch: 0, batch: 3590 train-loss: 0.7698538303375244\n",
      "[LOG 20200511-10:20:33] epoch: 0, batch: 3591 train-loss: 2.949674129486084\n",
      "[LOG 20200511-10:20:33] epoch: 0, batch: 3592 train-loss: 1.0176395177841187\n",
      "[LOG 20200511-10:20:33] epoch: 0, batch: 3593 train-loss: 1.5576508045196533\n",
      "[LOG 20200511-10:20:33] epoch: 0, batch: 3594 train-loss: 1.8426536321640015\n",
      "[LOG 20200511-10:20:33] epoch: 0, batch: 3595 train-loss: 1.7899036407470703\n",
      "[LOG 20200511-10:20:33] epoch: 0, batch: 3596 train-loss: 1.2524148225784302\n",
      "[LOG 20200511-10:20:34] epoch: 0, batch: 3597 train-loss: 0.6767600774765015\n",
      "[LOG 20200511-10:20:34] epoch: 0, batch: 3598 train-loss: 1.660912036895752\n",
      "[LOG 20200511-10:20:34] epoch: 0, batch: 3599 train-loss: 2.606321096420288\n",
      "[LOG 20200511-10:20:34] epoch: 0, batch: 3600 train-loss: 1.7417471408843994\n",
      "[LOG 20200511-10:20:34] epoch: 0, batch: 3601 train-loss: 1.9950597286224365\n",
      "[LOG 20200511-10:20:34] epoch: 0, batch: 3602 train-loss: 1.349414348602295\n",
      "[LOG 20200511-10:20:34] epoch: 0, batch: 3603 train-loss: 1.7050673961639404\n",
      "[LOG 20200511-10:20:34] epoch: 0, batch: 3604 train-loss: 1.3152153491973877\n",
      "[LOG 20200511-10:20:34] epoch: 0, batch: 3605 train-loss: 1.2617663145065308\n",
      "[LOG 20200511-10:20:35] epoch: 0, batch: 3606 train-loss: 1.2697147130966187\n",
      "[LOG 20200511-10:20:35] epoch: 0, batch: 3607 train-loss: 1.698004126548767\n",
      "[LOG 20200511-10:20:35] epoch: 0, batch: 3608 train-loss: 1.3341765403747559\n",
      "[LOG 20200511-10:20:35] epoch: 0, batch: 3609 train-loss: 1.923248291015625\n",
      "[LOG 20200511-10:20:35] epoch: 0, batch: 3610 train-loss: 1.1024820804595947\n",
      "[LOG 20200511-10:20:35] epoch: 0, batch: 3611 train-loss: 1.9853394031524658\n",
      "[LOG 20200511-10:20:35] epoch: 0, batch: 3612 train-loss: 1.8672736883163452\n",
      "[LOG 20200511-10:20:35] epoch: 0, batch: 3613 train-loss: 0.9492419362068176\n",
      "[LOG 20200511-10:20:36] epoch: 0, batch: 3614 train-loss: 1.8030694723129272\n",
      "[LOG 20200511-10:20:36] epoch: 0, batch: 3615 train-loss: 1.4268848896026611\n",
      "[LOG 20200511-10:20:36] epoch: 0, batch: 3616 train-loss: 1.414209246635437\n",
      "[LOG 20200511-10:20:36] epoch: 0, batch: 3617 train-loss: 0.7498937249183655\n",
      "[LOG 20200511-10:20:36] epoch: 0, batch: 3618 train-loss: 1.333857536315918\n",
      "[LOG 20200511-10:20:36] epoch: 0, batch: 3619 train-loss: 1.4530484676361084\n",
      "[LOG 20200511-10:20:36] epoch: 0, batch: 3620 train-loss: 1.262096643447876\n",
      "[LOG 20200511-10:20:36] epoch: 0, batch: 3621 train-loss: 1.8724675178527832\n",
      "[LOG 20200511-10:20:36] epoch: 0, batch: 3622 train-loss: 1.4160466194152832\n",
      "[LOG 20200511-10:20:37] epoch: 0, batch: 3623 train-loss: 1.9266059398651123\n",
      "[LOG 20200511-10:20:37] epoch: 0, batch: 3624 train-loss: 0.7965705394744873\n",
      "[LOG 20200511-10:20:37] epoch: 0, batch: 3625 train-loss: 0.9084737300872803\n",
      "[LOG 20200511-10:20:37] epoch: 0, batch: 3626 train-loss: 1.7747812271118164\n",
      "[LOG 20200511-10:20:37] epoch: 0, batch: 3627 train-loss: 1.5181477069854736\n",
      "[LOG 20200511-10:20:37] epoch: 0, batch: 3628 train-loss: 1.7406694889068604\n",
      "[LOG 20200511-10:20:37] epoch: 0, batch: 3629 train-loss: 2.5852203369140625\n",
      "[LOG 20200511-10:20:37] epoch: 0, batch: 3630 train-loss: 0.8028180599212646\n",
      "[LOG 20200511-10:20:37] epoch: 0, batch: 3631 train-loss: 1.0559099912643433\n",
      "[LOG 20200511-10:20:37] epoch: 0, batch: 3632 train-loss: 1.8998146057128906\n",
      "[LOG 20200511-10:20:38] epoch: 0, batch: 3633 train-loss: 1.6249637603759766\n",
      "[LOG 20200511-10:20:38] epoch: 0, batch: 3634 train-loss: 1.593458652496338\n",
      "[LOG 20200511-10:20:38] epoch: 0, batch: 3635 train-loss: 2.91979718208313\n",
      "[LOG 20200511-10:20:38] epoch: 0, batch: 3636 train-loss: 0.8532117009162903\n",
      "[LOG 20200511-10:20:38] epoch: 0, batch: 3637 train-loss: 1.3625333309173584\n",
      "[LOG 20200511-10:20:38] epoch: 0, batch: 3638 train-loss: 0.7529076933860779\n",
      "[LOG 20200511-10:20:38] epoch: 0, batch: 3639 train-loss: 1.7798417806625366\n",
      "[LOG 20200511-10:20:38] epoch: 0, batch: 3640 train-loss: 0.7923409938812256\n",
      "[LOG 20200511-10:20:38] epoch: 0, batch: 3641 train-loss: 0.5734280347824097\n",
      "[LOG 20200511-10:20:39] epoch: 0, batch: 3642 train-loss: 1.7078564167022705\n",
      "[LOG 20200511-10:20:39] epoch: 0, batch: 3643 train-loss: 0.971065104007721\n",
      "[LOG 20200511-10:20:39] epoch: 0, batch: 3644 train-loss: 1.8749712705612183\n",
      "[LOG 20200511-10:20:39] epoch: 0, batch: 3645 train-loss: 1.5484278202056885\n",
      "[LOG 20200511-10:20:39] epoch: 0, batch: 3646 train-loss: 0.9752703905105591\n",
      "[LOG 20200511-10:20:39] epoch: 0, batch: 3647 train-loss: 2.215669870376587\n",
      "[LOG 20200511-10:20:39] epoch: 0, batch: 3648 train-loss: 2.1443958282470703\n",
      "[LOG 20200511-10:20:39] epoch: 0, batch: 3649 train-loss: 2.0546817779541016\n",
      "[LOG 20200511-10:20:39] epoch: 0, batch: 3650 train-loss: 0.8421050310134888\n",
      "[LOG 20200511-10:20:39] epoch: 0, batch: 3651 train-loss: 1.319119930267334\n",
      "[LOG 20200511-10:20:40] epoch: 0, batch: 3652 train-loss: 0.8695553541183472\n",
      "[LOG 20200511-10:20:40] epoch: 0, batch: 3653 train-loss: 1.2551040649414062\n",
      "[LOG 20200511-10:20:40] epoch: 0, batch: 3654 train-loss: 1.7506723403930664\n",
      "[LOG 20200511-10:20:40] epoch: 0, batch: 3655 train-loss: 2.710409164428711\n",
      "[LOG 20200511-10:20:40] epoch: 0, batch: 3656 train-loss: 0.44904205203056335\n",
      "[LOG 20200511-10:20:40] epoch: 0, batch: 3657 train-loss: 1.174808144569397\n",
      "[LOG 20200511-10:20:40] epoch: 0, batch: 3658 train-loss: 1.9865119457244873\n",
      "[LOG 20200511-10:20:40] epoch: 0, batch: 3659 train-loss: 1.3713141679763794\n",
      "[LOG 20200511-10:20:40] epoch: 0, batch: 3660 train-loss: 1.314208984375\n",
      "[LOG 20200511-10:20:40] epoch: 0, batch: 3661 train-loss: 1.223174810409546\n",
      "[LOG 20200511-10:20:41] epoch: 0, batch: 3662 train-loss: 2.2404122352600098\n",
      "[LOG 20200511-10:20:41] epoch: 0, batch: 3663 train-loss: 1.891434669494629\n",
      "[LOG 20200511-10:20:41] epoch: 0, batch: 3664 train-loss: 1.6049907207489014\n",
      "[LOG 20200511-10:20:41] epoch: 0, batch: 3665 train-loss: 1.3601703643798828\n",
      "[LOG 20200511-10:20:41] epoch: 0, batch: 3666 train-loss: 1.9802825450897217\n",
      "[LOG 20200511-10:20:41] epoch: 0, batch: 3667 train-loss: 1.241371989250183\n",
      "[LOG 20200511-10:20:41] epoch: 0, batch: 3668 train-loss: 1.404764175415039\n",
      "[LOG 20200511-10:20:41] epoch: 0, batch: 3669 train-loss: 0.9185400009155273\n",
      "[LOG 20200511-10:20:41] epoch: 0, batch: 3670 train-loss: 1.0097270011901855\n",
      "[LOG 20200511-10:20:42] epoch: 0, batch: 3671 train-loss: 1.5933979749679565\n",
      "[LOG 20200511-10:20:42] epoch: 0, batch: 3672 train-loss: 2.057748794555664\n",
      "[LOG 20200511-10:20:42] epoch: 0, batch: 3673 train-loss: 1.6840298175811768\n",
      "[LOG 20200511-10:20:42] epoch: 0, batch: 3674 train-loss: 2.1662631034851074\n",
      "[LOG 20200511-10:20:42] epoch: 0, batch: 3675 train-loss: 1.3809723854064941\n",
      "[LOG 20200511-10:20:42] epoch: 0, batch: 3676 train-loss: 1.745335340499878\n",
      "[LOG 20200511-10:20:42] epoch: 0, batch: 3677 train-loss: 1.7927536964416504\n",
      "[LOG 20200511-10:20:42] epoch: 0, batch: 3678 train-loss: 1.305672287940979\n",
      "[LOG 20200511-10:20:42] epoch: 0, batch: 3679 train-loss: 1.9053871631622314\n",
      "[LOG 20200511-10:20:43] epoch: 0, batch: 3680 train-loss: 1.4472267627716064\n",
      "[LOG 20200511-10:20:43] epoch: 0, batch: 3681 train-loss: 1.563726782798767\n",
      "[LOG 20200511-10:20:43] epoch: 0, batch: 3682 train-loss: 2.163051128387451\n",
      "[LOG 20200511-10:20:43] epoch: 0, batch: 3683 train-loss: 1.0806031227111816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:20:43] epoch: 0, batch: 3684 train-loss: 1.815937876701355\n",
      "[LOG 20200511-10:20:43] epoch: 0, batch: 3685 train-loss: 2.1580638885498047\n",
      "[LOG 20200511-10:20:43] epoch: 0, batch: 3686 train-loss: 1.3159397840499878\n",
      "[LOG 20200511-10:20:43] epoch: 0, batch: 3687 train-loss: 0.8569552302360535\n",
      "[LOG 20200511-10:20:44] epoch: 0, batch: 3688 train-loss: 0.9902334213256836\n",
      "[LOG 20200511-10:20:44] epoch: 0, batch: 3689 train-loss: 0.4686127007007599\n",
      "[LOG 20200511-10:20:44] epoch: 0, batch: 3690 train-loss: 0.583040177822113\n",
      "[LOG 20200511-10:20:44] epoch: 0, batch: 3691 train-loss: 1.453751564025879\n",
      "[LOG 20200511-10:20:44] epoch: 0, batch: 3692 train-loss: 1.2154451608657837\n",
      "[LOG 20200511-10:20:44] epoch: 0, batch: 3693 train-loss: 1.6040376424789429\n",
      "[LOG 20200511-10:20:44] epoch: 0, batch: 3694 train-loss: 1.3885693550109863\n",
      "[LOG 20200511-10:20:44] epoch: 0, batch: 3695 train-loss: 2.167109251022339\n",
      "[LOG 20200511-10:20:44] epoch: 0, batch: 3696 train-loss: 1.2788437604904175\n",
      "[LOG 20200511-10:20:44] epoch: 0, batch: 3697 train-loss: 1.8774240016937256\n",
      "[LOG 20200511-10:20:45] epoch: 0, batch: 3698 train-loss: 2.1809351444244385\n",
      "[LOG 20200511-10:20:45] epoch: 0, batch: 3699 train-loss: 1.3760555982589722\n",
      "[LOG 20200511-10:20:45] epoch: 0, batch: 3700 train-loss: 1.736055850982666\n",
      "[LOG 20200511-10:20:45] epoch: 0, batch: 3701 train-loss: 1.2487130165100098\n",
      "[LOG 20200511-10:20:45] epoch: 0, batch: 3702 train-loss: 0.8378363251686096\n",
      "[LOG 20200511-10:20:45] epoch: 0, batch: 3703 train-loss: 1.0997742414474487\n",
      "[LOG 20200511-10:20:45] epoch: 0, batch: 3704 train-loss: 2.6027345657348633\n",
      "[LOG 20200511-10:20:45] epoch: 0, batch: 3705 train-loss: 1.868953824043274\n",
      "[LOG 20200511-10:20:46] epoch: 0, batch: 3706 train-loss: 1.2695826292037964\n",
      "[LOG 20200511-10:20:46] epoch: 0, batch: 3707 train-loss: 1.0123822689056396\n",
      "[LOG 20200511-10:20:46] epoch: 0, batch: 3708 train-loss: 0.9919835925102234\n",
      "[LOG 20200511-10:20:46] epoch: 0, batch: 3709 train-loss: 1.393815040588379\n",
      "[LOG 20200511-10:20:46] epoch: 0, batch: 3710 train-loss: 0.8360100984573364\n",
      "[LOG 20200511-10:20:46] epoch: 0, batch: 3711 train-loss: 1.7826135158538818\n",
      "[LOG 20200511-10:20:46] epoch: 0, batch: 3712 train-loss: 1.660494089126587\n",
      "[LOG 20200511-10:20:47] epoch: 0, batch: 3713 train-loss: 1.0439167022705078\n",
      "[LOG 20200511-10:20:47] epoch: 0, batch: 3714 train-loss: 1.8953807353973389\n",
      "[LOG 20200511-10:20:47] epoch: 0, batch: 3715 train-loss: 0.6355674266815186\n",
      "[LOG 20200511-10:20:47] epoch: 0, batch: 3716 train-loss: 1.7637152671813965\n",
      "[LOG 20200511-10:20:47] epoch: 0, batch: 3717 train-loss: 1.7727102041244507\n",
      "[LOG 20200511-10:20:47] epoch: 0, batch: 3718 train-loss: 1.7048062086105347\n",
      "[LOG 20200511-10:20:47] epoch: 0, batch: 3719 train-loss: 1.535905361175537\n",
      "[LOG 20200511-10:20:47] epoch: 0, batch: 3720 train-loss: 1.514737844467163\n",
      "[LOG 20200511-10:20:47] epoch: 0, batch: 3721 train-loss: 0.8484865427017212\n",
      "[LOG 20200511-10:20:48] epoch: 0, batch: 3722 train-loss: 1.1252005100250244\n",
      "[LOG 20200511-10:20:48] epoch: 0, batch: 3723 train-loss: 1.9426616430282593\n",
      "[LOG 20200511-10:20:48] epoch: 0, batch: 3724 train-loss: 1.129709243774414\n",
      "[LOG 20200511-10:20:48] epoch: 0, batch: 3725 train-loss: 2.5875847339630127\n",
      "[LOG 20200511-10:20:48] epoch: 0, batch: 3726 train-loss: 1.2910963296890259\n",
      "[LOG 20200511-10:20:48] epoch: 0, batch: 3727 train-loss: 2.0049281120300293\n",
      "[LOG 20200511-10:20:48] epoch: 0, batch: 3728 train-loss: 0.9584077000617981\n",
      "[LOG 20200511-10:20:48] epoch: 0, batch: 3729 train-loss: 1.7018532752990723\n",
      "[LOG 20200511-10:20:49] epoch: 0, batch: 3730 train-loss: 1.0492427349090576\n",
      "[LOG 20200511-10:20:49] epoch: 0, batch: 3731 train-loss: 3.1728124618530273\n",
      "[LOG 20200511-10:20:49] epoch: 0, batch: 3732 train-loss: 2.230902910232544\n",
      "[LOG 20200511-10:20:49] epoch: 0, batch: 3733 train-loss: 0.8224657773971558\n",
      "[LOG 20200511-10:20:49] epoch: 0, batch: 3734 train-loss: 0.8477753400802612\n",
      "[LOG 20200511-10:20:49] epoch: 0, batch: 3735 train-loss: 0.5007907152175903\n",
      "[LOG 20200511-10:20:49] epoch: 0, batch: 3736 train-loss: 1.086090326309204\n",
      "[LOG 20200511-10:20:49] epoch: 0, batch: 3737 train-loss: 0.9803170561790466\n",
      "[LOG 20200511-10:20:50] epoch: 0, batch: 3738 train-loss: 2.008122444152832\n",
      "[LOG 20200511-10:20:50] epoch: 0, batch: 3739 train-loss: 1.1072936058044434\n",
      "[LOG 20200511-10:20:50] epoch: 0, batch: 3740 train-loss: 1.3333814144134521\n",
      "[LOG 20200511-10:20:50] epoch: 0, batch: 3741 train-loss: 1.4195630550384521\n",
      "[LOG 20200511-10:20:50] epoch: 0, batch: 3742 train-loss: 1.205209493637085\n",
      "[LOG 20200511-10:20:50] epoch: 0, batch: 3743 train-loss: 1.633448839187622\n",
      "[LOG 20200511-10:20:50] epoch: 0, batch: 3744 train-loss: 1.6162354946136475\n",
      "[LOG 20200511-10:20:50] epoch: 0, batch: 3745 train-loss: 1.094726800918579\n",
      "[LOG 20200511-10:20:51] epoch: 0, batch: 3746 train-loss: 2.259965419769287\n",
      "[LOG 20200511-10:20:51] epoch: 0, batch: 3747 train-loss: 1.2090654373168945\n",
      "[LOG 20200511-10:20:51] epoch: 0, batch: 3748 train-loss: 1.6160032749176025\n",
      "[LOG 20200511-10:20:51] epoch: 0, batch: 3749 train-loss: 1.9656686782836914\n",
      "[LOG 20200511-10:20:51] epoch: 0, batch: 3750 train-loss: 0.9155327677726746\n",
      "[LOG 20200511-10:20:51] epoch: 0, batch: 3751 train-loss: 1.7201228141784668\n",
      "[LOG 20200511-10:20:51] epoch: 0, batch: 3752 train-loss: 1.9179112911224365\n",
      "[LOG 20200511-10:20:51] epoch: 0, batch: 3753 train-loss: 0.9192748069763184\n",
      "[LOG 20200511-10:20:52] epoch: 0, batch: 3754 train-loss: 2.0423779487609863\n",
      "[LOG 20200511-10:20:52] epoch: 0, batch: 3755 train-loss: 1.9042507410049438\n",
      "[LOG 20200511-10:20:52] epoch: 0, batch: 3756 train-loss: 1.2714320421218872\n",
      "[LOG 20200511-10:20:52] epoch: 0, batch: 3757 train-loss: 2.103807210922241\n",
      "[LOG 20200511-10:20:52] epoch: 0, batch: 3758 train-loss: 1.196138620376587\n",
      "[LOG 20200511-10:20:52] epoch: 0, batch: 3759 train-loss: 2.2146005630493164\n",
      "[LOG 20200511-10:20:52] epoch: 0, batch: 3760 train-loss: 1.6360747814178467\n",
      "[LOG 20200511-10:20:52] epoch: 0, batch: 3761 train-loss: 1.2241472005844116\n",
      "[LOG 20200511-10:20:53] epoch: 0, batch: 3762 train-loss: 2.4530444145202637\n",
      "[LOG 20200511-10:20:53] epoch: 0, batch: 3763 train-loss: 2.5609664916992188\n",
      "[LOG 20200511-10:20:53] epoch: 0, batch: 3764 train-loss: 1.46010422706604\n",
      "[LOG 20200511-10:20:53] epoch: 0, batch: 3765 train-loss: 0.8428253531455994\n",
      "[LOG 20200511-10:20:53] epoch: 0, batch: 3766 train-loss: 1.4599618911743164\n",
      "[LOG 20200511-10:20:53] epoch: 0, batch: 3767 train-loss: 2.213501453399658\n",
      "[LOG 20200511-10:20:53] epoch: 0, batch: 3768 train-loss: 1.3128756284713745\n",
      "[LOG 20200511-10:20:53] epoch: 0, batch: 3769 train-loss: 1.7308982610702515\n",
      "[LOG 20200511-10:20:54] epoch: 0, batch: 3770 train-loss: 1.7344337701797485\n",
      "[LOG 20200511-10:20:54] epoch: 0, batch: 3771 train-loss: 1.099848985671997\n",
      "[LOG 20200511-10:20:54] epoch: 0, batch: 3772 train-loss: 2.0541276931762695\n",
      "[LOG 20200511-10:20:54] epoch: 0, batch: 3773 train-loss: 1.2779200077056885\n",
      "[LOG 20200511-10:20:54] epoch: 0, batch: 3774 train-loss: 1.4219480752944946\n",
      "[LOG 20200511-10:20:54] epoch: 0, batch: 3775 train-loss: 1.0049190521240234\n",
      "[LOG 20200511-10:20:54] epoch: 0, batch: 3776 train-loss: 0.9218306541442871\n",
      "[LOG 20200511-10:20:54] epoch: 0, batch: 3777 train-loss: 1.488532304763794\n",
      "[LOG 20200511-10:20:55] epoch: 0, batch: 3778 train-loss: 1.1668882369995117\n",
      "[LOG 20200511-10:20:55] epoch: 0, batch: 3779 train-loss: 1.0346324443817139\n",
      "[LOG 20200511-10:20:55] epoch: 0, batch: 3780 train-loss: 2.637324810028076\n",
      "[LOG 20200511-10:20:55] epoch: 0, batch: 3781 train-loss: 1.0592249631881714\n",
      "[LOG 20200511-10:20:55] epoch: 0, batch: 3782 train-loss: 1.2832947969436646\n",
      "[LOG 20200511-10:20:55] epoch: 0, batch: 3783 train-loss: 1.782905101776123\n",
      "[LOG 20200511-10:20:55] epoch: 0, batch: 3784 train-loss: 2.375680923461914\n",
      "[LOG 20200511-10:20:55] epoch: 0, batch: 3785 train-loss: 1.5549012422561646\n",
      "[LOG 20200511-10:20:56] epoch: 0, batch: 3786 train-loss: 1.241882085800171\n",
      "[LOG 20200511-10:20:56] epoch: 0, batch: 3787 train-loss: 1.5829858779907227\n",
      "[LOG 20200511-10:20:56] epoch: 0, batch: 3788 train-loss: 1.4481947422027588\n",
      "[LOG 20200511-10:20:56] epoch: 0, batch: 3789 train-loss: 1.536912202835083\n",
      "[LOG 20200511-10:20:56] epoch: 0, batch: 3790 train-loss: 0.6627724170684814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:20:56] epoch: 0, batch: 3791 train-loss: 1.7510278224945068\n",
      "[LOG 20200511-10:20:56] epoch: 0, batch: 3792 train-loss: 1.3546500205993652\n",
      "[LOG 20200511-10:20:56] epoch: 0, batch: 3793 train-loss: 1.8713244199752808\n",
      "[LOG 20200511-10:20:57] epoch: 0, batch: 3794 train-loss: 1.413400411605835\n",
      "[LOG 20200511-10:20:57] epoch: 0, batch: 3795 train-loss: 1.7306830883026123\n",
      "[LOG 20200511-10:20:57] epoch: 0, batch: 3796 train-loss: 1.2881348133087158\n",
      "[LOG 20200511-10:20:57] epoch: 0, batch: 3797 train-loss: 1.7827788591384888\n",
      "[LOG 20200511-10:20:57] epoch: 0, batch: 3798 train-loss: 1.0978862047195435\n",
      "[LOG 20200511-10:20:57] epoch: 0, batch: 3799 train-loss: 1.2540255784988403\n",
      "[LOG 20200511-10:20:57] epoch: 0, batch: 3800 train-loss: 2.0665090084075928\n",
      "[LOG 20200511-10:20:57] epoch: 0, batch: 3801 train-loss: 1.6961274147033691\n",
      "[LOG 20200511-10:20:57] epoch: 0, batch: 3802 train-loss: 2.072939157485962\n",
      "[LOG 20200511-10:20:58] epoch: 0, batch: 3803 train-loss: 1.01906418800354\n",
      "[LOG 20200511-10:20:58] epoch: 0, batch: 3804 train-loss: 1.711451768875122\n",
      "[LOG 20200511-10:20:58] epoch: 0, batch: 3805 train-loss: 1.0292937755584717\n",
      "[LOG 20200511-10:20:58] epoch: 0, batch: 3806 train-loss: 1.596319317817688\n",
      "[LOG 20200511-10:20:58] epoch: 0, batch: 3807 train-loss: 0.9461485743522644\n",
      "[LOG 20200511-10:20:58] epoch: 0, batch: 3808 train-loss: 0.8912845253944397\n",
      "[LOG 20200511-10:20:58] epoch: 0, batch: 3809 train-loss: 0.9313815832138062\n",
      "[LOG 20200511-10:20:58] epoch: 0, batch: 3810 train-loss: 1.403109073638916\n",
      "[LOG 20200511-10:20:58] epoch: 0, batch: 3811 train-loss: 1.5326745510101318\n",
      "[LOG 20200511-10:20:59] epoch: 0, batch: 3812 train-loss: 1.007921576499939\n",
      "[LOG 20200511-10:20:59] epoch: 0, batch: 3813 train-loss: 1.5858982801437378\n",
      "[LOG 20200511-10:20:59] epoch: 0, batch: 3814 train-loss: 2.0672035217285156\n",
      "[LOG 20200511-10:20:59] epoch: 0, batch: 3815 train-loss: 0.6479994058609009\n",
      "[LOG 20200511-10:20:59] epoch: 0, batch: 3816 train-loss: 1.2767196893692017\n",
      "[LOG 20200511-10:20:59] epoch: 0, batch: 3817 train-loss: 1.3643301725387573\n",
      "[LOG 20200511-10:20:59] epoch: 0, batch: 3818 train-loss: 1.4422070980072021\n",
      "[LOG 20200511-10:20:59] epoch: 0, batch: 3819 train-loss: 1.3024592399597168\n",
      "[LOG 20200511-10:21:00] epoch: 0, batch: 3820 train-loss: 1.3038164377212524\n",
      "[LOG 20200511-10:21:00] epoch: 0, batch: 3821 train-loss: 1.741318941116333\n",
      "[LOG 20200511-10:21:00] epoch: 0, batch: 3822 train-loss: 1.276506781578064\n",
      "[LOG 20200511-10:21:00] epoch: 0, batch: 3823 train-loss: 2.321962356567383\n",
      "[LOG 20200511-10:21:00] epoch: 0, batch: 3824 train-loss: 1.3288545608520508\n",
      "[LOG 20200511-10:21:00] epoch: 0, batch: 3825 train-loss: 1.1337001323699951\n",
      "[LOG 20200511-10:21:00] epoch: 0, batch: 3826 train-loss: 0.8818960189819336\n",
      "[LOG 20200511-10:21:00] epoch: 0, batch: 3827 train-loss: 0.8928726315498352\n",
      "[LOG 20200511-10:21:00] epoch: 0, batch: 3828 train-loss: 0.8149755001068115\n",
      "[LOG 20200511-10:21:01] epoch: 0, batch: 3829 train-loss: 1.1406147480010986\n",
      "[LOG 20200511-10:21:01] epoch: 0, batch: 3830 train-loss: 1.0435484647750854\n",
      "[LOG 20200511-10:21:01] epoch: 0, batch: 3831 train-loss: 1.8451993465423584\n",
      "[LOG 20200511-10:21:01] epoch: 0, batch: 3832 train-loss: 1.1311280727386475\n",
      "[LOG 20200511-10:21:01] epoch: 0, batch: 3833 train-loss: 1.3312538862228394\n",
      "[LOG 20200511-10:21:01] epoch: 0, batch: 3834 train-loss: 1.4851856231689453\n",
      "[LOG 20200511-10:21:01] epoch: 0, batch: 3835 train-loss: 1.3428014516830444\n",
      "[LOG 20200511-10:21:01] epoch: 0, batch: 3836 train-loss: 2.3237733840942383\n",
      "[LOG 20200511-10:21:02] epoch: 0, batch: 3837 train-loss: 1.0433326959609985\n",
      "[LOG 20200511-10:21:02] epoch: 0, batch: 3838 train-loss: 0.949553370475769\n",
      "[LOG 20200511-10:21:02] epoch: 0, batch: 3839 train-loss: 1.8144134283065796\n",
      "[LOG 20200511-10:21:02] epoch: 0, batch: 3840 train-loss: 1.0985815525054932\n",
      "[LOG 20200511-10:21:02] epoch: 0, batch: 3841 train-loss: 2.281790256500244\n",
      "[LOG 20200511-10:21:02] epoch: 0, batch: 3842 train-loss: 1.4616589546203613\n",
      "[LOG 20200511-10:21:02] epoch: 0, batch: 3843 train-loss: 2.7432637214660645\n",
      "[LOG 20200511-10:21:02] epoch: 0, batch: 3844 train-loss: 1.3229196071624756\n",
      "[LOG 20200511-10:21:03] epoch: 0, batch: 3845 train-loss: 2.424776554107666\n",
      "[LOG 20200511-10:21:03] epoch: 0, batch: 3846 train-loss: 1.47776460647583\n",
      "[LOG 20200511-10:21:03] epoch: 0, batch: 3847 train-loss: 1.8438133001327515\n",
      "[LOG 20200511-10:21:03] epoch: 0, batch: 3848 train-loss: 2.0093765258789062\n",
      "[LOG 20200511-10:21:03] epoch: 0, batch: 3849 train-loss: 2.128108024597168\n",
      "[LOG 20200511-10:21:03] epoch: 0, batch: 3850 train-loss: 1.1119818687438965\n",
      "[LOG 20200511-10:21:03] epoch: 0, batch: 3851 train-loss: 1.2583955526351929\n",
      "[LOG 20200511-10:21:03] epoch: 0, batch: 3852 train-loss: 1.0750514268875122\n",
      "[LOG 20200511-10:21:04] epoch: 0, batch: 3853 train-loss: 1.2721437215805054\n",
      "[LOG 20200511-10:21:04] epoch: 0, batch: 3854 train-loss: 1.3324319124221802\n",
      "[LOG 20200511-10:21:04] epoch: 0, batch: 3855 train-loss: 1.2797884941101074\n",
      "[LOG 20200511-10:21:04] epoch: 0, batch: 3856 train-loss: 1.9439196586608887\n",
      "[LOG 20200511-10:21:04] epoch: 0, batch: 3857 train-loss: 1.4992318153381348\n",
      "[LOG 20200511-10:21:04] epoch: 0, batch: 3858 train-loss: 1.1146125793457031\n",
      "[LOG 20200511-10:21:04] epoch: 0, batch: 3859 train-loss: 1.3518452644348145\n",
      "[LOG 20200511-10:21:04] epoch: 0, batch: 3860 train-loss: 1.6765332221984863\n",
      "[LOG 20200511-10:21:04] epoch: 0, batch: 3861 train-loss: 1.1635892391204834\n",
      "[LOG 20200511-10:21:05] epoch: 0, batch: 3862 train-loss: 2.0298027992248535\n",
      "[LOG 20200511-10:21:05] epoch: 0, batch: 3863 train-loss: 1.0013468265533447\n",
      "[LOG 20200511-10:21:05] epoch: 0, batch: 3864 train-loss: 1.2319344282150269\n",
      "[LOG 20200511-10:21:05] epoch: 0, batch: 3865 train-loss: 1.6760060787200928\n",
      "[LOG 20200511-10:21:05] epoch: 0, batch: 3866 train-loss: 2.2998456954956055\n",
      "[LOG 20200511-10:21:05] epoch: 0, batch: 3867 train-loss: 1.1241650581359863\n",
      "[LOG 20200511-10:21:05] epoch: 0, batch: 3868 train-loss: 0.6980229616165161\n",
      "[LOG 20200511-10:21:05] epoch: 0, batch: 3869 train-loss: 0.728406548500061\n",
      "[LOG 20200511-10:21:05] epoch: 0, batch: 3870 train-loss: 1.1068733930587769\n",
      "[LOG 20200511-10:21:06] epoch: 0, batch: 3871 train-loss: 1.220877766609192\n",
      "[LOG 20200511-10:21:06] epoch: 0, batch: 3872 train-loss: 2.0989692211151123\n",
      "[LOG 20200511-10:21:06] epoch: 0, batch: 3873 train-loss: 1.453588843345642\n",
      "[LOG 20200511-10:21:06] epoch: 0, batch: 3874 train-loss: 0.7406167387962341\n",
      "[LOG 20200511-10:21:06] epoch: 0, batch: 3875 train-loss: 2.2627944946289062\n",
      "[LOG 20200511-10:21:06] epoch: 0, batch: 3876 train-loss: 1.0741355419158936\n",
      "[LOG 20200511-10:21:06] epoch: 0, batch: 3877 train-loss: 1.1483711004257202\n",
      "[LOG 20200511-10:21:06] epoch: 0, batch: 3878 train-loss: 1.1541621685028076\n",
      "[LOG 20200511-10:21:07] epoch: 0, batch: 3879 train-loss: 1.6134082078933716\n",
      "[LOG 20200511-10:21:07] epoch: 0, batch: 3880 train-loss: 1.4346487522125244\n",
      "[LOG 20200511-10:21:07] epoch: 0, batch: 3881 train-loss: 1.5302282571792603\n",
      "[LOG 20200511-10:21:07] epoch: 0, batch: 3882 train-loss: 1.3529274463653564\n",
      "[LOG 20200511-10:21:07] epoch: 0, batch: 3883 train-loss: 0.6585444211959839\n",
      "[LOG 20200511-10:21:07] epoch: 0, batch: 3884 train-loss: 0.7420743107795715\n",
      "[LOG 20200511-10:21:07] epoch: 0, batch: 3885 train-loss: 1.5338761806488037\n",
      "[LOG 20200511-10:21:07] epoch: 0, batch: 3886 train-loss: 0.9176448583602905\n",
      "[LOG 20200511-10:21:08] epoch: 0, batch: 3887 train-loss: 1.7679600715637207\n",
      "[LOG 20200511-10:21:08] epoch: 0, batch: 3888 train-loss: 1.3601430654525757\n",
      "[LOG 20200511-10:21:08] epoch: 0, batch: 3889 train-loss: 0.6631233096122742\n",
      "[LOG 20200511-10:21:08] epoch: 0, batch: 3890 train-loss: 1.8444738388061523\n",
      "[LOG 20200511-10:21:08] epoch: 0, batch: 3891 train-loss: 1.1749845743179321\n",
      "[LOG 20200511-10:21:08] epoch: 0, batch: 3892 train-loss: 1.45651376247406\n",
      "[LOG 20200511-10:21:08] epoch: 0, batch: 3893 train-loss: 1.3328137397766113\n",
      "[LOG 20200511-10:21:08] epoch: 0, batch: 3894 train-loss: 0.7638928890228271\n",
      "[LOG 20200511-10:21:09] epoch: 0, batch: 3895 train-loss: 1.2332372665405273\n",
      "[LOG 20200511-10:21:09] epoch: 0, batch: 3896 train-loss: 1.986013650894165\n",
      "[LOG 20200511-10:21:09] epoch: 0, batch: 3897 train-loss: 1.3199894428253174\n",
      "[LOG 20200511-10:21:09] epoch: 0, batch: 3898 train-loss: 1.0739365816116333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:21:09] epoch: 0, batch: 3899 train-loss: 0.5383800268173218\n",
      "[LOG 20200511-10:21:09] epoch: 0, batch: 3900 train-loss: 1.2088607549667358\n",
      "[LOG 20200511-10:21:09] epoch: 0, batch: 3901 train-loss: 1.5487028360366821\n",
      "[LOG 20200511-10:21:09] epoch: 0, batch: 3902 train-loss: 1.9694578647613525\n",
      "[LOG 20200511-10:21:09] epoch: 0, batch: 3903 train-loss: 1.457407832145691\n",
      "[LOG 20200511-10:21:10] epoch: 0, batch: 3904 train-loss: 1.7185077667236328\n",
      "[LOG 20200511-10:21:10] epoch: 0, batch: 3905 train-loss: 1.8504878282546997\n",
      "[LOG 20200511-10:21:10] epoch: 0, batch: 3906 train-loss: 1.8069673776626587\n",
      "[LOG 20200511-10:21:10] epoch: 0, batch: 3907 train-loss: 1.3861448764801025\n",
      "[LOG 20200511-10:21:10] epoch: 0, batch: 3908 train-loss: 2.186211109161377\n",
      "[LOG 20200511-10:21:10] epoch: 0, batch: 3909 train-loss: 1.9910778999328613\n",
      "[LOG 20200511-10:21:10] epoch: 0, batch: 3910 train-loss: 0.997516930103302\n",
      "[LOG 20200511-10:21:10] epoch: 0, batch: 3911 train-loss: 1.5903650522232056\n",
      "[LOG 20200511-10:21:11] epoch: 0, batch: 3912 train-loss: 1.694894552230835\n",
      "[LOG 20200511-10:21:11] epoch: 0, batch: 3913 train-loss: 0.7764649987220764\n",
      "[LOG 20200511-10:21:11] epoch: 0, batch: 3914 train-loss: 1.959957242012024\n",
      "[LOG 20200511-10:21:11] epoch: 0, batch: 3915 train-loss: 0.7686278820037842\n",
      "[LOG 20200511-10:21:11] epoch: 0, batch: 3916 train-loss: 1.1894111633300781\n",
      "[LOG 20200511-10:21:11] epoch: 0, batch: 3917 train-loss: 0.9553850293159485\n",
      "[LOG 20200511-10:21:11] epoch: 0, batch: 3918 train-loss: 1.6657761335372925\n",
      "[LOG 20200511-10:21:11] epoch: 0, batch: 3919 train-loss: 2.0443429946899414\n",
      "[LOG 20200511-10:21:12] epoch: 0, batch: 3920 train-loss: 1.1134178638458252\n",
      "[LOG 20200511-10:21:12] epoch: 0, batch: 3921 train-loss: 1.2435392141342163\n",
      "[LOG 20200511-10:21:12] epoch: 0, batch: 3922 train-loss: 1.1726776361465454\n",
      "[LOG 20200511-10:21:12] epoch: 0, batch: 3923 train-loss: 1.6626609563827515\n",
      "[LOG 20200511-10:21:12] epoch: 0, batch: 3924 train-loss: 1.3117103576660156\n",
      "[LOG 20200511-10:21:12] epoch: 0, batch: 3925 train-loss: 1.1254961490631104\n",
      "[LOG 20200511-10:21:12] epoch: 0, batch: 3926 train-loss: 2.158247947692871\n",
      "[LOG 20200511-10:21:12] epoch: 0, batch: 3927 train-loss: 1.5825462341308594\n",
      "[LOG 20200511-10:21:13] epoch: 0, batch: 3928 train-loss: 1.679699182510376\n",
      "[LOG 20200511-10:21:13] epoch: 0, batch: 3929 train-loss: 1.7097148895263672\n",
      "[LOG 20200511-10:21:13] epoch: 0, batch: 3930 train-loss: 1.7312719821929932\n",
      "[LOG 20200511-10:21:13] epoch: 0, batch: 3931 train-loss: 1.5039870738983154\n",
      "[LOG 20200511-10:21:13] epoch: 0, batch: 3932 train-loss: 0.8872851729393005\n",
      "[LOG 20200511-10:21:13] epoch: 0, batch: 3933 train-loss: 2.5299174785614014\n",
      "[LOG 20200511-10:21:13] epoch: 0, batch: 3934 train-loss: 0.7170201539993286\n",
      "[LOG 20200511-10:21:13] epoch: 0, batch: 3935 train-loss: 1.5909619331359863\n",
      "[LOG 20200511-10:21:14] epoch: 0, batch: 3936 train-loss: 1.7233860492706299\n",
      "[LOG 20200511-10:21:14] epoch: 0, batch: 3937 train-loss: 2.4990241527557373\n",
      "[LOG 20200511-10:21:14] epoch: 0, batch: 3938 train-loss: 1.6183427572250366\n",
      "[LOG 20200511-10:21:14] epoch: 0, batch: 3939 train-loss: 1.5973941087722778\n",
      "[LOG 20200511-10:21:14] epoch: 0, batch: 3940 train-loss: 1.4364408254623413\n",
      "[LOG 20200511-10:21:14] epoch: 0, batch: 3941 train-loss: 1.2895901203155518\n",
      "[LOG 20200511-10:21:14] epoch: 0, batch: 3942 train-loss: 0.6367334127426147\n",
      "[LOG 20200511-10:21:14] epoch: 0, batch: 3943 train-loss: 1.191480278968811\n",
      "[LOG 20200511-10:21:15] epoch: 0, batch: 3944 train-loss: 1.2080316543579102\n",
      "[LOG 20200511-10:21:15] epoch: 0, batch: 3945 train-loss: 1.4215528964996338\n",
      "[LOG 20200511-10:21:15] epoch: 0, batch: 3946 train-loss: 1.9467800855636597\n",
      "[LOG 20200511-10:21:15] epoch: 0, batch: 3947 train-loss: 0.9317689538002014\n",
      "[LOG 20200511-10:21:15] epoch: 0, batch: 3948 train-loss: 2.482530117034912\n",
      "[LOG 20200511-10:21:15] epoch: 0, batch: 3949 train-loss: 0.9070376753807068\n",
      "[LOG 20200511-10:21:15] epoch: 0, batch: 3950 train-loss: 1.6237879991531372\n",
      "[LOG 20200511-10:21:15] epoch: 0, batch: 3951 train-loss: 0.6122910976409912\n",
      "[LOG 20200511-10:21:16] epoch: 0, batch: 3952 train-loss: 1.5012987852096558\n",
      "[LOG 20200511-10:21:16] epoch: 0, batch: 3953 train-loss: 2.0340464115142822\n",
      "[LOG 20200511-10:21:16] epoch: 0, batch: 3954 train-loss: 1.9202632904052734\n",
      "[LOG 20200511-10:21:16] epoch: 0, batch: 3955 train-loss: 1.4224902391433716\n",
      "[LOG 20200511-10:21:16] epoch: 0, batch: 3956 train-loss: 1.08133065700531\n",
      "[LOG 20200511-10:21:16] epoch: 0, batch: 3957 train-loss: 0.8845339417457581\n",
      "[LOG 20200511-10:21:16] epoch: 0, batch: 3958 train-loss: 2.7420568466186523\n",
      "[LOG 20200511-10:21:16] epoch: 0, batch: 3959 train-loss: 1.2268869876861572\n",
      "[LOG 20200511-10:21:16] epoch: 0, batch: 3960 train-loss: 1.5155999660491943\n",
      "[LOG 20200511-10:21:17] epoch: 0, batch: 3961 train-loss: 1.7802562713623047\n",
      "[LOG 20200511-10:21:17] epoch: 0, batch: 3962 train-loss: 0.693461537361145\n",
      "[LOG 20200511-10:21:17] epoch: 0, batch: 3963 train-loss: 2.7045373916625977\n",
      "[LOG 20200511-10:21:17] epoch: 0, batch: 3964 train-loss: 1.4181886911392212\n",
      "[LOG 20200511-10:21:17] epoch: 0, batch: 3965 train-loss: 1.4932286739349365\n",
      "[LOG 20200511-10:21:17] epoch: 0, batch: 3966 train-loss: 1.3707576990127563\n",
      "[LOG 20200511-10:21:17] epoch: 0, batch: 3967 train-loss: 0.7824957966804504\n",
      "[LOG 20200511-10:21:17] epoch: 0, batch: 3968 train-loss: 1.3166950941085815\n",
      "[LOG 20200511-10:21:17] epoch: 0, batch: 3969 train-loss: 1.5482807159423828\n",
      "[LOG 20200511-10:21:18] epoch: 0, batch: 3970 train-loss: 1.31309175491333\n",
      "[LOG 20200511-10:21:18] epoch: 0, batch: 3971 train-loss: 2.3702056407928467\n",
      "[LOG 20200511-10:21:18] epoch: 0, batch: 3972 train-loss: 1.7912402153015137\n",
      "[LOG 20200511-10:21:18] epoch: 0, batch: 3973 train-loss: 1.6419129371643066\n",
      "[LOG 20200511-10:21:18] epoch: 0, batch: 3974 train-loss: 1.9524047374725342\n",
      "[LOG 20200511-10:21:18] epoch: 0, batch: 3975 train-loss: 1.8434200286865234\n",
      "[LOG 20200511-10:21:18] epoch: 0, batch: 3976 train-loss: 1.72575843334198\n",
      "[LOG 20200511-10:21:19] epoch: 0, batch: 3977 train-loss: 2.2309062480926514\n",
      "[LOG 20200511-10:21:19] epoch: 0, batch: 3978 train-loss: 0.6526663303375244\n",
      "[LOG 20200511-10:21:19] epoch: 0, batch: 3979 train-loss: 1.6050629615783691\n",
      "[LOG 20200511-10:21:19] epoch: 0, batch: 3980 train-loss: 0.9826480150222778\n",
      "[LOG 20200511-10:21:19] epoch: 0, batch: 3981 train-loss: 0.6953664422035217\n",
      "[LOG 20200511-10:21:19] epoch: 0, batch: 3982 train-loss: 1.3193072080612183\n",
      "[LOG 20200511-10:21:19] epoch: 0, batch: 3983 train-loss: 1.458314299583435\n",
      "[LOG 20200511-10:21:19] epoch: 0, batch: 3984 train-loss: 1.460355281829834\n",
      "[LOG 20200511-10:21:19] epoch: 0, batch: 3985 train-loss: 1.9322253465652466\n",
      "[LOG 20200511-10:21:20] epoch: 0, batch: 3986 train-loss: 1.4607899188995361\n",
      "[LOG 20200511-10:21:20] epoch: 0, batch: 3987 train-loss: 0.8553950190544128\n",
      "[LOG 20200511-10:21:20] epoch: 0, batch: 3988 train-loss: 0.9000161290168762\n",
      "[LOG 20200511-10:21:20] epoch: 0, batch: 3989 train-loss: 1.2588119506835938\n",
      "[LOG 20200511-10:21:20] epoch: 0, batch: 3990 train-loss: 1.1079466342926025\n",
      "[LOG 20200511-10:21:20] epoch: 0, batch: 3991 train-loss: 2.0069422721862793\n",
      "[LOG 20200511-10:21:20] epoch: 0, batch: 3992 train-loss: 1.1248544454574585\n",
      "[LOG 20200511-10:21:20] epoch: 0, batch: 3993 train-loss: 1.6223840713500977\n",
      "[LOG 20200511-10:21:20] epoch: 0, batch: 3994 train-loss: 2.130948305130005\n",
      "[LOG 20200511-10:21:21] epoch: 0, batch: 3995 train-loss: 2.1888954639434814\n",
      "[LOG 20200511-10:21:21] epoch: 0, batch: 3996 train-loss: 0.9483464956283569\n",
      "[LOG 20200511-10:21:21] epoch: 0, batch: 3997 train-loss: 1.6297367811203003\n",
      "[LOG 20200511-10:21:21] epoch: 0, batch: 3998 train-loss: 0.5215772390365601\n",
      "[LOG 20200511-10:21:21] epoch: 0, batch: 3999 train-loss: 2.059657096862793\n",
      "[LOG 20200511-10:21:21] epoch: 0, batch: 4000 train-loss: 0.860977053642273\n",
      "[LOG 20200511-10:21:21] epoch: 0, batch: 4001 train-loss: 1.6320006847381592\n",
      "[LOG 20200511-10:21:21] epoch: 0, batch: 4002 train-loss: 1.942762851715088\n",
      "[LOG 20200511-10:21:21] epoch: 0, batch: 4003 train-loss: 2.146841049194336\n",
      "[LOG 20200511-10:21:21] epoch: 0, batch: 4004 train-loss: 1.1273863315582275\n",
      "[LOG 20200511-10:21:22] epoch: 0, batch: 4005 train-loss: 2.1295433044433594\n",
      "[LOG 20200511-10:21:22] epoch: 0, batch: 4006 train-loss: 0.8119217157363892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:21:22] epoch: 0, batch: 4007 train-loss: 1.8285434246063232\n",
      "[LOG 20200511-10:21:22] epoch: 0, batch: 4008 train-loss: 1.8538472652435303\n",
      "[LOG 20200511-10:21:22] epoch: 0, batch: 4009 train-loss: 0.9880653023719788\n",
      "[LOG 20200511-10:21:22] epoch: 0, batch: 4010 train-loss: 1.2752047777175903\n",
      "[LOG 20200511-10:21:22] epoch: 0, batch: 4011 train-loss: 1.992714285850525\n",
      "[LOG 20200511-10:21:22] epoch: 0, batch: 4012 train-loss: 1.2719546556472778\n",
      "[LOG 20200511-10:21:22] epoch: 0, batch: 4013 train-loss: 1.7652145624160767\n",
      "[LOG 20200511-10:21:23] epoch: 0, batch: 4014 train-loss: 1.7464059591293335\n",
      "[LOG 20200511-10:21:23] epoch: 0, batch: 4015 train-loss: 1.670220971107483\n",
      "[LOG 20200511-10:21:23] epoch: 0, batch: 4016 train-loss: 0.8998217582702637\n",
      "[LOG 20200511-10:21:23] epoch: 0, batch: 4017 train-loss: 2.193202018737793\n",
      "[LOG 20200511-10:21:23] epoch: 0, batch: 4018 train-loss: 1.2453935146331787\n",
      "[LOG 20200511-10:21:23] epoch: 0, batch: 4019 train-loss: 2.8722074031829834\n",
      "[LOG 20200511-10:21:23] epoch: 0, batch: 4020 train-loss: 1.3890511989593506\n",
      "[LOG 20200511-10:21:23] epoch: 0, batch: 4021 train-loss: 1.7998075485229492\n",
      "[LOG 20200511-10:21:23] epoch: 0, batch: 4022 train-loss: 1.3956527709960938\n",
      "[LOG 20200511-10:21:24] epoch: 0, batch: 4023 train-loss: 1.3067854642868042\n",
      "[LOG 20200511-10:21:24] epoch: 0, batch: 4024 train-loss: 1.1087661981582642\n",
      "[LOG 20200511-10:21:24] epoch: 0, batch: 4025 train-loss: 1.5093525648117065\n",
      "[LOG 20200511-10:21:24] epoch: 0, batch: 4026 train-loss: 1.8421025276184082\n",
      "[LOG 20200511-10:21:24] epoch: 0, batch: 4027 train-loss: 1.6844466924667358\n",
      "[LOG 20200511-10:21:24] epoch: 0, batch: 4028 train-loss: 1.223472237586975\n",
      "[LOG 20200511-10:21:24] epoch: 0, batch: 4029 train-loss: 1.1947681903839111\n",
      "[LOG 20200511-10:21:24] epoch: 0, batch: 4030 train-loss: 0.6298378109931946\n",
      "[LOG 20200511-10:21:24] epoch: 0, batch: 4031 train-loss: 1.187536597251892\n",
      "[LOG 20200511-10:21:25] epoch: 0, batch: 4032 train-loss: 1.5411479473114014\n",
      "[LOG 20200511-10:21:25] epoch: 0, batch: 4033 train-loss: 0.4864623546600342\n",
      "[LOG 20200511-10:21:25] epoch: 0, batch: 4034 train-loss: 1.6008062362670898\n",
      "[LOG 20200511-10:21:25] epoch: 0, batch: 4035 train-loss: 1.0185717344284058\n",
      "[LOG 20200511-10:21:25] epoch: 0, batch: 4036 train-loss: 1.1351765394210815\n",
      "[LOG 20200511-10:21:25] epoch: 0, batch: 4037 train-loss: 1.181647539138794\n",
      "[LOG 20200511-10:21:25] epoch: 0, batch: 4038 train-loss: 2.3640267848968506\n",
      "[LOG 20200511-10:21:25] epoch: 0, batch: 4039 train-loss: 2.1104423999786377\n",
      "[LOG 20200511-10:21:25] epoch: 0, batch: 4040 train-loss: 2.0799195766448975\n",
      "[LOG 20200511-10:21:26] epoch: 0, batch: 4041 train-loss: 1.3182141780853271\n",
      "[LOG 20200511-10:21:26] epoch: 0, batch: 4042 train-loss: 1.7243376970291138\n",
      "[LOG 20200511-10:21:26] epoch: 0, batch: 4043 train-loss: 2.215329885482788\n",
      "[LOG 20200511-10:21:26] epoch: 0, batch: 4044 train-loss: 1.3874622583389282\n",
      "[LOG 20200511-10:21:26] epoch: 0, batch: 4045 train-loss: 1.8653950691223145\n",
      "[LOG 20200511-10:21:26] epoch: 0, batch: 4046 train-loss: 1.5075733661651611\n",
      "[LOG 20200511-10:21:26] epoch: 0, batch: 4047 train-loss: 2.059823513031006\n",
      "[LOG 20200511-10:21:26] epoch: 0, batch: 4048 train-loss: 0.68142169713974\n",
      "[LOG 20200511-10:21:27] epoch: 0, batch: 4049 train-loss: 1.339094638824463\n",
      "[LOG 20200511-10:21:27] epoch: 0, batch: 4050 train-loss: 1.09968101978302\n",
      "[LOG 20200511-10:21:27] epoch: 0, batch: 4051 train-loss: 0.9934983253479004\n",
      "[LOG 20200511-10:21:27] epoch: 0, batch: 4052 train-loss: 1.1199818849563599\n",
      "[LOG 20200511-10:21:27] epoch: 0, batch: 4053 train-loss: 1.4248305559158325\n",
      "[LOG 20200511-10:21:27] epoch: 0, batch: 4054 train-loss: 2.2794837951660156\n",
      "[LOG 20200511-10:21:27] epoch: 0, batch: 4055 train-loss: 1.5348490476608276\n",
      "[LOG 20200511-10:21:27] epoch: 0, batch: 4056 train-loss: 1.4224907159805298\n",
      "[LOG 20200511-10:21:28] epoch: 0, batch: 4057 train-loss: 1.9904917478561401\n",
      "[LOG 20200511-10:21:28] epoch: 0, batch: 4058 train-loss: 1.386800765991211\n",
      "[LOG 20200511-10:21:28] epoch: 0, batch: 4059 train-loss: 2.6793417930603027\n",
      "[LOG 20200511-10:21:28] epoch: 0, batch: 4060 train-loss: 0.8822625875473022\n",
      "[LOG 20200511-10:21:28] epoch: 0, batch: 4061 train-loss: 1.2612136602401733\n",
      "[LOG 20200511-10:21:28] epoch: 0, batch: 4062 train-loss: 0.7918904423713684\n",
      "[LOG 20200511-10:21:28] epoch: 0, batch: 4063 train-loss: 0.8454545736312866\n",
      "[LOG 20200511-10:21:28] epoch: 0, batch: 4064 train-loss: 1.5109314918518066\n",
      "[LOG 20200511-10:21:29] epoch: 0, batch: 4065 train-loss: 1.3967361450195312\n",
      "[LOG 20200511-10:21:29] epoch: 0, batch: 4066 train-loss: 1.1071966886520386\n",
      "[LOG 20200511-10:21:29] epoch: 0, batch: 4067 train-loss: 2.4756476879119873\n",
      "[LOG 20200511-10:21:29] epoch: 0, batch: 4068 train-loss: 2.23254132270813\n",
      "[LOG 20200511-10:21:29] epoch: 0, batch: 4069 train-loss: 1.470687985420227\n",
      "[LOG 20200511-10:21:29] epoch: 0, batch: 4070 train-loss: 1.0243101119995117\n",
      "[LOG 20200511-10:21:29] epoch: 0, batch: 4071 train-loss: 2.5086958408355713\n",
      "[LOG 20200511-10:21:29] epoch: 0, batch: 4072 train-loss: 0.9488920569419861\n",
      "[LOG 20200511-10:21:30] epoch: 0, batch: 4073 train-loss: 0.9579135179519653\n",
      "[LOG 20200511-10:21:30] epoch: 0, batch: 4074 train-loss: 1.4245600700378418\n",
      "[LOG 20200511-10:21:30] epoch: 0, batch: 4075 train-loss: 1.9779654741287231\n",
      "[LOG 20200511-10:21:30] epoch: 0, batch: 4076 train-loss: 1.2331123352050781\n",
      "[LOG 20200511-10:21:30] epoch: 0, batch: 4077 train-loss: 1.9144912958145142\n",
      "[LOG 20200511-10:21:30] epoch: 0, batch: 4078 train-loss: 0.9833263754844666\n",
      "[LOG 20200511-10:21:30] epoch: 0, batch: 4079 train-loss: 1.1982076168060303\n",
      "[LOG 20200511-10:21:30] epoch: 0, batch: 4080 train-loss: 2.104363441467285\n",
      "[LOG 20200511-10:21:30] epoch: 0, batch: 4081 train-loss: 1.833921194076538\n",
      "[LOG 20200511-10:21:31] epoch: 0, batch: 4082 train-loss: 1.7257428169250488\n",
      "[LOG 20200511-10:21:31] epoch: 0, batch: 4083 train-loss: 1.2591296434402466\n",
      "[LOG 20200511-10:21:31] epoch: 0, batch: 4084 train-loss: 1.1238415241241455\n",
      "[LOG 20200511-10:21:31] epoch: 0, batch: 4085 train-loss: 1.874845266342163\n",
      "[LOG 20200511-10:21:31] epoch: 0, batch: 4086 train-loss: 0.7275509834289551\n",
      "[LOG 20200511-10:21:31] epoch: 0, batch: 4087 train-loss: 1.025068998336792\n",
      "[LOG 20200511-10:21:31] epoch: 0, batch: 4088 train-loss: 1.4774742126464844\n",
      "[LOG 20200511-10:21:31] epoch: 0, batch: 4089 train-loss: 1.7810124158859253\n",
      "[LOG 20200511-10:21:31] epoch: 0, batch: 4090 train-loss: 1.19223153591156\n",
      "[LOG 20200511-10:21:32] epoch: 0, batch: 4091 train-loss: 2.1430771350860596\n",
      "[LOG 20200511-10:21:32] epoch: 0, batch: 4092 train-loss: 1.0439915657043457\n",
      "[LOG 20200511-10:21:32] epoch: 0, batch: 4093 train-loss: 1.7412296533584595\n",
      "[LOG 20200511-10:21:32] epoch: 0, batch: 4094 train-loss: 2.307278633117676\n",
      "[LOG 20200511-10:21:32] epoch: 0, batch: 4095 train-loss: 0.9952277541160583\n",
      "[LOG 20200511-10:21:32] epoch: 0, batch: 4096 train-loss: 1.0444921255111694\n",
      "[LOG 20200511-10:21:32] epoch: 0, batch: 4097 train-loss: 1.7800986766815186\n",
      "[LOG 20200511-10:21:32] epoch: 0, batch: 4098 train-loss: 0.934170126914978\n",
      "[LOG 20200511-10:21:32] epoch: 0, batch: 4099 train-loss: 1.3544141054153442\n",
      "[LOG 20200511-10:21:33] epoch: 0, batch: 4100 train-loss: 0.5903230309486389\n",
      "[LOG 20200511-10:21:33] epoch: 0, batch: 4101 train-loss: 2.869600534439087\n",
      "[LOG 20200511-10:21:33] epoch: 0, batch: 4102 train-loss: 1.1521438360214233\n",
      "[LOG 20200511-10:21:33] epoch: 0, batch: 4103 train-loss: 1.9234461784362793\n",
      "[LOG 20200511-10:21:33] epoch: 0, batch: 4104 train-loss: 1.1802170276641846\n",
      "[LOG 20200511-10:21:33] epoch: 0, batch: 4105 train-loss: 0.9296590685844421\n",
      "[LOG 20200511-10:21:33] epoch: 0, batch: 4106 train-loss: 2.468442440032959\n",
      "[LOG 20200511-10:21:33] epoch: 0, batch: 4107 train-loss: 1.0074483156204224\n",
      "[LOG 20200511-10:21:33] epoch: 0, batch: 4108 train-loss: 1.3979716300964355\n",
      "[LOG 20200511-10:21:34] epoch: 0, batch: 4109 train-loss: 1.970563530921936\n",
      "[LOG 20200511-10:21:34] epoch: 0, batch: 4110 train-loss: 2.1436409950256348\n",
      "[LOG 20200511-10:21:34] epoch: 0, batch: 4111 train-loss: 1.989454746246338\n",
      "[LOG 20200511-10:21:34] epoch: 0, batch: 4112 train-loss: 1.4451110363006592\n",
      "[LOG 20200511-10:21:34] epoch: 0, batch: 4113 train-loss: 1.2591809034347534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:21:34] epoch: 0, batch: 4114 train-loss: 1.4640815258026123\n",
      "[LOG 20200511-10:21:34] epoch: 0, batch: 4115 train-loss: 2.888279438018799\n",
      "[LOG 20200511-10:21:34] epoch: 0, batch: 4116 train-loss: 1.1754708290100098\n",
      "[LOG 20200511-10:21:34] epoch: 0, batch: 4117 train-loss: 1.1627016067504883\n",
      "[LOG 20200511-10:21:34] epoch: 0, batch: 4118 train-loss: 1.039508581161499\n",
      "[LOG 20200511-10:21:35] epoch: 0, batch: 4119 train-loss: 1.9761675596237183\n",
      "[LOG 20200511-10:21:35] epoch: 0, batch: 4120 train-loss: 1.5098642110824585\n",
      "[LOG 20200511-10:21:35] epoch: 0, batch: 4121 train-loss: 1.7074229717254639\n",
      "[LOG 20200511-10:21:35] epoch: 0, batch: 4122 train-loss: 1.6367576122283936\n",
      "[LOG 20200511-10:21:35] epoch: 0, batch: 4123 train-loss: 1.9413079023361206\n",
      "[LOG 20200511-10:21:35] epoch: 0, batch: 4124 train-loss: 1.0839965343475342\n",
      "[LOG 20200511-10:21:35] epoch: 0, batch: 4125 train-loss: 0.9646753072738647\n",
      "[LOG 20200511-10:21:35] epoch: 0, batch: 4126 train-loss: 1.148728847503662\n",
      "[LOG 20200511-10:21:36] epoch: 0, batch: 4127 train-loss: 1.366013765335083\n",
      "[LOG 20200511-10:21:36] epoch: 0, batch: 4128 train-loss: 1.4663163423538208\n",
      "[LOG 20200511-10:21:36] epoch: 0, batch: 4129 train-loss: 1.1810786724090576\n",
      "[LOG 20200511-10:21:36] epoch: 0, batch: 4130 train-loss: 1.0816237926483154\n",
      "[LOG 20200511-10:21:36] epoch: 0, batch: 4131 train-loss: 1.373718023300171\n",
      "[LOG 20200511-10:21:36] epoch: 0, batch: 4132 train-loss: 1.6128844022750854\n",
      "[LOG 20200511-10:21:36] epoch: 0, batch: 4133 train-loss: 0.7250610589981079\n",
      "[LOG 20200511-10:21:36] epoch: 0, batch: 4134 train-loss: 1.659577488899231\n",
      "[LOG 20200511-10:21:37] epoch: 0, batch: 4135 train-loss: 1.3994792699813843\n",
      "[LOG 20200511-10:21:37] epoch: 0, batch: 4136 train-loss: 1.8416800498962402\n",
      "[LOG 20200511-10:21:37] epoch: 0, batch: 4137 train-loss: 1.1324305534362793\n",
      "[LOG 20200511-10:21:37] epoch: 0, batch: 4138 train-loss: 1.1423335075378418\n",
      "[LOG 20200511-10:21:37] epoch: 0, batch: 4139 train-loss: 0.7355661392211914\n",
      "[LOG 20200511-10:21:37] epoch: 0, batch: 4140 train-loss: 1.256543755531311\n",
      "[LOG 20200511-10:21:37] epoch: 0, batch: 4141 train-loss: 1.6792398691177368\n",
      "[LOG 20200511-10:21:37] epoch: 0, batch: 4142 train-loss: 2.7522177696228027\n",
      "[LOG 20200511-10:21:38] epoch: 0, batch: 4143 train-loss: 1.4710540771484375\n",
      "[LOG 20200511-10:21:38] epoch: 0, batch: 4144 train-loss: 1.4967563152313232\n",
      "[LOG 20200511-10:21:38] epoch: 0, batch: 4145 train-loss: 2.3503503799438477\n",
      "[LOG 20200511-10:21:38] epoch: 0, batch: 4146 train-loss: 1.212755799293518\n",
      "[LOG 20200511-10:21:38] epoch: 0, batch: 4147 train-loss: 1.966474175453186\n",
      "[LOG 20200511-10:21:38] epoch: 0, batch: 4148 train-loss: 1.1360461711883545\n",
      "[LOG 20200511-10:21:38] epoch: 0, batch: 4149 train-loss: 2.033498525619507\n",
      "[LOG 20200511-10:21:38] epoch: 0, batch: 4150 train-loss: 1.0583364963531494\n",
      "[LOG 20200511-10:21:38] epoch: 0, batch: 4151 train-loss: 2.266116142272949\n",
      "[LOG 20200511-10:21:39] epoch: 0, batch: 4152 train-loss: 1.034769892692566\n",
      "[LOG 20200511-10:21:39] epoch: 0, batch: 4153 train-loss: 1.656982183456421\n",
      "[LOG 20200511-10:21:39] epoch: 0, batch: 4154 train-loss: 0.9509027600288391\n",
      "[LOG 20200511-10:21:39] epoch: 0, batch: 4155 train-loss: 1.3777074813842773\n",
      "[LOG 20200511-10:21:39] epoch: 0, batch: 4156 train-loss: 1.2841441631317139\n",
      "[LOG 20200511-10:21:39] epoch: 0, batch: 4157 train-loss: 1.4890427589416504\n",
      "[LOG 20200511-10:21:39] epoch: 0, batch: 4158 train-loss: 1.2639410495758057\n",
      "[LOG 20200511-10:21:39] epoch: 0, batch: 4159 train-loss: 0.4943314790725708\n",
      "[LOG 20200511-10:21:40] epoch: 0, batch: 4160 train-loss: 1.3022674322128296\n",
      "[LOG 20200511-10:21:40] epoch: 0, batch: 4161 train-loss: 1.2033045291900635\n",
      "[LOG 20200511-10:21:40] epoch: 0, batch: 4162 train-loss: 1.3181341886520386\n",
      "[LOG 20200511-10:21:40] epoch: 0, batch: 4163 train-loss: 1.0940208435058594\n",
      "[LOG 20200511-10:21:40] epoch: 0, batch: 4164 train-loss: 1.9628126621246338\n",
      "[LOG 20200511-10:21:40] epoch: 0, batch: 4165 train-loss: 0.5349326729774475\n",
      "[LOG 20200511-10:21:40] epoch: 0, batch: 4166 train-loss: 1.2142224311828613\n",
      "[LOG 20200511-10:21:40] epoch: 0, batch: 4167 train-loss: 1.622106671333313\n",
      "[LOG 20200511-10:21:41] epoch: 0, batch: 4168 train-loss: 1.4871230125427246\n",
      "[LOG 20200511-10:21:41] epoch: 0, batch: 4169 train-loss: 1.0481274127960205\n",
      "[LOG 20200511-10:21:41] epoch: 0, batch: 4170 train-loss: 0.8865991830825806\n",
      "[LOG 20200511-10:21:41] epoch: 0, batch: 4171 train-loss: 0.9611900448799133\n",
      "[LOG 20200511-10:21:41] epoch: 0, batch: 4172 train-loss: 1.1822991371154785\n",
      "[LOG 20200511-10:21:41] epoch: 0, batch: 4173 train-loss: 2.0275707244873047\n",
      "[LOG 20200511-10:21:41] epoch: 0, batch: 4174 train-loss: 1.0032342672348022\n",
      "[LOG 20200511-10:21:41] epoch: 0, batch: 4175 train-loss: 0.9059565663337708\n",
      "[LOG 20200511-10:21:42] epoch: 0, batch: 4176 train-loss: 1.6474769115447998\n",
      "[LOG 20200511-10:21:42] epoch: 0, batch: 4177 train-loss: 0.9311511516571045\n",
      "[LOG 20200511-10:21:42] epoch: 0, batch: 4178 train-loss: 2.132517099380493\n",
      "[LOG 20200511-10:21:42] epoch: 0, batch: 4179 train-loss: 2.8586626052856445\n",
      "[LOG 20200511-10:21:42] epoch: 0, batch: 4180 train-loss: 2.2085087299346924\n",
      "[LOG 20200511-10:21:42] epoch: 0, batch: 4181 train-loss: 1.601273775100708\n",
      "[LOG 20200511-10:21:42] epoch: 0, batch: 4182 train-loss: 0.7986888289451599\n",
      "[LOG 20200511-10:21:42] epoch: 0, batch: 4183 train-loss: 1.6045558452606201\n",
      "[LOG 20200511-10:21:42] epoch: 0, batch: 4184 train-loss: 1.7550774812698364\n",
      "[LOG 20200511-10:21:43] epoch: 0, batch: 4185 train-loss: 1.2073475122451782\n",
      "[LOG 20200511-10:21:43] epoch: 0, batch: 4186 train-loss: 1.4028143882751465\n",
      "[LOG 20200511-10:21:43] epoch: 0, batch: 4187 train-loss: 1.4402409791946411\n",
      "[LOG 20200511-10:21:43] epoch: 0, batch: 4188 train-loss: 0.6586647033691406\n",
      "[LOG 20200511-10:21:43] epoch: 0, batch: 4189 train-loss: 1.2946579456329346\n",
      "[LOG 20200511-10:21:43] epoch: 0, batch: 4190 train-loss: 1.2900047302246094\n",
      "[LOG 20200511-10:21:43] epoch: 0, batch: 4191 train-loss: 1.5427322387695312\n",
      "[LOG 20200511-10:21:43] epoch: 0, batch: 4192 train-loss: 0.7632704973220825\n",
      "[LOG 20200511-10:21:43] epoch: 0, batch: 4193 train-loss: 1.737485408782959\n",
      "[LOG 20200511-10:21:44] epoch: 0, batch: 4194 train-loss: 2.120321750640869\n",
      "[LOG 20200511-10:21:44] epoch: 0, batch: 4195 train-loss: 1.2507256269454956\n",
      "[LOG 20200511-10:21:44] epoch: 0, batch: 4196 train-loss: 2.2184574604034424\n",
      "[LOG 20200511-10:21:44] epoch: 0, batch: 4197 train-loss: 2.6522157192230225\n",
      "[LOG 20200511-10:21:44] epoch: 0, batch: 4198 train-loss: 1.2802459001541138\n",
      "[LOG 20200511-10:21:44] epoch: 0, batch: 4199 train-loss: 1.3859308958053589\n",
      "[LOG 20200511-10:21:44] epoch: 0, batch: 4200 train-loss: 1.9066617488861084\n",
      "[LOG 20200511-10:21:44] epoch: 0, batch: 4201 train-loss: 1.7455368041992188\n",
      "[LOG 20200511-10:21:44] epoch: 0, batch: 4202 train-loss: 1.2322273254394531\n",
      "[LOG 20200511-10:21:45] epoch: 0, batch: 4203 train-loss: 1.210242509841919\n",
      "[LOG 20200511-10:21:45] epoch: 0, batch: 4204 train-loss: 1.4698703289031982\n",
      "[LOG 20200511-10:21:45] epoch: 0, batch: 4205 train-loss: 1.134937047958374\n",
      "[LOG 20200511-10:21:45] epoch: 0, batch: 4206 train-loss: 1.5454435348510742\n",
      "[LOG 20200511-10:21:45] epoch: 0, batch: 4207 train-loss: 1.874191403388977\n",
      "[LOG 20200511-10:21:45] epoch: 0, batch: 4208 train-loss: 1.655195951461792\n",
      "[LOG 20200511-10:21:45] epoch: 0, batch: 4209 train-loss: 1.3625162839889526\n",
      "[LOG 20200511-10:21:45] epoch: 0, batch: 4210 train-loss: 1.2275981903076172\n",
      "[LOG 20200511-10:21:46] epoch: 0, batch: 4211 train-loss: 1.47553551197052\n",
      "[LOG 20200511-10:21:46] epoch: 0, batch: 4212 train-loss: 1.5052680969238281\n",
      "[LOG 20200511-10:21:46] epoch: 0, batch: 4213 train-loss: 1.5848568677902222\n",
      "[LOG 20200511-10:21:46] epoch: 0, batch: 4214 train-loss: 1.6842551231384277\n",
      "[LOG 20200511-10:21:46] epoch: 0, batch: 4215 train-loss: 1.3634248971939087\n",
      "[LOG 20200511-10:21:46] epoch: 0, batch: 4216 train-loss: 1.7863280773162842\n",
      "[LOG 20200511-10:21:46] epoch: 0, batch: 4217 train-loss: 0.6456262469291687\n",
      "[LOG 20200511-10:21:46] epoch: 0, batch: 4218 train-loss: 1.463759183883667\n",
      "[LOG 20200511-10:21:47] epoch: 0, batch: 4219 train-loss: 1.8449841737747192\n",
      "[LOG 20200511-10:21:47] epoch: 0, batch: 4220 train-loss: 1.9544953107833862\n",
      "[LOG 20200511-10:21:47] epoch: 0, batch: 4221 train-loss: 2.0102479457855225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:21:47] epoch: 0, batch: 4222 train-loss: 0.7569904923439026\n",
      "[LOG 20200511-10:21:47] epoch: 0, batch: 4223 train-loss: 1.0704796314239502\n",
      "[LOG 20200511-10:21:47] epoch: 0, batch: 4224 train-loss: 0.8305360674858093\n",
      "[LOG 20200511-10:21:47] epoch: 0, batch: 4225 train-loss: 0.975537896156311\n",
      "[LOG 20200511-10:21:47] epoch: 0, batch: 4226 train-loss: 1.4359803199768066\n",
      "[LOG 20200511-10:21:48] epoch: 0, batch: 4227 train-loss: 1.1021696329116821\n",
      "[LOG 20200511-10:21:48] epoch: 0, batch: 4228 train-loss: 0.9738318920135498\n",
      "[LOG 20200511-10:21:48] epoch: 0, batch: 4229 train-loss: 1.5178260803222656\n",
      "[LOG 20200511-10:21:48] epoch: 0, batch: 4230 train-loss: 2.3237102031707764\n",
      "[LOG 20200511-10:21:48] epoch: 0, batch: 4231 train-loss: 0.8269152641296387\n",
      "[LOG 20200511-10:21:48] epoch: 0, batch: 4232 train-loss: 0.8155354857444763\n",
      "[LOG 20200511-10:21:48] epoch: 0, batch: 4233 train-loss: 1.4979784488677979\n",
      "[LOG 20200511-10:21:48] epoch: 0, batch: 4234 train-loss: 1.787421464920044\n",
      "[LOG 20200511-10:21:48] epoch: 0, batch: 4235 train-loss: 1.2562988996505737\n",
      "[LOG 20200511-10:21:49] epoch: 0, batch: 4236 train-loss: 1.0076769590377808\n",
      "[LOG 20200511-10:21:49] epoch: 0, batch: 4237 train-loss: 1.6180541515350342\n",
      "[LOG 20200511-10:21:49] epoch: 0, batch: 4238 train-loss: 1.6120270490646362\n",
      "[LOG 20200511-10:21:49] epoch: 0, batch: 4239 train-loss: 1.0557955503463745\n",
      "[LOG 20200511-10:21:49] epoch: 0, batch: 4240 train-loss: 1.654878854751587\n",
      "[LOG 20200511-10:21:49] epoch: 0, batch: 4241 train-loss: 1.4564566612243652\n",
      "[LOG 20200511-10:21:49] epoch: 0, batch: 4242 train-loss: 1.0829142332077026\n",
      "[LOG 20200511-10:21:49] epoch: 0, batch: 4243 train-loss: 2.0201125144958496\n",
      "[LOG 20200511-10:21:50] epoch: 0, batch: 4244 train-loss: 0.8918114304542542\n",
      "[LOG 20200511-10:21:50] epoch: 0, batch: 4245 train-loss: 1.3430545330047607\n",
      "[LOG 20200511-10:21:50] epoch: 0, batch: 4246 train-loss: 0.6797896027565002\n",
      "[LOG 20200511-10:21:50] epoch: 0, batch: 4247 train-loss: 1.8171725273132324\n",
      "[LOG 20200511-10:21:50] epoch: 0, batch: 4248 train-loss: 0.7716813087463379\n",
      "[LOG 20200511-10:21:50] epoch: 0, batch: 4249 train-loss: 0.44315749406814575\n",
      "[LOG 20200511-10:21:50] epoch: 0, batch: 4250 train-loss: 1.4328727722167969\n",
      "[LOG 20200511-10:21:50] epoch: 0, batch: 4251 train-loss: 1.7590621709823608\n",
      "[LOG 20200511-10:21:51] epoch: 0, batch: 4252 train-loss: 0.7754387855529785\n",
      "[LOG 20200511-10:21:51] epoch: 0, batch: 4253 train-loss: 1.634354829788208\n",
      "[LOG 20200511-10:21:51] epoch: 0, batch: 4254 train-loss: 1.2570955753326416\n",
      "[LOG 20200511-10:21:51] epoch: 0, batch: 4255 train-loss: 0.4297712445259094\n",
      "[LOG 20200511-10:21:51] epoch: 0, batch: 4256 train-loss: 2.9581942558288574\n",
      "[LOG 20200511-10:21:51] epoch: 0, batch: 4257 train-loss: 0.5189388990402222\n",
      "[LOG 20200511-10:21:51] epoch: 0, batch: 4258 train-loss: 0.7840140461921692\n",
      "[LOG 20200511-10:21:51] epoch: 0, batch: 4259 train-loss: 1.7884799242019653\n",
      "[LOG 20200511-10:21:51] epoch: 0, batch: 4260 train-loss: 0.9771081209182739\n",
      "[LOG 20200511-10:21:52] epoch: 0, batch: 4261 train-loss: 1.666959285736084\n",
      "[LOG 20200511-10:21:52] epoch: 0, batch: 4262 train-loss: 1.4347163438796997\n",
      "[LOG 20200511-10:21:52] epoch: 0, batch: 4263 train-loss: 2.112044095993042\n",
      "[LOG 20200511-10:21:52] epoch: 0, batch: 4264 train-loss: 1.3383651971817017\n",
      "[LOG 20200511-10:21:52] epoch: 0, batch: 4265 train-loss: 1.1085325479507446\n",
      "[LOG 20200511-10:21:52] epoch: 0, batch: 4266 train-loss: 1.241750717163086\n",
      "[LOG 20200511-10:21:52] epoch: 0, batch: 4267 train-loss: 0.9350190758705139\n",
      "[LOG 20200511-10:21:52] epoch: 0, batch: 4268 train-loss: 1.8969504833221436\n",
      "[LOG 20200511-10:21:52] epoch: 0, batch: 4269 train-loss: 0.5505840182304382\n",
      "[LOG 20200511-10:21:53] epoch: 0, batch: 4270 train-loss: 1.6598865985870361\n",
      "[LOG 20200511-10:21:53] epoch: 0, batch: 4271 train-loss: 1.702333927154541\n",
      "[LOG 20200511-10:21:53] epoch: 0, batch: 4272 train-loss: 1.5806788206100464\n",
      "[LOG 20200511-10:21:53] epoch: 0, batch: 4273 train-loss: 1.2189948558807373\n",
      "[LOG 20200511-10:21:53] epoch: 0, batch: 4274 train-loss: 0.9619986414909363\n",
      "[LOG 20200511-10:21:53] epoch: 0, batch: 4275 train-loss: 1.3106229305267334\n",
      "[LOG 20200511-10:21:53] epoch: 0, batch: 4276 train-loss: 1.4433906078338623\n",
      "[LOG 20200511-10:21:53] epoch: 0, batch: 4277 train-loss: 1.11139714717865\n",
      "[LOG 20200511-10:21:53] epoch: 0, batch: 4278 train-loss: 1.2225823402404785\n",
      "[LOG 20200511-10:21:54] epoch: 0, batch: 4279 train-loss: 1.4724435806274414\n",
      "[LOG 20200511-10:21:54] epoch: 0, batch: 4280 train-loss: 0.45926812291145325\n",
      "[LOG 20200511-10:21:54] epoch: 0, batch: 4281 train-loss: 1.426492691040039\n",
      "[LOG 20200511-10:21:54] epoch: 0, batch: 4282 train-loss: 1.3854273557662964\n",
      "[LOG 20200511-10:21:54] epoch: 0, batch: 4283 train-loss: 1.3135722875595093\n",
      "[LOG 20200511-10:21:54] epoch: 0, batch: 4284 train-loss: 2.162696123123169\n",
      "[LOG 20200511-10:21:54] epoch: 0, batch: 4285 train-loss: 1.1900516748428345\n",
      "[LOG 20200511-10:21:54] epoch: 0, batch: 4286 train-loss: 1.7521775960922241\n",
      "[LOG 20200511-10:21:55] epoch: 0, batch: 4287 train-loss: 1.6279025077819824\n",
      "[LOG 20200511-10:21:55] epoch: 0, batch: 4288 train-loss: 1.078553557395935\n",
      "[LOG 20200511-10:21:55] epoch: 0, batch: 4289 train-loss: 0.6839483976364136\n",
      "[LOG 20200511-10:21:55] epoch: 0, batch: 4290 train-loss: 1.014756679534912\n",
      "[LOG 20200511-10:21:55] epoch: 0, batch: 4291 train-loss: 1.0437119007110596\n",
      "[LOG 20200511-10:21:55] epoch: 0, batch: 4292 train-loss: 1.679207682609558\n",
      "[LOG 20200511-10:21:55] epoch: 0, batch: 4293 train-loss: 2.7060985565185547\n",
      "[LOG 20200511-10:21:55] epoch: 0, batch: 4294 train-loss: 2.147010326385498\n",
      "[LOG 20200511-10:21:56] epoch: 0, batch: 4295 train-loss: 1.145975112915039\n",
      "[LOG 20200511-10:21:56] epoch: 0, batch: 4296 train-loss: 0.8141074180603027\n",
      "[LOG 20200511-10:21:56] epoch: 0, batch: 4297 train-loss: 0.9188035726547241\n",
      "[LOG 20200511-10:21:56] epoch: 0, batch: 4298 train-loss: 1.172394037246704\n",
      "[LOG 20200511-10:21:56] epoch: 0, batch: 4299 train-loss: 1.6262904405593872\n",
      "[LOG 20200511-10:21:56] epoch: 0, batch: 4300 train-loss: 0.6420745849609375\n",
      "[LOG 20200511-10:21:56] epoch: 0, batch: 4301 train-loss: 0.40008118748664856\n",
      "[LOG 20200511-10:21:56] epoch: 0, batch: 4302 train-loss: 1.4989848136901855\n",
      "[LOG 20200511-10:21:57] epoch: 0, batch: 4303 train-loss: 1.6727702617645264\n",
      "[LOG 20200511-10:21:57] epoch: 0, batch: 4304 train-loss: 2.973496437072754\n",
      "[LOG 20200511-10:21:57] epoch: 0, batch: 4305 train-loss: 1.3476741313934326\n",
      "[LOG 20200511-10:21:57] epoch: 0, batch: 4306 train-loss: 1.2726004123687744\n",
      "[LOG 20200511-10:21:57] epoch: 0, batch: 4307 train-loss: 1.4465134143829346\n",
      "[LOG 20200511-10:21:57] epoch: 0, batch: 4308 train-loss: 1.8045085668563843\n",
      "[LOG 20200511-10:21:57] epoch: 0, batch: 4309 train-loss: 1.9740891456604004\n",
      "[LOG 20200511-10:21:57] epoch: 0, batch: 4310 train-loss: 2.16528058052063\n",
      "[LOG 20200511-10:21:58] epoch: 0, batch: 4311 train-loss: 2.1711089611053467\n",
      "[LOG 20200511-10:21:58] epoch: 0, batch: 4312 train-loss: 2.8308913707733154\n",
      "[LOG 20200511-10:21:58] epoch: 0, batch: 4313 train-loss: 1.4231281280517578\n",
      "[LOG 20200511-10:21:58] epoch: 0, batch: 4314 train-loss: 1.454487919807434\n",
      "[LOG 20200511-10:21:58] epoch: 0, batch: 4315 train-loss: 1.1270767450332642\n",
      "[LOG 20200511-10:21:58] epoch: 0, batch: 4316 train-loss: 1.0479249954223633\n",
      "[LOG 20200511-10:21:58] epoch: 0, batch: 4317 train-loss: 0.6407735347747803\n",
      "[LOG 20200511-10:21:59] epoch: 0, batch: 4318 train-loss: 1.513824224472046\n",
      "[LOG 20200511-10:21:59] epoch: 0, batch: 4319 train-loss: 2.2372679710388184\n",
      "[LOG 20200511-10:21:59] epoch: 0, batch: 4320 train-loss: 0.7077797651290894\n",
      "[LOG 20200511-10:21:59] epoch: 0, batch: 4321 train-loss: 2.251908302307129\n",
      "[LOG 20200511-10:21:59] epoch: 0, batch: 4322 train-loss: 0.6537947654724121\n",
      "[LOG 20200511-10:21:59] epoch: 0, batch: 4323 train-loss: 1.2284035682678223\n",
      "[LOG 20200511-10:21:59] epoch: 0, batch: 4324 train-loss: 1.1970916986465454\n",
      "[LOG 20200511-10:22:00] epoch: 0, batch: 4325 train-loss: 0.9435266256332397\n",
      "[LOG 20200511-10:22:00] epoch: 0, batch: 4326 train-loss: 0.5732688307762146\n",
      "[LOG 20200511-10:22:00] epoch: 0, batch: 4327 train-loss: 0.873610258102417\n",
      "[LOG 20200511-10:22:00] epoch: 0, batch: 4328 train-loss: 2.3119053840637207\n",
      "[LOG 20200511-10:22:00] epoch: 0, batch: 4329 train-loss: 1.3232723474502563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:22:00] epoch: 0, batch: 4330 train-loss: 1.3086642026901245\n",
      "[LOG 20200511-10:22:00] epoch: 0, batch: 4331 train-loss: 1.1577374935150146\n",
      "[LOG 20200511-10:22:00] epoch: 0, batch: 4332 train-loss: 0.7279834151268005\n",
      "[LOG 20200511-10:22:00] epoch: 0, batch: 4333 train-loss: 1.3612505197525024\n",
      "[LOG 20200511-10:22:01] epoch: 0, batch: 4334 train-loss: 1.66188383102417\n",
      "[LOG 20200511-10:22:01] epoch: 0, batch: 4335 train-loss: 1.8409607410430908\n",
      "[LOG 20200511-10:22:01] epoch: 0, batch: 4336 train-loss: 1.1810803413391113\n",
      "[LOG 20200511-10:22:01] epoch: 0, batch: 4337 train-loss: 1.5782088041305542\n",
      "[LOG 20200511-10:22:01] epoch: 0, batch: 4338 train-loss: 2.3842334747314453\n",
      "[LOG 20200511-10:22:01] epoch: 0, batch: 4339 train-loss: 1.7620388269424438\n",
      "[LOG 20200511-10:22:01] epoch: 0, batch: 4340 train-loss: 1.0753482580184937\n",
      "[LOG 20200511-10:22:01] epoch: 0, batch: 4341 train-loss: 0.7554908394813538\n",
      "[LOG 20200511-10:22:01] epoch: 0, batch: 4342 train-loss: 1.8418376445770264\n",
      "[LOG 20200511-10:22:02] epoch: 0, batch: 4343 train-loss: 2.722923755645752\n",
      "[LOG 20200511-10:22:02] epoch: 0, batch: 4344 train-loss: 0.9306675791740417\n",
      "[LOG 20200511-10:22:02] epoch: 0, batch: 4345 train-loss: 1.3914985656738281\n",
      "[LOG 20200511-10:22:02] epoch: 0, batch: 4346 train-loss: 0.9879898428916931\n",
      "[LOG 20200511-10:22:02] epoch: 0, batch: 4347 train-loss: 0.8600422739982605\n",
      "[LOG 20200511-10:22:02] epoch: 0, batch: 4348 train-loss: 1.2059903144836426\n",
      "[LOG 20200511-10:22:02] epoch: 0, batch: 4349 train-loss: 0.7915931940078735\n",
      "[LOG 20200511-10:22:02] epoch: 0, batch: 4350 train-loss: 1.863267183303833\n",
      "[LOG 20200511-10:22:03] epoch: 0, batch: 4351 train-loss: 1.773721694946289\n",
      "[LOG 20200511-10:22:03] epoch: 0, batch: 4352 train-loss: 1.6445313692092896\n",
      "[LOG 20200511-10:22:03] epoch: 0, batch: 4353 train-loss: 0.9071310758590698\n",
      "[LOG 20200511-10:22:03] epoch: 0, batch: 4354 train-loss: 0.6913282871246338\n",
      "[LOG 20200511-10:22:03] epoch: 0, batch: 4355 train-loss: 0.7491796016693115\n",
      "[LOG 20200511-10:22:03] epoch: 0, batch: 4356 train-loss: 1.5730589628219604\n",
      "[LOG 20200511-10:22:03] epoch: 0, batch: 4357 train-loss: 1.2752293348312378\n",
      "[LOG 20200511-10:22:03] epoch: 0, batch: 4358 train-loss: 0.8923104405403137\n",
      "[LOG 20200511-10:22:04] epoch: 0, batch: 4359 train-loss: 0.8273432850837708\n",
      "[LOG 20200511-10:22:04] epoch: 0, batch: 4360 train-loss: 1.4984922409057617\n",
      "[LOG 20200511-10:22:04] epoch: 0, batch: 4361 train-loss: 1.3003716468811035\n",
      "[LOG 20200511-10:22:04] epoch: 0, batch: 4362 train-loss: 1.0210633277893066\n",
      "[LOG 20200511-10:22:04] epoch: 0, batch: 4363 train-loss: 1.6582534313201904\n",
      "[LOG 20200511-10:22:04] epoch: 0, batch: 4364 train-loss: 1.0746926069259644\n",
      "[LOG 20200511-10:22:04] epoch: 0, batch: 4365 train-loss: 2.743032932281494\n",
      "[LOG 20200511-10:22:04] epoch: 0, batch: 4366 train-loss: 0.6457396149635315\n",
      "[LOG 20200511-10:22:04] epoch: 0, batch: 4367 train-loss: 1.8160871267318726\n",
      "[LOG 20200511-10:22:05] epoch: 0, batch: 4368 train-loss: 1.5081676244735718\n",
      "[LOG 20200511-10:22:05] epoch: 0, batch: 4369 train-loss: 2.1321849822998047\n",
      "[LOG 20200511-10:22:05] epoch: 0, batch: 4370 train-loss: 0.581508994102478\n",
      "[LOG 20200511-10:22:05] epoch: 0, batch: 4371 train-loss: 2.1328184604644775\n",
      "[LOG 20200511-10:22:05] epoch: 0, batch: 4372 train-loss: 0.473448783159256\n",
      "[LOG 20200511-10:22:05] epoch: 0, batch: 4373 train-loss: 0.6964085102081299\n",
      "[LOG 20200511-10:22:05] epoch: 0, batch: 4374 train-loss: 1.0000884532928467\n",
      "[LOG 20200511-10:22:05] epoch: 0, batch: 4375 train-loss: 1.066551685333252\n",
      "[LOG 20200511-10:22:06] epoch: 0, batch: 4376 train-loss: 1.2290304899215698\n",
      "[LOG 20200511-10:22:06] epoch: 0, batch: 4377 train-loss: 1.051100730895996\n",
      "[LOG 20200511-10:22:06] epoch: 0, batch: 4378 train-loss: 1.4030084609985352\n",
      "[LOG 20200511-10:22:06] epoch: 0, batch: 4379 train-loss: 1.325494647026062\n",
      "[LOG 20200511-10:22:06] epoch: 0, batch: 4380 train-loss: 0.5680131912231445\n",
      "[LOG 20200511-10:22:06] epoch: 0, batch: 4381 train-loss: 1.577863335609436\n",
      "[LOG 20200511-10:22:06] epoch: 0, batch: 4382 train-loss: 1.9686511754989624\n",
      "[LOG 20200511-10:22:06] epoch: 0, batch: 4383 train-loss: 1.8413469791412354\n",
      "[LOG 20200511-10:22:07] epoch: 0, batch: 4384 train-loss: 1.51447594165802\n",
      "[LOG 20200511-10:22:07] epoch: 0, batch: 4385 train-loss: 0.735085129737854\n",
      "[LOG 20200511-10:22:07] epoch: 0, batch: 4386 train-loss: 0.959187388420105\n",
      "[LOG 20200511-10:22:07] epoch: 0, batch: 4387 train-loss: 1.6772193908691406\n",
      "[LOG 20200511-10:22:07] epoch: 0, batch: 4388 train-loss: 1.6283206939697266\n",
      "[LOG 20200511-10:22:07] epoch: 0, batch: 4389 train-loss: 2.14158296585083\n",
      "[LOG 20200511-10:22:07] epoch: 0, batch: 4390 train-loss: 1.6678677797317505\n",
      "[LOG 20200511-10:22:07] epoch: 0, batch: 4391 train-loss: 1.7260234355926514\n",
      "[LOG 20200511-10:22:08] epoch: 0, batch: 4392 train-loss: 1.357947587966919\n",
      "[LOG 20200511-10:22:08] epoch: 0, batch: 4393 train-loss: 1.0592930316925049\n",
      "[LOG 20200511-10:22:08] epoch: 0, batch: 4394 train-loss: 2.5289602279663086\n",
      "[LOG 20200511-10:22:08] epoch: 0, batch: 4395 train-loss: 1.746962070465088\n",
      "[LOG 20200511-10:22:08] epoch: 0, batch: 4396 train-loss: 0.46941110491752625\n",
      "[LOG 20200511-10:22:08] epoch: 0, batch: 4397 train-loss: 1.3537075519561768\n",
      "[LOG 20200511-10:22:08] epoch: 0, batch: 4398 train-loss: 2.056133270263672\n",
      "[LOG 20200511-10:22:08] epoch: 0, batch: 4399 train-loss: 0.9624099135398865\n",
      "[LOG 20200511-10:22:08] epoch: 0, batch: 4400 train-loss: 1.6747827529907227\n",
      "[LOG 20200511-10:22:09] epoch: 0, batch: 4401 train-loss: 1.2303348779678345\n",
      "[LOG 20200511-10:22:09] epoch: 0, batch: 4402 train-loss: 0.9100093841552734\n",
      "[LOG 20200511-10:22:09] epoch: 0, batch: 4403 train-loss: 2.2602379322052\n",
      "[LOG 20200511-10:22:09] epoch: 0, batch: 4404 train-loss: 1.8252538442611694\n",
      "[LOG 20200511-10:22:09] epoch: 0, batch: 4405 train-loss: 0.4192221760749817\n",
      "[LOG 20200511-10:22:09] epoch: 0, batch: 4406 train-loss: 0.7509846091270447\n",
      "[LOG 20200511-10:22:09] epoch: 0, batch: 4407 train-loss: 1.6182602643966675\n",
      "[LOG 20200511-10:22:09] epoch: 0, batch: 4408 train-loss: 0.5805152654647827\n",
      "[LOG 20200511-10:22:10] epoch: 0, batch: 4409 train-loss: 1.562804937362671\n",
      "[LOG 20200511-10:22:10] epoch: 0, batch: 4410 train-loss: 0.5529407262802124\n",
      "[LOG 20200511-10:22:10] epoch: 0, batch: 4411 train-loss: 2.383270740509033\n",
      "[LOG 20200511-10:22:10] epoch: 0, batch: 4412 train-loss: 0.9827333688735962\n",
      "[LOG 20200511-10:22:10] epoch: 0, batch: 4413 train-loss: 1.1413789987564087\n",
      "[LOG 20200511-10:22:10] epoch: 0, batch: 4414 train-loss: 2.1215426921844482\n",
      "[LOG 20200511-10:22:10] epoch: 0, batch: 4415 train-loss: 1.1584582328796387\n",
      "[LOG 20200511-10:22:10] epoch: 0, batch: 4416 train-loss: 0.4631271958351135\n",
      "[LOG 20200511-10:22:11] epoch: 0, batch: 4417 train-loss: 1.7500842809677124\n",
      "[LOG 20200511-10:22:11] epoch: 0, batch: 4418 train-loss: 1.3115609884262085\n",
      "[LOG 20200511-10:22:11] epoch: 0, batch: 4419 train-loss: 0.719867467880249\n",
      "[LOG 20200511-10:22:11] epoch: 0, batch: 4420 train-loss: 1.2886240482330322\n",
      "[LOG 20200511-10:22:11] epoch: 0, batch: 4421 train-loss: 0.7583600878715515\n",
      "[LOG 20200511-10:22:11] epoch: 0, batch: 4422 train-loss: 1.5270411968231201\n",
      "[LOG 20200511-10:22:11] epoch: 0, batch: 4423 train-loss: 1.8997695446014404\n",
      "[LOG 20200511-10:22:11] epoch: 0, batch: 4424 train-loss: 2.3664588928222656\n",
      "[LOG 20200511-10:22:11] epoch: 0, batch: 4425 train-loss: 0.8314612507820129\n",
      "[LOG 20200511-10:22:12] epoch: 0, batch: 4426 train-loss: 0.8987381458282471\n",
      "[LOG 20200511-10:22:12] epoch: 0, batch: 4427 train-loss: 1.8354225158691406\n",
      "[LOG 20200511-10:22:12] epoch: 0, batch: 4428 train-loss: 1.1294569969177246\n",
      "[LOG 20200511-10:22:12] epoch: 0, batch: 4429 train-loss: 0.9119182825088501\n",
      "[LOG 20200511-10:22:12] epoch: 0, batch: 4430 train-loss: 1.2136424779891968\n",
      "[LOG 20200511-10:22:12] epoch: 0, batch: 4431 train-loss: 1.4530448913574219\n",
      "[LOG 20200511-10:22:12] epoch: 0, batch: 4432 train-loss: 2.349531412124634\n",
      "[LOG 20200511-10:22:12] epoch: 0, batch: 4433 train-loss: 1.1060900688171387\n",
      "[LOG 20200511-10:22:13] epoch: 0, batch: 4434 train-loss: 1.2280491590499878\n",
      "[LOG 20200511-10:22:13] epoch: 0, batch: 4435 train-loss: 2.081037998199463\n",
      "[LOG 20200511-10:22:13] epoch: 0, batch: 4436 train-loss: 1.8299121856689453\n",
      "[LOG 20200511-10:22:13] epoch: 0, batch: 4437 train-loss: 1.5151575803756714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:22:13] epoch: 0, batch: 4438 train-loss: 1.4168598651885986\n",
      "[LOG 20200511-10:22:13] epoch: 0, batch: 4439 train-loss: 1.544587254524231\n",
      "[LOG 20200511-10:22:13] epoch: 0, batch: 4440 train-loss: 1.73048734664917\n",
      "[LOG 20200511-10:22:13] epoch: 0, batch: 4441 train-loss: 2.424300193786621\n",
      "[LOG 20200511-10:22:13] epoch: 0, batch: 4442 train-loss: 1.5006871223449707\n",
      "[LOG 20200511-10:22:14] epoch: 0, batch: 4443 train-loss: 1.271982192993164\n",
      "[LOG 20200511-10:22:14] epoch: 0, batch: 4444 train-loss: 0.6486751437187195\n",
      "[LOG 20200511-10:22:14] epoch: 0, batch: 4445 train-loss: 1.4837305545806885\n",
      "[LOG 20200511-10:22:14] epoch: 0, batch: 4446 train-loss: 0.8637396693229675\n",
      "[LOG 20200511-10:22:14] epoch: 0, batch: 4447 train-loss: 1.0555980205535889\n",
      "[LOG 20200511-10:22:14] epoch: 0, batch: 4448 train-loss: 1.9876139163970947\n",
      "[LOG 20200511-10:22:14] epoch: 0, batch: 4449 train-loss: 1.3424261808395386\n",
      "[LOG 20200511-10:22:14] epoch: 0, batch: 4450 train-loss: 1.3474434614181519\n",
      "[LOG 20200511-10:22:15] epoch: 0, batch: 4451 train-loss: 1.025388479232788\n",
      "[LOG 20200511-10:22:15] epoch: 0, batch: 4452 train-loss: 1.1616166830062866\n",
      "[LOG 20200511-10:22:15] epoch: 0, batch: 4453 train-loss: 1.2815040349960327\n",
      "[LOG 20200511-10:22:15] epoch: 0, batch: 4454 train-loss: 1.6365138292312622\n",
      "[LOG 20200511-10:22:15] epoch: 0, batch: 4455 train-loss: 1.4141477346420288\n",
      "[LOG 20200511-10:22:15] epoch: 0, batch: 4456 train-loss: 1.5303893089294434\n",
      "[LOG 20200511-10:22:15] epoch: 0, batch: 4457 train-loss: 0.5668625831604004\n",
      "[LOG 20200511-10:22:15] epoch: 0, batch: 4458 train-loss: 1.743825078010559\n",
      "[LOG 20200511-10:22:15] epoch: 0, batch: 4459 train-loss: 1.4040254354476929\n",
      "[LOG 20200511-10:22:16] epoch: 0, batch: 4460 train-loss: 0.743889570236206\n",
      "[LOG 20200511-10:22:16] epoch: 0, batch: 4461 train-loss: 2.211954116821289\n",
      "[LOG 20200511-10:22:16] epoch: 0, batch: 4462 train-loss: 1.1945408582687378\n",
      "[LOG 20200511-10:22:16] epoch: 0, batch: 4463 train-loss: 2.6205601692199707\n",
      "[LOG 20200511-10:22:16] epoch: 0, batch: 4464 train-loss: 1.2352445125579834\n",
      "[LOG 20200511-10:22:16] epoch: 0, batch: 4465 train-loss: 1.6679737567901611\n",
      "[LOG 20200511-10:22:16] epoch: 0, batch: 4466 train-loss: 2.193765640258789\n",
      "[LOG 20200511-10:22:16] epoch: 0, batch: 4467 train-loss: 2.8574881553649902\n",
      "[LOG 20200511-10:22:17] epoch: 0, batch: 4468 train-loss: 2.718629837036133\n",
      "[LOG 20200511-10:22:17] epoch: 0, batch: 4469 train-loss: 0.9932611584663391\n",
      "[LOG 20200511-10:22:17] epoch: 0, batch: 4470 train-loss: 2.1203980445861816\n",
      "[LOG 20200511-10:22:17] epoch: 0, batch: 4471 train-loss: 2.162576675415039\n",
      "[LOG 20200511-10:22:17] epoch: 0, batch: 4472 train-loss: 1.9532411098480225\n",
      "[LOG 20200511-10:22:17] epoch: 0, batch: 4473 train-loss: 1.1659328937530518\n",
      "[LOG 20200511-10:22:17] epoch: 0, batch: 4474 train-loss: 2.246534824371338\n",
      "[LOG 20200511-10:22:17] epoch: 0, batch: 4475 train-loss: 1.8858673572540283\n",
      "[LOG 20200511-10:22:18] epoch: 0, batch: 4476 train-loss: 2.16485595703125\n",
      "[LOG 20200511-10:22:18] epoch: 0, batch: 4477 train-loss: 2.0452418327331543\n",
      "[LOG 20200511-10:22:18] epoch: 0, batch: 4478 train-loss: 1.8561558723449707\n",
      "[LOG 20200511-10:22:18] epoch: 0, batch: 4479 train-loss: 1.50971257686615\n",
      "[LOG 20200511-10:22:18] epoch: 0, batch: 4480 train-loss: 0.7036479711532593\n",
      "[LOG 20200511-10:22:18] epoch: 0, batch: 4481 train-loss: 1.9465534687042236\n",
      "[LOG 20200511-10:22:18] epoch: 0, batch: 4482 train-loss: 1.1713231801986694\n",
      "[LOG 20200511-10:22:18] epoch: 0, batch: 4483 train-loss: 1.6038408279418945\n",
      "[LOG 20200511-10:22:18] epoch: 0, batch: 4484 train-loss: 1.401341199874878\n",
      "[LOG 20200511-10:22:19] epoch: 0, batch: 4485 train-loss: 1.4147377014160156\n",
      "[LOG 20200511-10:22:19] epoch: 0, batch: 4486 train-loss: 1.3679003715515137\n",
      "[LOG 20200511-10:22:19] epoch: 0, batch: 4487 train-loss: 1.0502043962478638\n",
      "[LOG 20200511-10:22:19] epoch: 0, batch: 4488 train-loss: 1.5038834810256958\n",
      "[LOG 20200511-10:22:19] epoch: 0, batch: 4489 train-loss: 2.2372074127197266\n",
      "[LOG 20200511-10:22:19] epoch: 0, batch: 4490 train-loss: 2.4631004333496094\n",
      "[LOG 20200511-10:22:19] epoch: 0, batch: 4491 train-loss: 0.7720386385917664\n",
      "[LOG 20200511-10:22:19] epoch: 0, batch: 4492 train-loss: 0.8514347076416016\n",
      "[LOG 20200511-10:22:20] epoch: 0, batch: 4493 train-loss: 1.609159231185913\n",
      "[LOG 20200511-10:22:20] epoch: 0, batch: 4494 train-loss: 1.246636152267456\n",
      "[LOG 20200511-10:22:20] epoch: 0, batch: 4495 train-loss: 0.9893996715545654\n",
      "[LOG 20200511-10:22:20] epoch: 0, batch: 4496 train-loss: 2.247070789337158\n",
      "[LOG 20200511-10:22:20] epoch: 0, batch: 4497 train-loss: 0.7277936935424805\n",
      "[LOG 20200511-10:22:20] epoch: 0, batch: 4498 train-loss: 2.528510093688965\n",
      "[LOG 20200511-10:22:20] epoch: 0, batch: 4499 train-loss: 0.8991699814796448\n",
      "[LOG 20200511-10:22:20] epoch: 0, batch: 4500 train-loss: 2.1492252349853516\n",
      "[LOG 20200511-10:22:21] epoch: 0, batch: 4501 train-loss: 0.41806691884994507\n",
      "[LOG 20200511-10:22:21] epoch: 0, batch: 4502 train-loss: 0.6744870543479919\n",
      "[LOG 20200511-10:22:21] epoch: 0, batch: 4503 train-loss: 1.3332417011260986\n",
      "[LOG 20200511-10:22:21] epoch: 0, batch: 4504 train-loss: 1.4113487005233765\n",
      "[LOG 20200511-10:22:21] epoch: 0, batch: 4505 train-loss: 1.1973339319229126\n",
      "[LOG 20200511-10:22:21] epoch: 0, batch: 4506 train-loss: 1.133842945098877\n",
      "[LOG 20200511-10:22:21] epoch: 0, batch: 4507 train-loss: 1.8815231323242188\n",
      "[LOG 20200511-10:22:21] epoch: 0, batch: 4508 train-loss: 2.4873313903808594\n",
      "[LOG 20200511-10:22:21] epoch: 0, batch: 4509 train-loss: 1.0181300640106201\n",
      "[LOG 20200511-10:22:22] epoch: 0, batch: 4510 train-loss: 2.0227279663085938\n",
      "[LOG 20200511-10:22:22] epoch: 0, batch: 4511 train-loss: 0.7002712488174438\n",
      "[LOG 20200511-10:22:22] epoch: 0, batch: 4512 train-loss: 0.8570317625999451\n",
      "[LOG 20200511-10:22:22] epoch: 0, batch: 4513 train-loss: 1.1964062452316284\n",
      "[LOG 20200511-10:22:22] epoch: 0, batch: 4514 train-loss: 2.4255166053771973\n",
      "[LOG 20200511-10:22:22] epoch: 0, batch: 4515 train-loss: 1.003152847290039\n",
      "[LOG 20200511-10:22:22] epoch: 0, batch: 4516 train-loss: 1.5006109476089478\n",
      "[LOG 20200511-10:22:22] epoch: 0, batch: 4517 train-loss: 1.440189003944397\n",
      "[LOG 20200511-10:22:22] epoch: 0, batch: 4518 train-loss: 1.915581226348877\n",
      "[LOG 20200511-10:22:23] epoch: 0, batch: 4519 train-loss: 1.1829272508621216\n",
      "[LOG 20200511-10:22:23] epoch: 0, batch: 4520 train-loss: 1.0216244459152222\n",
      "[LOG 20200511-10:22:23] epoch: 0, batch: 4521 train-loss: 1.5292432308197021\n",
      "[LOG 20200511-10:22:23] epoch: 0, batch: 4522 train-loss: 2.0745906829833984\n",
      "[LOG 20200511-10:22:23] epoch: 0, batch: 4523 train-loss: 1.1594821214675903\n",
      "[LOG 20200511-10:22:23] epoch: 0, batch: 4524 train-loss: 1.4877548217773438\n",
      "[LOG 20200511-10:22:23] epoch: 0, batch: 4525 train-loss: 1.8892083168029785\n",
      "[LOG 20200511-10:22:23] epoch: 0, batch: 4526 train-loss: 1.466888427734375\n",
      "[LOG 20200511-10:22:23] epoch: 0, batch: 4527 train-loss: 2.14516019821167\n",
      "[LOG 20200511-10:22:24] epoch: 0, batch: 4528 train-loss: 1.3011295795440674\n",
      "[LOG 20200511-10:22:24] epoch: 0, batch: 4529 train-loss: 1.5816248655319214\n",
      "[LOG 20200511-10:22:24] epoch: 0, batch: 4530 train-loss: 1.1033134460449219\n",
      "[LOG 20200511-10:22:24] epoch: 0, batch: 4531 train-loss: 0.9331084489822388\n",
      "[LOG 20200511-10:22:24] epoch: 0, batch: 4532 train-loss: 0.4078338146209717\n",
      "[LOG 20200511-10:22:24] epoch: 0, batch: 4533 train-loss: 1.3320008516311646\n",
      "[LOG 20200511-10:22:24] epoch: 0, batch: 4534 train-loss: 0.6954467296600342\n",
      "[LOG 20200511-10:22:24] epoch: 0, batch: 4535 train-loss: 0.9587448835372925\n",
      "[LOG 20200511-10:22:24] epoch: 0, batch: 4536 train-loss: 0.5923028588294983\n",
      "[LOG 20200511-10:22:25] epoch: 0, batch: 4537 train-loss: 1.5896291732788086\n",
      "[LOG 20200511-10:22:25] epoch: 0, batch: 4538 train-loss: 0.8166886568069458\n",
      "[LOG 20200511-10:22:25] epoch: 0, batch: 4539 train-loss: 2.0016510486602783\n",
      "[LOG 20200511-10:22:25] epoch: 0, batch: 4540 train-loss: 1.662825107574463\n",
      "[LOG 20200511-10:22:25] epoch: 0, batch: 4541 train-loss: 0.6301849484443665\n",
      "[LOG 20200511-10:22:25] epoch: 0, batch: 4542 train-loss: 1.4332005977630615\n",
      "[LOG 20200511-10:22:25] epoch: 0, batch: 4543 train-loss: 1.464268684387207\n",
      "[LOG 20200511-10:22:25] epoch: 0, batch: 4544 train-loss: 0.8647337555885315\n",
      "[LOG 20200511-10:22:25] epoch: 0, batch: 4545 train-loss: 2.251713991165161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:22:26] epoch: 0, batch: 4546 train-loss: 0.8693304061889648\n",
      "[LOG 20200511-10:22:26] epoch: 0, batch: 4547 train-loss: 2.57356595993042\n",
      "[LOG 20200511-10:22:26] epoch: 0, batch: 4548 train-loss: 0.7777783870697021\n",
      "[LOG 20200511-10:22:26] epoch: 0, batch: 4549 train-loss: 1.7615313529968262\n",
      "[LOG 20200511-10:22:26] epoch: 0, batch: 4550 train-loss: 1.306310772895813\n",
      "[LOG 20200511-10:22:26] epoch: 0, batch: 4551 train-loss: 0.9348247051239014\n",
      "[LOG 20200511-10:22:26] epoch: 0, batch: 4552 train-loss: 0.9086910486221313\n",
      "[LOG 20200511-10:22:26] epoch: 0, batch: 4553 train-loss: 1.8008251190185547\n",
      "[LOG 20200511-10:22:26] epoch: 0, batch: 4554 train-loss: 2.0919604301452637\n",
      "[LOG 20200511-10:22:27] epoch: 0, batch: 4555 train-loss: 1.9627668857574463\n",
      "[LOG 20200511-10:22:27] epoch: 0, batch: 4556 train-loss: 0.25220993161201477\n",
      "[LOG 20200511-10:22:27] epoch: 0, batch: 4557 train-loss: 1.3971428871154785\n",
      "[LOG 20200511-10:22:27] epoch: 0, batch: 4558 train-loss: 2.188753366470337\n",
      "[LOG 20200511-10:22:27] epoch: 0, batch: 4559 train-loss: 2.236118793487549\n",
      "[LOG 20200511-10:22:27] epoch: 0, batch: 4560 train-loss: 0.9642214775085449\n",
      "[LOG 20200511-10:22:27] epoch: 0, batch: 4561 train-loss: 1.4901692867279053\n",
      "[LOG 20200511-10:22:27] epoch: 0, batch: 4562 train-loss: 1.2161896228790283\n",
      "[LOG 20200511-10:22:28] epoch: 0, batch: 4563 train-loss: 1.5753551721572876\n",
      "[LOG 20200511-10:22:28] epoch: 0, batch: 4564 train-loss: 1.2435288429260254\n",
      "[LOG 20200511-10:22:28] epoch: 0, batch: 4565 train-loss: 1.6453578472137451\n",
      "[LOG 20200511-10:22:28] epoch: 0, batch: 4566 train-loss: 1.4090698957443237\n",
      "[LOG 20200511-10:22:28] epoch: 0, batch: 4567 train-loss: 1.6746826171875\n",
      "[LOG 20200511-10:22:28] epoch: 0, batch: 4568 train-loss: 1.5092337131500244\n",
      "[LOG 20200511-10:22:28] epoch: 0, batch: 4569 train-loss: 1.0163182020187378\n",
      "[LOG 20200511-10:22:28] epoch: 0, batch: 4570 train-loss: 1.754481315612793\n",
      "[LOG 20200511-10:22:28] epoch: 0, batch: 4571 train-loss: 1.107334852218628\n",
      "[LOG 20200511-10:22:29] epoch: 0, batch: 4572 train-loss: 0.9771568775177002\n",
      "[LOG 20200511-10:22:29] epoch: 0, batch: 4573 train-loss: 1.9509308338165283\n",
      "[LOG 20200511-10:22:29] epoch: 0, batch: 4574 train-loss: 1.888329267501831\n",
      "[LOG 20200511-10:22:29] epoch: 0, batch: 4575 train-loss: 0.7964150905609131\n",
      "[LOG 20200511-10:22:29] epoch: 0, batch: 4576 train-loss: 1.5156477689743042\n",
      "[LOG 20200511-10:22:29] epoch: 0, batch: 4577 train-loss: 0.7272339463233948\n",
      "[LOG 20200511-10:22:29] epoch: 0, batch: 4578 train-loss: 1.6734955310821533\n",
      "[LOG 20200511-10:22:30] epoch: 0, batch: 4579 train-loss: 0.8986572027206421\n",
      "[LOG 20200511-10:22:30] epoch: 0, batch: 4580 train-loss: 1.5831669569015503\n",
      "[LOG 20200511-10:22:30] epoch: 0, batch: 4581 train-loss: 1.3600409030914307\n",
      "[LOG 20200511-10:22:30] epoch: 0, batch: 4582 train-loss: 1.805940866470337\n",
      "[LOG 20200511-10:22:30] epoch: 0, batch: 4583 train-loss: 1.8537466526031494\n",
      "[LOG 20200511-10:22:30] epoch: 0, batch: 4584 train-loss: 0.5066032409667969\n",
      "[LOG 20200511-10:22:30] epoch: 0, batch: 4585 train-loss: 1.5840520858764648\n",
      "[LOG 20200511-10:22:30] epoch: 0, batch: 4586 train-loss: 1.2134041786193848\n",
      "[LOG 20200511-10:22:30] epoch: 0, batch: 4587 train-loss: 1.013838529586792\n",
      "[LOG 20200511-10:22:31] epoch: 0, batch: 4588 train-loss: 0.5912752747535706\n",
      "[LOG 20200511-10:22:31] epoch: 0, batch: 4589 train-loss: 1.356718897819519\n",
      "[LOG 20200511-10:22:31] epoch: 0, batch: 4590 train-loss: 1.3651683330535889\n",
      "[LOG 20200511-10:22:31] epoch: 0, batch: 4591 train-loss: 0.8973714709281921\n",
      "[LOG 20200511-10:22:31] epoch: 0, batch: 4592 train-loss: 0.8624362349510193\n",
      "[LOG 20200511-10:22:31] epoch: 0, batch: 4593 train-loss: 2.754437208175659\n",
      "[LOG 20200511-10:22:31] epoch: 0, batch: 4594 train-loss: 0.6588450074195862\n",
      "[LOG 20200511-10:22:31] epoch: 0, batch: 4595 train-loss: 2.8174571990966797\n",
      "[LOG 20200511-10:22:32] epoch: 0, batch: 4596 train-loss: 0.9490703344345093\n",
      "[LOG 20200511-10:22:32] epoch: 0, batch: 4597 train-loss: 1.1597501039505005\n",
      "[LOG 20200511-10:22:32] epoch: 0, batch: 4598 train-loss: 1.6776671409606934\n",
      "[LOG 20200511-10:22:32] epoch: 0, batch: 4599 train-loss: 1.610511064529419\n",
      "[LOG 20200511-10:22:32] epoch: 0, batch: 4600 train-loss: 1.200782060623169\n",
      "[LOG 20200511-10:22:32] epoch: 0, batch: 4601 train-loss: 1.1135270595550537\n",
      "[LOG 20200511-10:22:32] epoch: 0, batch: 4602 train-loss: 1.4647839069366455\n",
      "[LOG 20200511-10:22:33] epoch: 0, batch: 4603 train-loss: 1.6675686836242676\n",
      "[LOG 20200511-10:22:33] epoch: 0, batch: 4604 train-loss: 1.1618967056274414\n",
      "[LOG 20200511-10:22:33] epoch: 0, batch: 4605 train-loss: 1.2286396026611328\n",
      "[LOG 20200511-10:22:33] epoch: 0, batch: 4606 train-loss: 1.2242703437805176\n",
      "[LOG 20200511-10:22:33] epoch: 0, batch: 4607 train-loss: 0.658203661441803\n",
      "[LOG 20200511-10:22:33] epoch: 0, batch: 4608 train-loss: 1.4730582237243652\n",
      "[LOG 20200511-10:22:33] epoch: 0, batch: 4609 train-loss: 1.2109417915344238\n",
      "[LOG 20200511-10:22:33] epoch: 0, batch: 4610 train-loss: 1.2327361106872559\n",
      "[LOG 20200511-10:22:34] epoch: 0, batch: 4611 train-loss: 2.1933746337890625\n",
      "[LOG 20200511-10:22:34] epoch: 0, batch: 4612 train-loss: 1.160253882408142\n",
      "[LOG 20200511-10:22:34] epoch: 0, batch: 4613 train-loss: 1.444794774055481\n",
      "[LOG 20200511-10:22:34] epoch: 0, batch: 4614 train-loss: 1.5998001098632812\n",
      "[LOG 20200511-10:22:34] epoch: 0, batch: 4615 train-loss: 2.1201398372650146\n",
      "[LOG 20200511-10:22:34] epoch: 0, batch: 4616 train-loss: 1.5724672079086304\n",
      "[LOG 20200511-10:22:34] epoch: 0, batch: 4617 train-loss: 0.7682873010635376\n",
      "[LOG 20200511-10:22:34] epoch: 0, batch: 4618 train-loss: 0.943250298500061\n",
      "[LOG 20200511-10:22:35] epoch: 0, batch: 4619 train-loss: 1.044581413269043\n",
      "[LOG 20200511-10:22:35] epoch: 0, batch: 4620 train-loss: 0.8715454936027527\n",
      "[LOG 20200511-10:22:35] epoch: 0, batch: 4621 train-loss: 2.791931390762329\n",
      "[LOG 20200511-10:22:35] epoch: 0, batch: 4622 train-loss: 0.9987417459487915\n",
      "[LOG 20200511-10:22:35] epoch: 0, batch: 4623 train-loss: 1.2768068313598633\n",
      "[LOG 20200511-10:22:35] epoch: 0, batch: 4624 train-loss: 0.6401001214981079\n",
      "[LOG 20200511-10:22:35] epoch: 0, batch: 4625 train-loss: 2.40666127204895\n",
      "[LOG 20200511-10:22:35] epoch: 0, batch: 4626 train-loss: 1.4656670093536377\n",
      "[LOG 20200511-10:22:36] epoch: 0, batch: 4627 train-loss: 1.478007435798645\n",
      "[LOG 20200511-10:22:36] epoch: 0, batch: 4628 train-loss: 1.9717795848846436\n",
      "[LOG 20200511-10:22:36] epoch: 0, batch: 4629 train-loss: 2.053248882293701\n",
      "[LOG 20200511-10:22:36] epoch: 0, batch: 4630 train-loss: 0.9796839356422424\n",
      "[LOG 20200511-10:22:36] epoch: 0, batch: 4631 train-loss: 0.5327644348144531\n",
      "[LOG 20200511-10:22:36] epoch: 0, batch: 4632 train-loss: 1.2051947116851807\n",
      "[LOG 20200511-10:22:36] epoch: 0, batch: 4633 train-loss: 1.319368600845337\n",
      "[LOG 20200511-10:22:36] epoch: 0, batch: 4634 train-loss: 1.1715216636657715\n",
      "[LOG 20200511-10:22:36] epoch: 0, batch: 4635 train-loss: 1.290846586227417\n",
      "[LOG 20200511-10:22:37] epoch: 0, batch: 4636 train-loss: 1.8689138889312744\n",
      "[LOG 20200511-10:22:37] epoch: 0, batch: 4637 train-loss: 1.5820999145507812\n",
      "[LOG 20200511-10:22:37] epoch: 0, batch: 4638 train-loss: 0.8383660316467285\n",
      "[LOG 20200511-10:22:37] epoch: 0, batch: 4639 train-loss: 1.1744506359100342\n",
      "[LOG 20200511-10:22:37] epoch: 0, batch: 4640 train-loss: 1.154613733291626\n",
      "[LOG 20200511-10:22:37] epoch: 0, batch: 4641 train-loss: 0.9121294617652893\n",
      "[LOG 20200511-10:22:37] epoch: 0, batch: 4642 train-loss: 1.3623950481414795\n",
      "[LOG 20200511-10:22:37] epoch: 0, batch: 4643 train-loss: 2.1581532955169678\n",
      "[LOG 20200511-10:22:38] epoch: 0, batch: 4644 train-loss: 2.3013696670532227\n",
      "[LOG 20200511-10:22:38] epoch: 0, batch: 4645 train-loss: 0.7432440519332886\n",
      "[LOG 20200511-10:22:38] epoch: 0, batch: 4646 train-loss: 2.8554625511169434\n",
      "[LOG 20200511-10:22:38] epoch: 0, batch: 4647 train-loss: 1.1351569890975952\n",
      "[LOG 20200511-10:22:38] epoch: 0, batch: 4648 train-loss: 1.379369854927063\n",
      "[LOG 20200511-10:22:38] epoch: 0, batch: 4649 train-loss: 1.9437288045883179\n",
      "[LOG 20200511-10:22:38] epoch: 0, batch: 4650 train-loss: 2.314730405807495\n",
      "[LOG 20200511-10:22:38] epoch: 0, batch: 4651 train-loss: 2.6505861282348633\n",
      "[LOG 20200511-10:22:39] epoch: 0, batch: 4652 train-loss: 0.77860426902771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:22:39] epoch: 0, batch: 4653 train-loss: 1.9614633321762085\n",
      "[LOG 20200511-10:22:39] epoch: 0, batch: 4654 train-loss: 1.5854682922363281\n",
      "[LOG 20200511-10:22:39] epoch: 0, batch: 4655 train-loss: 1.770467758178711\n",
      "[LOG 20200511-10:22:39] epoch: 0, batch: 4656 train-loss: 2.1108672618865967\n",
      "[LOG 20200511-10:22:39] epoch: 0, batch: 4657 train-loss: 0.8651242256164551\n",
      "[LOG 20200511-10:22:39] epoch: 0, batch: 4658 train-loss: 1.8661222457885742\n",
      "[LOG 20200511-10:22:39] epoch: 0, batch: 4659 train-loss: 1.368603229522705\n",
      "[LOG 20200511-10:22:40] epoch: 0, batch: 4660 train-loss: 1.7586536407470703\n",
      "[LOG 20200511-10:22:40] epoch: 0, batch: 4661 train-loss: 0.6969100832939148\n",
      "[LOG 20200511-10:22:40] epoch: 0, batch: 4662 train-loss: 1.3698375225067139\n",
      "[LOG 20200511-10:22:40] epoch: 0, batch: 4663 train-loss: 0.8925231695175171\n",
      "[LOG 20200511-10:22:40] epoch: 0, batch: 4664 train-loss: 0.8208420276641846\n",
      "[LOG 20200511-10:22:40] epoch: 0, batch: 4665 train-loss: 0.9748578071594238\n",
      "[LOG 20200511-10:22:40] epoch: 0, batch: 4666 train-loss: 0.8908703327178955\n",
      "[LOG 20200511-10:22:40] epoch: 0, batch: 4667 train-loss: 1.1199824810028076\n",
      "[LOG 20200511-10:22:40] epoch: 0, batch: 4668 train-loss: 1.956268310546875\n",
      "[LOG 20200511-10:22:41] epoch: 0, batch: 4669 train-loss: 1.5905307531356812\n",
      "[LOG 20200511-10:22:41] epoch: 0, batch: 4670 train-loss: 2.149991035461426\n",
      "[LOG 20200511-10:22:41] epoch: 0, batch: 4671 train-loss: 1.4256930351257324\n",
      "[LOG 20200511-10:22:41] epoch: 0, batch: 4672 train-loss: 2.3313944339752197\n",
      "[LOG 20200511-10:22:41] epoch: 0, batch: 4673 train-loss: 1.4148292541503906\n",
      "[LOG 20200511-10:22:41] epoch: 0, batch: 4674 train-loss: 1.0740840435028076\n",
      "[LOG 20200511-10:22:41] epoch: 0, batch: 4675 train-loss: 2.5908291339874268\n",
      "[LOG 20200511-10:22:41] epoch: 0, batch: 4676 train-loss: 1.4551950693130493\n",
      "[LOG 20200511-10:22:42] epoch: 0, batch: 4677 train-loss: 1.291763424873352\n",
      "[LOG 20200511-10:22:42] epoch: 0, batch: 4678 train-loss: 1.9728007316589355\n",
      "[LOG 20200511-10:22:42] epoch: 0, batch: 4679 train-loss: 0.9580060839653015\n",
      "[LOG 20200511-10:22:42] epoch: 0, batch: 4680 train-loss: 1.03254234790802\n",
      "[LOG 20200511-10:22:42] epoch: 0, batch: 4681 train-loss: 1.8878579139709473\n",
      "[LOG 20200511-10:22:42] epoch: 0, batch: 4682 train-loss: 1.7852201461791992\n",
      "[LOG 20200511-10:22:42] epoch: 0, batch: 4683 train-loss: 1.4852795600891113\n",
      "[LOG 20200511-10:22:42] epoch: 0, batch: 4684 train-loss: 1.032084584236145\n",
      "[LOG 20200511-10:22:42] epoch: 0, batch: 4685 train-loss: 1.4406771659851074\n",
      "[LOG 20200511-10:22:43] epoch: 0, batch: 4686 train-loss: 1.408915400505066\n",
      "[LOG 20200511-10:22:43] epoch: 0, batch: 4687 train-loss: 1.9879584312438965\n",
      "[LOG 20200511-10:22:43] epoch: 0, batch: 4688 train-loss: 0.627489447593689\n",
      "[LOG 20200511-10:22:43] epoch: 0, batch: 4689 train-loss: 0.7416220307350159\n",
      "[LOG 20200511-10:22:43] epoch: 0, batch: 4690 train-loss: 1.394322156906128\n",
      "[LOG 20200511-10:22:43] epoch: 0, batch: 4691 train-loss: 1.7044150829315186\n",
      "[LOG 20200511-10:22:43] epoch: 0, batch: 4692 train-loss: 1.857334852218628\n",
      "[LOG 20200511-10:22:43] epoch: 0, batch: 4693 train-loss: 1.280206322669983\n",
      "[LOG 20200511-10:22:43] epoch: 0, batch: 4694 train-loss: 0.5932390689849854\n",
      "[LOG 20200511-10:22:44] epoch: 0, batch: 4695 train-loss: 1.0005683898925781\n",
      "[LOG 20200511-10:22:44] epoch: 0, batch: 4696 train-loss: 2.629563570022583\n",
      "[LOG 20200511-10:22:44] epoch: 0, batch: 4697 train-loss: 0.9076669812202454\n",
      "[LOG 20200511-10:22:44] epoch: 0, batch: 4698 train-loss: 1.2624506950378418\n",
      "[LOG 20200511-10:22:44] epoch: 0, batch: 4699 train-loss: 1.783132553100586\n",
      "[LOG 20200511-10:22:44] epoch: 0, batch: 4700 train-loss: 1.9578516483306885\n",
      "[LOG 20200511-10:22:44] epoch: 0, batch: 4701 train-loss: 1.2586508989334106\n",
      "[LOG 20200511-10:22:44] epoch: 0, batch: 4702 train-loss: 1.7636653184890747\n",
      "[LOG 20200511-10:22:45] epoch: 0, batch: 4703 train-loss: 2.6047213077545166\n",
      "[LOG 20200511-10:22:45] epoch: 0, batch: 4704 train-loss: 1.3317756652832031\n",
      "[LOG 20200511-10:22:45] epoch: 0, batch: 4705 train-loss: 0.5573703050613403\n",
      "[LOG 20200511-10:22:45] epoch: 0, batch: 4706 train-loss: 0.9525882005691528\n",
      "[LOG 20200511-10:22:45] epoch: 0, batch: 4707 train-loss: 1.304762363433838\n",
      "[LOG 20200511-10:22:45] epoch: 0, batch: 4708 train-loss: 1.468127727508545\n",
      "[LOG 20200511-10:22:45] epoch: 0, batch: 4709 train-loss: 2.4072041511535645\n",
      "[LOG 20200511-10:22:45] epoch: 0, batch: 4710 train-loss: 1.1453790664672852\n",
      "[LOG 20200511-10:22:45] epoch: 0, batch: 4711 train-loss: 1.4581246376037598\n",
      "[LOG 20200511-10:22:46] epoch: 0, batch: 4712 train-loss: 1.1811357736587524\n",
      "[LOG 20200511-10:22:46] epoch: 0, batch: 4713 train-loss: 1.4458738565444946\n",
      "[LOG 20200511-10:22:46] epoch: 0, batch: 4714 train-loss: 1.2544922828674316\n",
      "[LOG 20200511-10:22:46] epoch: 0, batch: 4715 train-loss: 1.043749451637268\n",
      "[LOG 20200511-10:22:46] epoch: 0, batch: 4716 train-loss: 1.9368634223937988\n",
      "[LOG 20200511-10:22:46] epoch: 0, batch: 4717 train-loss: 1.4117603302001953\n",
      "[LOG 20200511-10:22:46] epoch: 0, batch: 4718 train-loss: 2.3345165252685547\n",
      "[LOG 20200511-10:22:46] epoch: 0, batch: 4719 train-loss: 1.101647138595581\n",
      "[LOG 20200511-10:22:46] epoch: 0, batch: 4720 train-loss: 1.1214855909347534\n",
      "[LOG 20200511-10:22:47] epoch: 0, batch: 4721 train-loss: 1.6974900960922241\n",
      "[LOG 20200511-10:22:47] epoch: 0, batch: 4722 train-loss: 1.603093147277832\n",
      "[LOG 20200511-10:22:47] epoch: 0, batch: 4723 train-loss: 1.9601335525512695\n",
      "[LOG 20200511-10:22:47] epoch: 0, batch: 4724 train-loss: 0.8453198671340942\n",
      "[LOG 20200511-10:22:47] epoch: 0, batch: 4725 train-loss: 0.8461018204689026\n",
      "[LOG 20200511-10:22:47] epoch: 0, batch: 4726 train-loss: 1.062427043914795\n",
      "[LOG 20200511-10:22:47] epoch: 0, batch: 4727 train-loss: 1.7390053272247314\n",
      "[LOG 20200511-10:22:47] epoch: 0, batch: 4728 train-loss: 1.803720474243164\n",
      "[LOG 20200511-10:22:47] epoch: 0, batch: 4729 train-loss: 2.3056488037109375\n",
      "[LOG 20200511-10:22:48] epoch: 0, batch: 4730 train-loss: 1.260846495628357\n",
      "[LOG 20200511-10:22:48] epoch: 0, batch: 4731 train-loss: 1.551285982131958\n",
      "[LOG 20200511-10:22:48] epoch: 0, batch: 4732 train-loss: 1.5618858337402344\n",
      "[LOG 20200511-10:22:48] epoch: 0, batch: 4733 train-loss: 1.2066688537597656\n",
      "[LOG 20200511-10:22:48] epoch: 0, batch: 4734 train-loss: 1.625473141670227\n",
      "[LOG 20200511-10:22:48] epoch: 0, batch: 4735 train-loss: 0.9257862567901611\n",
      "[LOG 20200511-10:22:48] epoch: 0, batch: 4736 train-loss: 2.3980660438537598\n",
      "[LOG 20200511-10:22:48] epoch: 0, batch: 4737 train-loss: 1.005288004875183\n",
      "[LOG 20200511-10:22:48] epoch: 0, batch: 4738 train-loss: 0.7825500965118408\n",
      "[LOG 20200511-10:22:49] epoch: 0, batch: 4739 train-loss: 1.6063826084136963\n",
      "[LOG 20200511-10:22:49] epoch: 0, batch: 4740 train-loss: 0.9239770770072937\n",
      "[LOG 20200511-10:22:49] epoch: 0, batch: 4741 train-loss: 1.2434797286987305\n",
      "[LOG 20200511-10:22:49] epoch: 0, batch: 4742 train-loss: 1.6523633003234863\n",
      "[LOG 20200511-10:22:49] epoch: 0, batch: 4743 train-loss: 1.0985270738601685\n",
      "[LOG 20200511-10:22:49] epoch: 0, batch: 4744 train-loss: 1.28807532787323\n",
      "[LOG 20200511-10:22:49] epoch: 0, batch: 4745 train-loss: 1.540130376815796\n",
      "[LOG 20200511-10:22:49] epoch: 0, batch: 4746 train-loss: 2.269253730773926\n",
      "[LOG 20200511-10:22:50] epoch: 0, batch: 4747 train-loss: 1.6699810028076172\n",
      "[LOG 20200511-10:22:50] epoch: 0, batch: 4748 train-loss: 0.7420731782913208\n",
      "[LOG 20200511-10:22:50] epoch: 0, batch: 4749 train-loss: 0.7031615972518921\n",
      "[LOG 20200511-10:22:50] epoch: 0, batch: 4750 train-loss: 1.9627766609191895\n",
      "[LOG 20200511-10:22:50] epoch: 0, batch: 4751 train-loss: 1.8087067604064941\n",
      "[LOG 20200511-10:22:50] epoch: 0, batch: 4752 train-loss: 0.9109941720962524\n",
      "[LOG 20200511-10:22:50] epoch: 0, batch: 4753 train-loss: 1.975095510482788\n",
      "[LOG 20200511-10:22:50] epoch: 0, batch: 4754 train-loss: 2.3292620182037354\n",
      "[LOG 20200511-10:22:50] epoch: 0, batch: 4755 train-loss: 1.1043972969055176\n",
      "[LOG 20200511-10:22:51] epoch: 0, batch: 4756 train-loss: 2.161756992340088\n",
      "[LOG 20200511-10:22:51] epoch: 0, batch: 4757 train-loss: 2.3127379417419434\n",
      "[LOG 20200511-10:22:51] epoch: 0, batch: 4758 train-loss: 0.8130634427070618\n",
      "[LOG 20200511-10:22:51] epoch: 0, batch: 4759 train-loss: 1.8164663314819336\n",
      "[LOG 20200511-10:22:51] epoch: 0, batch: 4760 train-loss: 2.6752984523773193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:22:51] epoch: 0, batch: 4761 train-loss: 1.0610246658325195\n",
      "[LOG 20200511-10:22:51] epoch: 0, batch: 4762 train-loss: 1.9589051008224487\n",
      "[LOG 20200511-10:22:51] epoch: 0, batch: 4763 train-loss: 1.1724413633346558\n",
      "[LOG 20200511-10:22:52] epoch: 0, batch: 4764 train-loss: 1.4616014957427979\n",
      "[LOG 20200511-10:22:52] epoch: 0, batch: 4765 train-loss: 1.2409145832061768\n",
      "[LOG 20200511-10:22:52] epoch: 0, batch: 4766 train-loss: 1.2398078441619873\n",
      "[LOG 20200511-10:22:52] epoch: 0, batch: 4767 train-loss: 1.495368242263794\n",
      "[LOG 20200511-10:22:52] epoch: 0, batch: 4768 train-loss: 1.2446868419647217\n",
      "[LOG 20200511-10:22:52] epoch: 0, batch: 4769 train-loss: 1.4185646772384644\n",
      "[LOG 20200511-10:22:52] epoch: 0, batch: 4770 train-loss: 1.711419939994812\n",
      "[LOG 20200511-10:22:52] epoch: 0, batch: 4771 train-loss: 1.3135887384414673\n",
      "[LOG 20200511-10:22:52] epoch: 0, batch: 4772 train-loss: 1.95099675655365\n",
      "[LOG 20200511-10:22:53] epoch: 0, batch: 4773 train-loss: 1.5815213918685913\n",
      "[LOG 20200511-10:22:53] epoch: 0, batch: 4774 train-loss: 1.5954967737197876\n",
      "[LOG 20200511-10:22:53] epoch: 0, batch: 4775 train-loss: 0.5667134523391724\n",
      "[LOG 20200511-10:22:53] epoch: 0, batch: 4776 train-loss: 1.4035351276397705\n",
      "[LOG 20200511-10:22:53] epoch: 0, batch: 4777 train-loss: 1.1887201070785522\n",
      "[LOG 20200511-10:22:53] epoch: 0, batch: 4778 train-loss: 2.028303861618042\n",
      "[LOG 20200511-10:22:53] epoch: 0, batch: 4779 train-loss: 1.848240613937378\n",
      "[LOG 20200511-10:22:53] epoch: 0, batch: 4780 train-loss: 1.0049940347671509\n",
      "[LOG 20200511-10:22:54] epoch: 0, batch: 4781 train-loss: 1.3612446784973145\n",
      "[LOG 20200511-10:22:54] epoch: 0, batch: 4782 train-loss: 1.3327810764312744\n",
      "[LOG 20200511-10:22:54] epoch: 0, batch: 4783 train-loss: 0.7360543012619019\n",
      "[LOG 20200511-10:22:54] epoch: 0, batch: 4784 train-loss: 1.2242724895477295\n",
      "[LOG 20200511-10:22:54] epoch: 0, batch: 4785 train-loss: 1.8836883306503296\n",
      "[LOG 20200511-10:22:54] epoch: 0, batch: 4786 train-loss: 0.7283529043197632\n",
      "[LOG 20200511-10:22:54] epoch: 0, batch: 4787 train-loss: 2.0146775245666504\n",
      "[LOG 20200511-10:22:55] epoch: 0, batch: 4788 train-loss: 1.0365697145462036\n",
      "[LOG 20200511-10:22:55] epoch: 0, batch: 4789 train-loss: 2.58073091506958\n",
      "[LOG 20200511-10:22:55] epoch: 0, batch: 4790 train-loss: 1.416050672531128\n",
      "[LOG 20200511-10:22:55] epoch: 0, batch: 4791 train-loss: 1.238037109375\n",
      "[LOG 20200511-10:22:55] epoch: 0, batch: 4792 train-loss: 1.0474843978881836\n",
      "[LOG 20200511-10:22:55] epoch: 0, batch: 4793 train-loss: 2.492769956588745\n",
      "[LOG 20200511-10:22:55] epoch: 0, batch: 4794 train-loss: 1.611209511756897\n",
      "[LOG 20200511-10:22:55] epoch: 0, batch: 4795 train-loss: 0.8249738812446594\n",
      "[LOG 20200511-10:22:56] epoch: 0, batch: 4796 train-loss: 2.0400376319885254\n",
      "[LOG 20200511-10:22:56] epoch: 0, batch: 4797 train-loss: 0.7651495933532715\n",
      "[LOG 20200511-10:22:56] epoch: 0, batch: 4798 train-loss: 1.130291223526001\n",
      "[LOG 20200511-10:22:56] epoch: 0, batch: 4799 train-loss: 0.9297038912773132\n",
      "[LOG 20200511-10:22:56] epoch: 0, batch: 4800 train-loss: 3.0391643047332764\n",
      "[LOG 20200511-10:22:56] epoch: 0, batch: 4801 train-loss: 0.5167157053947449\n",
      "[LOG 20200511-10:22:56] epoch: 0, batch: 4802 train-loss: 1.899693489074707\n",
      "[LOG 20200511-10:22:56] epoch: 0, batch: 4803 train-loss: 1.1935596466064453\n",
      "[LOG 20200511-10:22:56] epoch: 0, batch: 4804 train-loss: 2.4994168281555176\n",
      "[LOG 20200511-10:22:57] epoch: 0, batch: 4805 train-loss: 1.1163486242294312\n",
      "[LOG 20200511-10:22:57] epoch: 0, batch: 4806 train-loss: 1.7031810283660889\n",
      "[LOG 20200511-10:22:57] epoch: 0, batch: 4807 train-loss: 0.6092516183853149\n",
      "[LOG 20200511-10:22:57] epoch: 0, batch: 4808 train-loss: 1.8089613914489746\n",
      "[LOG 20200511-10:22:57] epoch: 0, batch: 4809 train-loss: 1.167740821838379\n",
      "[LOG 20200511-10:22:57] epoch: 0, batch: 4810 train-loss: 1.3836253881454468\n",
      "[LOG 20200511-10:22:57] epoch: 0, batch: 4811 train-loss: 1.4094293117523193\n",
      "[LOG 20200511-10:22:57] epoch: 0, batch: 4812 train-loss: 2.587959051132202\n",
      "[LOG 20200511-10:22:57] epoch: 0, batch: 4813 train-loss: 0.4003235399723053\n",
      "[LOG 20200511-10:22:57] epoch: 0, batch: 4814 train-loss: 1.185953974723816\n",
      "[LOG 20200511-10:22:58] epoch: 0, batch: 4815 train-loss: 0.9734452366828918\n",
      "[LOG 20200511-10:22:58] epoch: 0, batch: 4816 train-loss: 1.848111629486084\n",
      "[LOG 20200511-10:22:58] epoch: 0, batch: 4817 train-loss: 0.7751754522323608\n",
      "[LOG 20200511-10:22:58] epoch: 0, batch: 4818 train-loss: 1.6421942710876465\n",
      "[LOG 20200511-10:22:58] epoch: 0, batch: 4819 train-loss: 1.105811595916748\n",
      "[LOG 20200511-10:22:58] epoch: 0, batch: 4820 train-loss: 0.9559487700462341\n",
      "[LOG 20200511-10:22:58] epoch: 0, batch: 4821 train-loss: 1.1100908517837524\n",
      "[LOG 20200511-10:22:58] epoch: 0, batch: 4822 train-loss: 1.4552170038223267\n",
      "[LOG 20200511-10:22:58] epoch: 0, batch: 4823 train-loss: 1.203961968421936\n",
      "[LOG 20200511-10:22:59] epoch: 0, batch: 4824 train-loss: 2.6762123107910156\n",
      "[LOG 20200511-10:22:59] epoch: 0, batch: 4825 train-loss: 1.3768551349639893\n",
      "[LOG 20200511-10:22:59] epoch: 0, batch: 4826 train-loss: 1.7441648244857788\n",
      "[LOG 20200511-10:22:59] epoch: 0, batch: 4827 train-loss: 0.8455430269241333\n",
      "[LOG 20200511-10:22:59] epoch: 0, batch: 4828 train-loss: 1.5796167850494385\n",
      "[LOG 20200511-10:22:59] epoch: 0, batch: 4829 train-loss: 2.3906795978546143\n",
      "[LOG 20200511-10:22:59] epoch: 0, batch: 4830 train-loss: 1.3708513975143433\n",
      "[LOG 20200511-10:22:59] epoch: 0, batch: 4831 train-loss: 0.6730629801750183\n",
      "[LOG 20200511-10:22:59] epoch: 0, batch: 4832 train-loss: 2.0945050716400146\n",
      "[LOG 20200511-10:22:59] epoch: 0, batch: 4833 train-loss: 1.2365261316299438\n",
      "[LOG 20200511-10:23:00] epoch: 0, batch: 4834 train-loss: 0.732571005821228\n",
      "[LOG 20200511-10:23:00] epoch: 0, batch: 4835 train-loss: 1.2471736669540405\n",
      "[LOG 20200511-10:23:00] epoch: 0, batch: 4836 train-loss: 1.2926969528198242\n",
      "[LOG 20200511-10:23:00] epoch: 0, batch: 4837 train-loss: 1.6013308763504028\n",
      "[LOG 20200511-10:23:00] epoch: 0, batch: 4838 train-loss: 0.9420685768127441\n",
      "[LOG 20200511-10:23:00] epoch: 0, batch: 4839 train-loss: 1.3024388551712036\n",
      "[LOG 20200511-10:23:00] epoch: 0, batch: 4840 train-loss: 1.4948967695236206\n",
      "[LOG 20200511-10:23:00] epoch: 0, batch: 4841 train-loss: 1.7143890857696533\n",
      "[LOG 20200511-10:23:00] epoch: 0, batch: 4842 train-loss: 1.0989086627960205\n",
      "[LOG 20200511-10:23:00] epoch: 0, batch: 4843 train-loss: 1.7741938829421997\n",
      "[LOG 20200511-10:23:01] epoch: 0, batch: 4844 train-loss: 1.67155122756958\n",
      "[LOG 20200511-10:23:01] epoch: 0, batch: 4845 train-loss: 2.100581645965576\n",
      "[LOG 20200511-10:23:01] epoch: 0, batch: 4846 train-loss: 1.0149011611938477\n",
      "[LOG 20200511-10:23:01] epoch: 0, batch: 4847 train-loss: 0.6567139029502869\n",
      "[LOG 20200511-10:23:01] epoch: 0, batch: 4848 train-loss: 1.6869813203811646\n",
      "[LOG 20200511-10:23:01] epoch: 0, batch: 4849 train-loss: 1.8062517642974854\n",
      "[LOG 20200511-10:23:01] epoch: 0, batch: 4850 train-loss: 1.034833550453186\n",
      "[LOG 20200511-10:23:01] epoch: 0, batch: 4851 train-loss: 1.4121530055999756\n",
      "[LOG 20200511-10:23:01] epoch: 0, batch: 4852 train-loss: 1.616267204284668\n",
      "[LOG 20200511-10:23:01] epoch: 0, batch: 4853 train-loss: 1.6400513648986816\n",
      "[LOG 20200511-10:23:02] epoch: 0, batch: 4854 train-loss: 0.9532895088195801\n",
      "[LOG 20200511-10:23:02] epoch: 0, batch: 4855 train-loss: 1.8943047523498535\n",
      "[LOG 20200511-10:23:02] epoch: 0, batch: 4856 train-loss: 0.6834020018577576\n",
      "[LOG 20200511-10:23:02] epoch: 0, batch: 4857 train-loss: 1.4831894636154175\n",
      "[LOG 20200511-10:23:02] epoch: 0, batch: 4858 train-loss: 1.0369611978530884\n",
      "[LOG 20200511-10:23:02] epoch: 0, batch: 4859 train-loss: 0.8024555444717407\n",
      "[LOG 20200511-10:23:02] epoch: 0, batch: 4860 train-loss: 2.013000726699829\n",
      "[LOG 20200511-10:23:02] epoch: 0, batch: 4861 train-loss: 1.3416705131530762\n",
      "[LOG 20200511-10:23:02] epoch: 0, batch: 4862 train-loss: 0.6408054828643799\n",
      "[LOG 20200511-10:23:03] epoch: 0, batch: 4863 train-loss: 1.628525733947754\n",
      "[LOG 20200511-10:23:03] epoch: 0, batch: 4864 train-loss: 1.1854186058044434\n",
      "[LOG 20200511-10:23:03] epoch: 0, batch: 4865 train-loss: 1.65447199344635\n",
      "[LOG 20200511-10:23:03] epoch: 0, batch: 4866 train-loss: 1.438253402709961\n",
      "[LOG 20200511-10:23:03] epoch: 0, batch: 4867 train-loss: 1.1309309005737305\n",
      "[LOG 20200511-10:23:03] epoch: 0, batch: 4868 train-loss: 1.4588865041732788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:23:03] epoch: 0, batch: 4869 train-loss: 1.0712257623672485\n",
      "[LOG 20200511-10:23:03] epoch: 0, batch: 4870 train-loss: 2.80119252204895\n",
      "[LOG 20200511-10:23:03] epoch: 0, batch: 4871 train-loss: 1.1814846992492676\n",
      "[LOG 20200511-10:23:03] epoch: 0, batch: 4872 train-loss: 1.3727200031280518\n",
      "[LOG 20200511-10:23:04] epoch: 0, batch: 4873 train-loss: 0.9247041344642639\n",
      "[LOG 20200511-10:23:04] epoch: 0, batch: 4874 train-loss: 0.9687657356262207\n",
      "[LOG 20200511-10:23:04] epoch: 0, batch: 4875 train-loss: 2.383187770843506\n",
      "[LOG 20200511-10:23:04] epoch: 0, batch: 4876 train-loss: 0.4621943235397339\n",
      "[LOG 20200511-10:23:04] epoch: 0, batch: 4877 train-loss: 2.143514394760132\n",
      "[LOG 20200511-10:23:04] epoch: 0, batch: 4878 train-loss: 1.322744369506836\n",
      "[LOG 20200511-10:23:04] epoch: 0, batch: 4879 train-loss: 2.574179172515869\n",
      "[LOG 20200511-10:23:04] epoch: 0, batch: 4880 train-loss: 0.68964684009552\n",
      "[LOG 20200511-10:23:04] epoch: 0, batch: 4881 train-loss: 1.5151853561401367\n",
      "[LOG 20200511-10:23:04] epoch: 0, batch: 4882 train-loss: 0.710879921913147\n",
      "[LOG 20200511-10:23:05] epoch: 0, batch: 4883 train-loss: 1.433588981628418\n",
      "[LOG 20200511-10:23:05] epoch: 0, batch: 4884 train-loss: 1.6183359622955322\n",
      "[LOG 20200511-10:23:05] epoch: 0, batch: 4885 train-loss: 1.107692837715149\n",
      "[LOG 20200511-10:23:05] epoch: 0, batch: 4886 train-loss: 0.6063232421875\n",
      "[LOG 20200511-10:23:05] epoch: 0, batch: 4887 train-loss: 1.3972973823547363\n",
      "[LOG 20200511-10:23:05] epoch: 0, batch: 4888 train-loss: 1.6812505722045898\n",
      "[LOG 20200511-10:23:05] epoch: 0, batch: 4889 train-loss: 2.0245182514190674\n",
      "[LOG 20200511-10:23:05] epoch: 0, batch: 4890 train-loss: 1.0938037633895874\n",
      "[LOG 20200511-10:23:05] epoch: 0, batch: 4891 train-loss: 2.5392885208129883\n",
      "[LOG 20200511-10:23:06] epoch: 0, batch: 4892 train-loss: 1.1712150573730469\n",
      "[LOG 20200511-10:23:06] epoch: 0, batch: 4893 train-loss: 1.673172950744629\n",
      "[LOG 20200511-10:23:06] epoch: 0, batch: 4894 train-loss: 0.9663083553314209\n",
      "[LOG 20200511-10:23:06] epoch: 0, batch: 4895 train-loss: 1.718017816543579\n",
      "[LOG 20200511-10:23:06] epoch: 0, batch: 4896 train-loss: 1.1299558877944946\n",
      "[LOG 20200511-10:23:06] epoch: 0, batch: 4897 train-loss: 0.7401337623596191\n",
      "[LOG 20200511-10:23:06] epoch: 0, batch: 4898 train-loss: 2.081993341445923\n",
      "[LOG 20200511-10:23:06] epoch: 0, batch: 4899 train-loss: 1.7899798154830933\n",
      "[LOG 20200511-10:23:06] epoch: 0, batch: 4900 train-loss: 1.5204561948776245\n",
      "[LOG 20200511-10:23:06] epoch: 0, batch: 4901 train-loss: 0.8954912424087524\n",
      "[LOG 20200511-10:23:07] epoch: 0, batch: 4902 train-loss: 0.8972397446632385\n",
      "[LOG 20200511-10:23:07] epoch: 0, batch: 4903 train-loss: 0.9875073432922363\n",
      "[LOG 20200511-10:23:07] epoch: 0, batch: 4904 train-loss: 0.36890214681625366\n",
      "[LOG 20200511-10:23:07] epoch: 0, batch: 4905 train-loss: 1.0968574285507202\n",
      "[LOG 20200511-10:23:07] epoch: 0, batch: 4906 train-loss: 1.215294361114502\n",
      "[LOG 20200511-10:23:07] epoch: 0, batch: 4907 train-loss: 1.233567714691162\n",
      "[LOG 20200511-10:23:07] epoch: 0, batch: 4908 train-loss: 0.935371458530426\n",
      "[LOG 20200511-10:23:07] epoch: 0, batch: 4909 train-loss: 1.1473429203033447\n",
      "[LOG 20200511-10:23:07] epoch: 0, batch: 4910 train-loss: 1.0467078685760498\n",
      "[LOG 20200511-10:23:07] epoch: 0, batch: 4911 train-loss: 0.9410479068756104\n",
      "[LOG 20200511-10:23:08] epoch: 0, batch: 4912 train-loss: 1.050803780555725\n",
      "[LOG 20200511-10:23:08] epoch: 0, batch: 4913 train-loss: 2.386272430419922\n",
      "[LOG 20200511-10:23:08] epoch: 0, batch: 4914 train-loss: 1.357590675354004\n",
      "[LOG 20200511-10:23:08] epoch: 0, batch: 4915 train-loss: 1.5751640796661377\n",
      "[LOG 20200511-10:23:08] epoch: 0, batch: 4916 train-loss: 1.0480244159698486\n",
      "[LOG 20200511-10:23:08] epoch: 0, batch: 4917 train-loss: 1.8207497596740723\n",
      "[LOG 20200511-10:23:08] epoch: 0, batch: 4918 train-loss: 0.42988455295562744\n",
      "[LOG 20200511-10:23:08] epoch: 0, batch: 4919 train-loss: 1.314469814300537\n",
      "[LOG 20200511-10:23:08] epoch: 0, batch: 4920 train-loss: 1.3818409442901611\n",
      "[LOG 20200511-10:23:08] epoch: 0, batch: 4921 train-loss: 1.85121750831604\n",
      "[LOG 20200511-10:23:09] epoch: 0, batch: 4922 train-loss: 1.2706702947616577\n",
      "[LOG 20200511-10:23:09] epoch: 0, batch: 4923 train-loss: 2.0314292907714844\n",
      "[LOG 20200511-10:23:09] epoch: 0, batch: 4924 train-loss: 1.4731578826904297\n",
      "[LOG 20200511-10:23:09] epoch: 0, batch: 4925 train-loss: 0.9290503263473511\n",
      "[LOG 20200511-10:23:09] epoch: 0, batch: 4926 train-loss: 1.1825945377349854\n",
      "[LOG 20200511-10:23:09] epoch: 0, batch: 4927 train-loss: 1.9172356128692627\n",
      "[LOG 20200511-10:23:09] epoch: 0, batch: 4928 train-loss: 2.471482276916504\n",
      "[LOG 20200511-10:23:09] epoch: 0, batch: 4929 train-loss: 1.1904138326644897\n",
      "[LOG 20200511-10:23:09] epoch: 0, batch: 4930 train-loss: 1.5172245502471924\n",
      "[LOG 20200511-10:23:09] epoch: 0, batch: 4931 train-loss: 2.745762348175049\n",
      "[LOG 20200511-10:23:10] epoch: 0, batch: 4932 train-loss: 1.2997967004776\n",
      "[LOG 20200511-10:23:10] epoch: 0, batch: 4933 train-loss: 1.6075342893600464\n",
      "[LOG 20200511-10:23:10] epoch: 0, batch: 4934 train-loss: 1.1836473941802979\n",
      "[LOG 20200511-10:23:10] epoch: 0, batch: 4935 train-loss: 1.4871680736541748\n",
      "[LOG 20200511-10:23:10] epoch: 0, batch: 4936 train-loss: 1.8651028871536255\n",
      "[LOG 20200511-10:23:10] epoch: 0, batch: 4937 train-loss: 1.5314189195632935\n",
      "[LOG 20200511-10:23:10] epoch: 0, batch: 4938 train-loss: 1.2585744857788086\n",
      "[LOG 20200511-10:23:10] epoch: 0, batch: 4939 train-loss: 1.8207001686096191\n",
      "[LOG 20200511-10:23:10] epoch: 0, batch: 4940 train-loss: 1.4779325723648071\n",
      "[LOG 20200511-10:23:10] epoch: 0, batch: 4941 train-loss: 1.3089170455932617\n",
      "[LOG 20200511-10:23:11] epoch: 0, batch: 4942 train-loss: 1.8333537578582764\n",
      "[LOG 20200511-10:23:11] epoch: 0, batch: 4943 train-loss: 1.1510281562805176\n",
      "[LOG 20200511-10:23:11] epoch: 0, batch: 4944 train-loss: 0.9659042358398438\n",
      "[LOG 20200511-10:23:11] epoch: 0, batch: 4945 train-loss: 1.1222056150436401\n",
      "[LOG 20200511-10:23:11] epoch: 0, batch: 4946 train-loss: 2.5721049308776855\n",
      "[LOG 20200511-10:23:11] epoch: 0, batch: 4947 train-loss: 1.797990322113037\n",
      "[LOG 20200511-10:23:11] epoch: 0, batch: 4948 train-loss: 2.304619789123535\n",
      "[LOG 20200511-10:23:11] epoch: 0, batch: 4949 train-loss: 1.8647961616516113\n",
      "[LOG 20200511-10:23:11] epoch: 0, batch: 4950 train-loss: 1.3390252590179443\n",
      "[LOG 20200511-10:23:11] epoch: 0, batch: 4951 train-loss: 1.8837870359420776\n",
      "[LOG 20200511-10:23:12] epoch: 0, batch: 4952 train-loss: 0.7890658378601074\n",
      "[LOG 20200511-10:23:12] epoch: 0, batch: 4953 train-loss: 1.8136777877807617\n",
      "[LOG 20200511-10:23:12] epoch: 0, batch: 4954 train-loss: 1.9922879934310913\n",
      "[LOG 20200511-10:23:12] epoch: 0, batch: 4955 train-loss: 1.6155245304107666\n",
      "[LOG 20200511-10:23:12] epoch: 0, batch: 4956 train-loss: 0.5257605910301208\n",
      "[LOG 20200511-10:23:12] epoch: 0, batch: 4957 train-loss: 2.4427990913391113\n",
      "[LOG 20200511-10:23:12] epoch: 0, batch: 4958 train-loss: 1.6170847415924072\n",
      "[LOG 20200511-10:23:12] epoch: 0, batch: 4959 train-loss: 1.7955223321914673\n",
      "[LOG 20200511-10:23:12] epoch: 0, batch: 4960 train-loss: 1.4058306217193604\n",
      "[LOG 20200511-10:23:13] epoch: 0, batch: 4961 train-loss: 2.0322208404541016\n",
      "[LOG 20200511-10:23:13] epoch: 0, batch: 4962 train-loss: 0.5717416405677795\n",
      "[LOG 20200511-10:23:13] epoch: 0, batch: 4963 train-loss: 1.8293334245681763\n",
      "[LOG 20200511-10:23:13] epoch: 0, batch: 4964 train-loss: 1.1712191104888916\n",
      "[LOG 20200511-10:23:13] epoch: 0, batch: 4965 train-loss: 1.0829558372497559\n",
      "[LOG 20200511-10:23:13] epoch: 0, batch: 4966 train-loss: 1.2749204635620117\n",
      "[LOG 20200511-10:23:13] epoch: 0, batch: 4967 train-loss: 2.3330485820770264\n",
      "[LOG 20200511-10:23:13] epoch: 0, batch: 4968 train-loss: 0.3905787169933319\n",
      "[LOG 20200511-10:23:13] epoch: 0, batch: 4969 train-loss: 1.8564798831939697\n",
      "[LOG 20200511-10:23:13] epoch: 0, batch: 4970 train-loss: 0.7220968008041382\n",
      "[LOG 20200511-10:23:14] epoch: 0, batch: 4971 train-loss: 0.6317765116691589\n",
      "[LOG 20200511-10:23:14] epoch: 0, batch: 4972 train-loss: 0.4393642246723175\n",
      "[LOG 20200511-10:23:14] epoch: 0, batch: 4973 train-loss: 1.6423075199127197\n",
      "[LOG 20200511-10:23:14] epoch: 0, batch: 4974 train-loss: 1.3247750997543335\n",
      "[LOG 20200511-10:23:14] epoch: 0, batch: 4975 train-loss: 0.6991682052612305\n",
      "[LOG 20200511-10:23:14] epoch: 0, batch: 4976 train-loss: 1.104902744293213\n",
      "[LOG 20200511-10:23:14] epoch: 0, batch: 4977 train-loss: 1.7081644535064697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:23:14] epoch: 0, batch: 4978 train-loss: 1.034332036972046\n",
      "[LOG 20200511-10:23:14] epoch: 0, batch: 4979 train-loss: 0.8664648532867432\n",
      "[LOG 20200511-10:23:14] epoch: 0, batch: 4980 train-loss: 0.6790313124656677\n",
      "[LOG 20200511-10:23:15] epoch: 0, batch: 4981 train-loss: 2.2072293758392334\n",
      "[LOG 20200511-10:23:15] epoch: 0, batch: 4982 train-loss: 1.682352066040039\n",
      "[LOG 20200511-10:23:15] epoch: 0, batch: 4983 train-loss: 1.6182596683502197\n",
      "[LOG 20200511-10:23:15] epoch: 0, batch: 4984 train-loss: 0.647000789642334\n",
      "[LOG 20200511-10:23:15] epoch: 0, batch: 4985 train-loss: 1.6657919883728027\n",
      "[LOG 20200511-10:23:15] epoch: 0, batch: 4986 train-loss: 1.1595441102981567\n",
      "[LOG 20200511-10:23:15] epoch: 0, batch: 4987 train-loss: 1.662237524986267\n",
      "[LOG 20200511-10:23:15] epoch: 0, batch: 4988 train-loss: 1.3341701030731201\n",
      "[LOG 20200511-10:23:15] epoch: 0, batch: 4989 train-loss: 1.727057933807373\n",
      "[LOG 20200511-10:23:15] epoch: 0, batch: 4990 train-loss: 0.5937897562980652\n",
      "[LOG 20200511-10:23:16] epoch: 0, batch: 4991 train-loss: 1.4261072874069214\n",
      "[LOG 20200511-10:23:16] epoch: 0, batch: 4992 train-loss: 0.9137653708457947\n",
      "[LOG 20200511-10:23:16] epoch: 0, batch: 4993 train-loss: 2.1923000812530518\n",
      "[LOG 20200511-10:23:16] epoch: 0, batch: 4994 train-loss: 0.7100273370742798\n",
      "[LOG 20200511-10:23:16] epoch: 0, batch: 4995 train-loss: 0.9552305340766907\n",
      "[LOG 20200511-10:23:16] epoch: 0, batch: 4996 train-loss: 1.6899099349975586\n",
      "[LOG 20200511-10:23:16] epoch: 0, batch: 4997 train-loss: 2.2714667320251465\n",
      "[LOG 20200511-10:23:16] epoch: 0, batch: 4998 train-loss: 2.6406242847442627\n",
      "[LOG 20200511-10:23:16] epoch: 0, batch: 4999 train-loss: 1.5177704095840454\n",
      "[LOG 20200511-10:23:17] epoch: 0, batch: 5000 train-loss: 1.6006847620010376\n",
      "[LOG 20200511-10:23:17] epoch: 0, batch: 5001 train-loss: 1.7547686100006104\n",
      "[LOG 20200511-10:23:17] epoch: 0, batch: 5002 train-loss: 0.9473000764846802\n",
      "[LOG 20200511-10:23:17] epoch: 0, batch: 5003 train-loss: 1.2319083213806152\n",
      "[LOG 20200511-10:23:17] epoch: 0, batch: 5004 train-loss: 1.9713551998138428\n",
      "[LOG 20200511-10:23:17] epoch: 0, batch: 5005 train-loss: 1.735215663909912\n",
      "[LOG 20200511-10:23:17] epoch: 0, batch: 5006 train-loss: 0.45320117473602295\n",
      "[LOG 20200511-10:23:17] epoch: 0, batch: 5007 train-loss: 1.2626008987426758\n",
      "[LOG 20200511-10:23:17] epoch: 0, batch: 5008 train-loss: 1.5648844242095947\n",
      "[LOG 20200511-10:23:17] epoch: 0, batch: 5009 train-loss: 0.8774800300598145\n",
      "[LOG 20200511-10:23:18] epoch: 0, batch: 5010 train-loss: 1.9793763160705566\n",
      "[LOG 20200511-10:23:18] epoch: 0, batch: 5011 train-loss: 1.701139211654663\n",
      "[LOG 20200511-10:23:18] epoch: 0, batch: 5012 train-loss: 1.5237468481063843\n",
      "[LOG 20200511-10:23:18] epoch: 0, batch: 5013 train-loss: 1.2756750583648682\n",
      "[LOG 20200511-10:23:18] epoch: 0, batch: 5014 train-loss: 1.7369039058685303\n",
      "[LOG 20200511-10:23:18] epoch: 0, batch: 5015 train-loss: 1.5541375875473022\n",
      "[LOG 20200511-10:23:18] epoch: 0, batch: 5016 train-loss: 1.16819167137146\n",
      "[LOG 20200511-10:23:18] epoch: 0, batch: 5017 train-loss: 1.3598710298538208\n",
      "[LOG 20200511-10:23:18] epoch: 0, batch: 5018 train-loss: 1.2425490617752075\n",
      "[LOG 20200511-10:23:18] epoch: 0, batch: 5019 train-loss: 0.7933955192565918\n",
      "[LOG 20200511-10:23:19] epoch: 0, batch: 5020 train-loss: 1.3703844547271729\n",
      "[LOG 20200511-10:23:19] epoch: 0, batch: 5021 train-loss: 1.701857328414917\n",
      "[LOG 20200511-10:23:19] epoch: 0, batch: 5022 train-loss: 0.6731956601142883\n",
      "[LOG 20200511-10:23:19] epoch: 0, batch: 5023 train-loss: 0.44649460911750793\n",
      "[LOG 20200511-10:23:19] epoch: 0, batch: 5024 train-loss: 0.927183985710144\n",
      "[LOG 20200511-10:23:19] epoch: 0, batch: 5025 train-loss: 0.6849632263183594\n",
      "[LOG 20200511-10:23:19] epoch: 0, batch: 5026 train-loss: 1.977027177810669\n",
      "[LOG 20200511-10:23:19] epoch: 0, batch: 5027 train-loss: 1.4802743196487427\n",
      "[LOG 20200511-10:23:19] epoch: 0, batch: 5028 train-loss: 2.142086982727051\n",
      "[LOG 20200511-10:23:20] epoch: 0, batch: 5029 train-loss: 1.15007483959198\n",
      "[LOG 20200511-10:23:20] epoch: 0, batch: 5030 train-loss: 1.840267300605774\n",
      "[LOG 20200511-10:23:20] epoch: 0, batch: 5031 train-loss: 1.3301268815994263\n",
      "[LOG 20200511-10:23:20] epoch: 0, batch: 5032 train-loss: 1.252227544784546\n",
      "[LOG 20200511-10:23:20] epoch: 0, batch: 5033 train-loss: 1.6330558061599731\n",
      "[LOG 20200511-10:23:20] epoch: 0, batch: 5034 train-loss: 0.8150362968444824\n",
      "[LOG 20200511-10:23:20] epoch: 0, batch: 5035 train-loss: 1.2615455389022827\n",
      "[LOG 20200511-10:23:20] epoch: 0, batch: 5036 train-loss: 0.7273958325386047\n",
      "[LOG 20200511-10:23:20] epoch: 0, batch: 5037 train-loss: 1.0410854816436768\n",
      "[LOG 20200511-10:23:20] epoch: 0, batch: 5038 train-loss: 2.0153396129608154\n",
      "[LOG 20200511-10:23:21] epoch: 0, batch: 5039 train-loss: 1.9934290647506714\n",
      "[LOG 20200511-10:23:21] epoch: 0, batch: 5040 train-loss: 1.2354339361190796\n",
      "[LOG 20200511-10:23:21] epoch: 0, batch: 5041 train-loss: 1.3381640911102295\n",
      "[LOG 20200511-10:23:21] epoch: 0, batch: 5042 train-loss: 1.2857319116592407\n",
      "[LOG 20200511-10:23:21] epoch: 0, batch: 5043 train-loss: 1.9938921928405762\n",
      "[LOG 20200511-10:23:21] epoch: 0, batch: 5044 train-loss: 1.3026217222213745\n",
      "[LOG 20200511-10:23:21] epoch: 0, batch: 5045 train-loss: 0.79549640417099\n",
      "[LOG 20200511-10:23:21] epoch: 0, batch: 5046 train-loss: 0.8730406761169434\n",
      "[LOG 20200511-10:23:21] epoch: 0, batch: 5047 train-loss: 1.3017622232437134\n",
      "[LOG 20200511-10:23:21] epoch: 0, batch: 5048 train-loss: 1.0845987796783447\n",
      "[LOG 20200511-10:23:22] epoch: 0, batch: 5049 train-loss: 0.8511366248130798\n",
      "[LOG 20200511-10:23:22] epoch: 0, batch: 5050 train-loss: 2.0489068031311035\n",
      "[LOG 20200511-10:23:22] epoch: 0, batch: 5051 train-loss: 3.282055139541626\n",
      "[LOG 20200511-10:23:22] epoch: 0, batch: 5052 train-loss: 0.6595995426177979\n",
      "[LOG 20200511-10:23:22] epoch: 0, batch: 5053 train-loss: 0.984326958656311\n",
      "[LOG 20200511-10:23:22] epoch: 0, batch: 5054 train-loss: 1.0548540353775024\n",
      "[LOG 20200511-10:23:22] epoch: 0, batch: 5055 train-loss: 1.415162444114685\n",
      "[LOG 20200511-10:23:22] epoch: 0, batch: 5056 train-loss: 1.7760852575302124\n",
      "[LOG 20200511-10:23:22] epoch: 0, batch: 5057 train-loss: 1.7459580898284912\n",
      "[LOG 20200511-10:23:23] epoch: 0, batch: 5058 train-loss: 0.8770369291305542\n",
      "[LOG 20200511-10:23:23] epoch: 0, batch: 5059 train-loss: 0.6773883104324341\n",
      "[LOG 20200511-10:23:23] epoch: 0, batch: 5060 train-loss: 1.4687422513961792\n",
      "[LOG 20200511-10:23:23] epoch: 0, batch: 5061 train-loss: 1.6696739196777344\n",
      "[LOG 20200511-10:23:23] epoch: 0, batch: 5062 train-loss: 1.3101634979248047\n",
      "[LOG 20200511-10:23:23] epoch: 0, batch: 5063 train-loss: 1.4076778888702393\n",
      "[LOG 20200511-10:23:23] epoch: 0, batch: 5064 train-loss: 2.3081228733062744\n",
      "[LOG 20200511-10:23:23] epoch: 0, batch: 5065 train-loss: 1.8606998920440674\n",
      "[LOG 20200511-10:23:23] epoch: 0, batch: 5066 train-loss: 1.27659010887146\n",
      "[LOG 20200511-10:23:23] epoch: 0, batch: 5067 train-loss: 1.8865008354187012\n",
      "[LOG 20200511-10:23:24] epoch: 0, batch: 5068 train-loss: 2.1124589443206787\n",
      "[LOG 20200511-10:23:24] epoch: 0, batch: 5069 train-loss: 1.0006171464920044\n",
      "[LOG 20200511-10:23:24] epoch: 0, batch: 5070 train-loss: 1.443866491317749\n",
      "[LOG 20200511-10:23:24] epoch: 0, batch: 5071 train-loss: 1.0265525579452515\n",
      "[LOG 20200511-10:23:24] epoch: 0, batch: 5072 train-loss: 1.7420390844345093\n",
      "[LOG 20200511-10:23:24] epoch: 0, batch: 5073 train-loss: 1.4109106063842773\n",
      "[LOG 20200511-10:23:24] epoch: 0, batch: 5074 train-loss: 1.328403115272522\n",
      "[LOG 20200511-10:23:24] epoch: 0, batch: 5075 train-loss: 1.1281654834747314\n",
      "[LOG 20200511-10:23:24] epoch: 0, batch: 5076 train-loss: 2.1762118339538574\n",
      "[LOG 20200511-10:23:24] epoch: 0, batch: 5077 train-loss: 1.1966243982315063\n",
      "[LOG 20200511-10:23:25] epoch: 0, batch: 5078 train-loss: 0.8394219279289246\n",
      "[LOG 20200511-10:23:25] epoch: 0, batch: 5079 train-loss: 1.3785148859024048\n",
      "[LOG 20200511-10:23:25] epoch: 0, batch: 5080 train-loss: 1.791024088859558\n",
      "[LOG 20200511-10:23:25] epoch: 0, batch: 5081 train-loss: 1.6655805110931396\n",
      "[LOG 20200511-10:23:25] epoch: 0, batch: 5082 train-loss: 1.5961235761642456\n",
      "[LOG 20200511-10:23:25] epoch: 0, batch: 5083 train-loss: 1.1976689100265503\n",
      "[LOG 20200511-10:23:25] epoch: 0, batch: 5084 train-loss: 2.0788233280181885\n",
      "[LOG 20200511-10:23:25] epoch: 0, batch: 5085 train-loss: 1.6411588191986084\n",
      "[LOG 20200511-10:23:25] epoch: 0, batch: 5086 train-loss: 1.3965568542480469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:23:26] epoch: 0, batch: 5087 train-loss: 0.9723148941993713\n",
      "[LOG 20200511-10:23:26] epoch: 0, batch: 5088 train-loss: 2.8115787506103516\n",
      "[LOG 20200511-10:23:26] epoch: 0, batch: 5089 train-loss: 1.450181484222412\n",
      "[LOG 20200511-10:23:26] epoch: 0, batch: 5090 train-loss: 1.2328788042068481\n",
      "[LOG 20200511-10:23:26] epoch: 0, batch: 5091 train-loss: 1.8170429468154907\n",
      "[LOG 20200511-10:23:26] epoch: 0, batch: 5092 train-loss: 1.8219406604766846\n",
      "[LOG 20200511-10:23:26] epoch: 0, batch: 5093 train-loss: 0.6566972136497498\n",
      "[LOG 20200511-10:23:26] epoch: 0, batch: 5094 train-loss: 1.7138819694519043\n",
      "[LOG 20200511-10:23:26] epoch: 0, batch: 5095 train-loss: 1.3943395614624023\n",
      "[LOG 20200511-10:23:27] epoch: 0, batch: 5096 train-loss: 1.2004330158233643\n",
      "[LOG 20200511-10:23:27] epoch: 0, batch: 5097 train-loss: 1.503671646118164\n",
      "[LOG 20200511-10:23:27] epoch: 0, batch: 5098 train-loss: 1.1968958377838135\n",
      "[LOG 20200511-10:23:27] epoch: 0, batch: 5099 train-loss: 1.0790358781814575\n",
      "[LOG 20200511-10:23:27] epoch: 0, batch: 5100 train-loss: 1.0996960401535034\n",
      "[LOG 20200511-10:23:27] epoch: 0, batch: 5101 train-loss: 2.192089557647705\n",
      "[LOG 20200511-10:23:27] epoch: 0, batch: 5102 train-loss: 1.4307026863098145\n",
      "[LOG 20200511-10:23:27] epoch: 0, batch: 5103 train-loss: 2.793863534927368\n",
      "[LOG 20200511-10:23:27] epoch: 0, batch: 5104 train-loss: 2.5104031562805176\n",
      "[LOG 20200511-10:23:27] epoch: 0, batch: 5105 train-loss: 1.7436326742172241\n",
      "[LOG 20200511-10:23:28] epoch: 0, batch: 5106 train-loss: 0.5983672738075256\n",
      "[LOG 20200511-10:23:28] epoch: 0, batch: 5107 train-loss: 0.6880669593811035\n",
      "[LOG 20200511-10:23:28] epoch: 0, batch: 5108 train-loss: 1.5114045143127441\n",
      "[LOG 20200511-10:23:28] epoch: 0, batch: 5109 train-loss: 1.4953275918960571\n",
      "[LOG 20200511-10:23:28] epoch: 0, batch: 5110 train-loss: 1.1965653896331787\n",
      "[LOG 20200511-10:23:28] epoch: 0, batch: 5111 train-loss: 0.9698194861412048\n",
      "[LOG 20200511-10:23:28] epoch: 0, batch: 5112 train-loss: 1.1556777954101562\n",
      "[LOG 20200511-10:23:28] epoch: 0, batch: 5113 train-loss: 1.0436385869979858\n",
      "[LOG 20200511-10:23:28] epoch: 0, batch: 5114 train-loss: 1.495023488998413\n",
      "[LOG 20200511-10:23:28] epoch: 0, batch: 5115 train-loss: 0.6321829557418823\n",
      "[LOG 20200511-10:23:29] epoch: 0, batch: 5116 train-loss: 1.916890025138855\n",
      "[LOG 20200511-10:23:29] epoch: 0, batch: 5117 train-loss: 1.232229471206665\n",
      "[LOG 20200511-10:23:29] epoch: 0, batch: 5118 train-loss: 0.6206724047660828\n",
      "[LOG 20200511-10:23:29] epoch: 0, batch: 5119 train-loss: 1.370242953300476\n",
      "[LOG 20200511-10:23:29] epoch: 0, batch: 5120 train-loss: 1.4883275032043457\n",
      "[LOG 20200511-10:23:29] epoch: 0, batch: 5121 train-loss: 1.1358205080032349\n",
      "[LOG 20200511-10:23:29] epoch: 0, batch: 5122 train-loss: 1.2032941579818726\n",
      "[LOG 20200511-10:23:29] epoch: 0, batch: 5123 train-loss: 2.1315484046936035\n",
      "[LOG 20200511-10:23:29] epoch: 0, batch: 5124 train-loss: 1.8865877389907837\n",
      "[LOG 20200511-10:23:30] epoch: 0, batch: 5125 train-loss: 1.002066969871521\n",
      "[LOG 20200511-10:23:30] epoch: 0, batch: 5126 train-loss: 1.483745813369751\n",
      "[LOG 20200511-10:23:30] epoch: 0, batch: 5127 train-loss: 0.9790248870849609\n",
      "[LOG 20200511-10:23:30] epoch: 0, batch: 5128 train-loss: 1.425124168395996\n",
      "[LOG 20200511-10:23:30] epoch: 0, batch: 5129 train-loss: 0.754546046257019\n",
      "[LOG 20200511-10:23:30] epoch: 0, batch: 5130 train-loss: 1.5393610000610352\n",
      "[LOG 20200511-10:23:30] epoch: 0, batch: 5131 train-loss: 0.6030480265617371\n",
      "[LOG 20200511-10:23:30] epoch: 0, batch: 5132 train-loss: 1.35654616355896\n",
      "[LOG 20200511-10:23:30] epoch: 0, batch: 5133 train-loss: 1.138580560684204\n",
      "[LOG 20200511-10:23:30] epoch: 0, batch: 5134 train-loss: 2.1628243923187256\n",
      "[LOG 20200511-10:23:31] epoch: 0, batch: 5135 train-loss: 0.9770538210868835\n",
      "[LOG 20200511-10:23:31] epoch: 0, batch: 5136 train-loss: 1.8014674186706543\n",
      "[LOG 20200511-10:23:31] epoch: 0, batch: 5137 train-loss: 0.9472928643226624\n",
      "[LOG 20200511-10:23:31] epoch: 0, batch: 5138 train-loss: 2.959019899368286\n",
      "[LOG 20200511-10:23:31] epoch: 0, batch: 5139 train-loss: 1.2793378829956055\n",
      "[LOG 20200511-10:23:31] epoch: 0, batch: 5140 train-loss: 2.3418524265289307\n",
      "[LOG 20200511-10:23:31] epoch: 0, batch: 5141 train-loss: 1.2503989934921265\n",
      "[LOG 20200511-10:23:31] epoch: 0, batch: 5142 train-loss: 1.698353886604309\n",
      "[LOG 20200511-10:23:31] epoch: 0, batch: 5143 train-loss: 2.117426872253418\n",
      "[LOG 20200511-10:23:31] epoch: 0, batch: 5144 train-loss: 0.8891202807426453\n",
      "[LOG 20200511-10:23:32] epoch: 0, batch: 5145 train-loss: 1.816851258277893\n",
      "[LOG 20200511-10:23:32] epoch: 0, batch: 5146 train-loss: 1.1005650758743286\n",
      "[LOG 20200511-10:23:32] epoch: 0, batch: 5147 train-loss: 0.850609302520752\n",
      "[LOG 20200511-10:23:32] epoch: 0, batch: 5148 train-loss: 1.5554747581481934\n",
      "[LOG 20200511-10:23:32] epoch: 0, batch: 5149 train-loss: 1.056766390800476\n",
      "[LOG 20200511-10:23:32] epoch: 0, batch: 5150 train-loss: 0.8613883256912231\n",
      "[LOG 20200511-10:23:32] epoch: 0, batch: 5151 train-loss: 1.9707860946655273\n",
      "[LOG 20200511-10:23:32] epoch: 0, batch: 5152 train-loss: 1.612654447555542\n",
      "[LOG 20200511-10:23:32] epoch: 0, batch: 5153 train-loss: 1.7819386720657349\n",
      "[LOG 20200511-10:23:32] epoch: 0, batch: 5154 train-loss: 1.9525408744812012\n",
      "[LOG 20200511-10:23:33] epoch: 0, batch: 5155 train-loss: 2.494797706604004\n",
      "[LOG 20200511-10:23:33] epoch: 0, batch: 5156 train-loss: 1.9253019094467163\n",
      "[LOG 20200511-10:23:33] epoch: 0, batch: 5157 train-loss: 0.9571197628974915\n",
      "[LOG 20200511-10:23:33] epoch: 0, batch: 5158 train-loss: 0.9271315336227417\n",
      "[LOG 20200511-10:23:33] epoch: 0, batch: 5159 train-loss: 1.4011449813842773\n",
      "[LOG 20200511-10:23:33] epoch: 0, batch: 5160 train-loss: 1.3990159034729004\n",
      "[LOG 20200511-10:23:33] epoch: 0, batch: 5161 train-loss: 0.6862507462501526\n",
      "[LOG 20200511-10:23:33] epoch: 0, batch: 5162 train-loss: 0.9389662742614746\n",
      "[LOG 20200511-10:23:33] epoch: 0, batch: 5163 train-loss: 1.761314868927002\n",
      "[LOG 20200511-10:23:33] epoch: 0, batch: 5164 train-loss: 2.08980131149292\n",
      "[LOG 20200511-10:23:34] epoch: 0, batch: 5165 train-loss: 1.591137170791626\n",
      "[LOG 20200511-10:23:34] epoch: 0, batch: 5166 train-loss: 2.453326940536499\n",
      "[LOG 20200511-10:23:34] epoch: 0, batch: 5167 train-loss: 2.0223803520202637\n",
      "[LOG 20200511-10:23:34] epoch: 0, batch: 5168 train-loss: 1.7441692352294922\n",
      "[LOG 20200511-10:23:34] epoch: 0, batch: 5169 train-loss: 1.659928321838379\n",
      "[LOG 20200511-10:23:34] epoch: 0, batch: 5170 train-loss: 1.214332103729248\n",
      "[LOG 20200511-10:23:34] epoch: 0, batch: 5171 train-loss: 1.027468204498291\n",
      "[LOG 20200511-10:23:34] epoch: 0, batch: 5172 train-loss: 1.5243875980377197\n",
      "[LOG 20200511-10:23:34] epoch: 0, batch: 5173 train-loss: 1.6999603509902954\n",
      "[LOG 20200511-10:23:35] epoch: 0, batch: 5174 train-loss: 0.8224915862083435\n",
      "[LOG 20200511-10:23:35] epoch: 0, batch: 5175 train-loss: 0.45457664132118225\n",
      "[LOG 20200511-10:23:35] epoch: 0, batch: 5176 train-loss: 1.0077838897705078\n",
      "[LOG 20200511-10:23:35] epoch: 0, batch: 5177 train-loss: 1.4840178489685059\n",
      "[LOG 20200511-10:23:35] epoch: 0, batch: 5178 train-loss: 0.6253079175949097\n",
      "[LOG 20200511-10:23:35] epoch: 0, batch: 5179 train-loss: 3.889995574951172\n",
      "[LOG 20200511-10:23:35] epoch: 0, batch: 5180 train-loss: 1.0290247201919556\n",
      "[LOG 20200511-10:23:35] epoch: 0, batch: 5181 train-loss: 1.6256587505340576\n",
      "[LOG 20200511-10:23:35] epoch: 0, batch: 5182 train-loss: 1.6632014513015747\n",
      "[LOG 20200511-10:23:35] epoch: 0, batch: 5183 train-loss: 1.3026055097579956\n",
      "[LOG 20200511-10:23:36] epoch: 0, batch: 5184 train-loss: 0.9839611649513245\n",
      "[LOG 20200511-10:23:36] epoch: 0, batch: 5185 train-loss: 2.1004719734191895\n",
      "[LOG 20200511-10:23:36] epoch: 0, batch: 5186 train-loss: 1.4084669351577759\n",
      "[LOG 20200511-10:23:36] epoch: 0, batch: 5187 train-loss: 2.3576395511627197\n",
      "[LOG 20200511-10:23:36] epoch: 0, batch: 5188 train-loss: 1.449973225593567\n",
      "[LOG 20200511-10:23:36] epoch: 0, batch: 5189 train-loss: 1.7275701761245728\n",
      "[LOG 20200511-10:23:36] epoch: 0, batch: 5190 train-loss: 1.0486211776733398\n",
      "[LOG 20200511-10:23:36] epoch: 0, batch: 5191 train-loss: 1.632857322692871\n",
      "[LOG 20200511-10:23:36] epoch: 0, batch: 5192 train-loss: 1.8085720539093018\n",
      "[LOG 20200511-10:23:36] epoch: 0, batch: 5193 train-loss: 1.1234455108642578\n",
      "[LOG 20200511-10:23:37] epoch: 0, batch: 5194 train-loss: 1.5084631443023682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:23:37] epoch: 0, batch: 5195 train-loss: 0.8859641551971436\n",
      "[LOG 20200511-10:23:37] epoch: 0, batch: 5196 train-loss: 1.6825107336044312\n",
      "[LOG 20200511-10:23:37] epoch: 0, batch: 5197 train-loss: 1.0361205339431763\n",
      "[LOG 20200511-10:23:37] epoch: 0, batch: 5198 train-loss: 1.5981831550598145\n",
      "[LOG 20200511-10:23:37] epoch: 0, batch: 5199 train-loss: 1.8448840379714966\n",
      "[LOG 20200511-10:23:37] epoch: 0, batch: 5200 train-loss: 2.3233754634857178\n",
      "[LOG 20200511-10:23:37] epoch: 0, batch: 5201 train-loss: 1.5743917226791382\n",
      "[LOG 20200511-10:23:38] epoch: 0, batch: 5202 train-loss: 1.2264093160629272\n",
      "[LOG 20200511-10:23:38] epoch: 0, batch: 5203 train-loss: 1.6814613342285156\n",
      "[LOG 20200511-10:23:38] epoch: 0, batch: 5204 train-loss: 1.8430484533309937\n",
      "[LOG 20200511-10:23:38] epoch: 0, batch: 5205 train-loss: 1.5382519960403442\n",
      "[LOG 20200511-10:23:38] epoch: 0, batch: 5206 train-loss: 1.8073036670684814\n",
      "[LOG 20200511-10:23:38] epoch: 0, batch: 5207 train-loss: 0.7174524664878845\n",
      "[LOG 20200511-10:23:39] epoch: 0, batch: 5208 train-loss: 1.198532223701477\n",
      "[LOG 20200511-10:23:39] epoch: 0, batch: 5209 train-loss: 0.8827089071273804\n",
      "[LOG 20200511-10:23:39] epoch: 0, batch: 5210 train-loss: 1.8008027076721191\n",
      "[LOG 20200511-10:23:39] epoch: 0, batch: 5211 train-loss: 1.268833875656128\n",
      "[LOG 20200511-10:23:39] epoch: 0, batch: 5212 train-loss: 0.6621875166893005\n",
      "[LOG 20200511-10:23:39] epoch: 0, batch: 5213 train-loss: 1.378856897354126\n",
      "[LOG 20200511-10:23:40] epoch: 0, batch: 5214 train-loss: 2.3167080879211426\n",
      "[LOG 20200511-10:23:40] epoch: 0, batch: 5215 train-loss: 1.130384087562561\n",
      "[LOG 20200511-10:23:40] epoch: 0, batch: 5216 train-loss: 0.8073184490203857\n",
      "[LOG 20200511-10:23:40] epoch: 0, batch: 5217 train-loss: 1.3250116109848022\n",
      "[LOG 20200511-10:23:40] epoch: 0, batch: 5218 train-loss: 1.0259171724319458\n",
      "[LOG 20200511-10:23:40] epoch: 0, batch: 5219 train-loss: 1.3421401977539062\n",
      "[LOG 20200511-10:23:40] epoch: 0, batch: 5220 train-loss: 1.9076677560806274\n",
      "[LOG 20200511-10:23:41] epoch: 0, batch: 5221 train-loss: 1.668196201324463\n",
      "[LOG 20200511-10:23:41] epoch: 0, batch: 5222 train-loss: 1.1274746656417847\n",
      "[LOG 20200511-10:23:41] epoch: 0, batch: 5223 train-loss: 1.9715731143951416\n",
      "[LOG 20200511-10:23:41] epoch: 0, batch: 5224 train-loss: 1.6800405979156494\n",
      "[LOG 20200511-10:23:41] epoch: 0, batch: 5225 train-loss: 0.5060235261917114\n",
      "[LOG 20200511-10:23:41] epoch: 0, batch: 5226 train-loss: 0.9476444125175476\n",
      "[LOG 20200511-10:23:41] epoch: 0, batch: 5227 train-loss: 1.1118133068084717\n",
      "[LOG 20200511-10:23:42] epoch: 0, batch: 5228 train-loss: 2.0221478939056396\n",
      "[LOG 20200511-10:23:42] epoch: 0, batch: 5229 train-loss: 0.5425307750701904\n",
      "[LOG 20200511-10:23:42] epoch: 0, batch: 5230 train-loss: 1.2406102418899536\n",
      "[LOG 20200511-10:23:42] epoch: 0, batch: 5231 train-loss: 0.7842127084732056\n",
      "[LOG 20200511-10:23:42] epoch: 0, batch: 5232 train-loss: 0.6762014031410217\n",
      "[LOG 20200511-10:23:42] epoch: 0, batch: 5233 train-loss: 1.2914965152740479\n",
      "[LOG 20200511-10:23:42] epoch: 0, batch: 5234 train-loss: 1.883392095565796\n",
      "[LOG 20200511-10:23:43] epoch: 0, batch: 5235 train-loss: 1.9173226356506348\n",
      "[LOG 20200511-10:23:43] epoch: 0, batch: 5236 train-loss: 0.7767090797424316\n",
      "[LOG 20200511-10:23:43] epoch: 0, batch: 5237 train-loss: 1.5606372356414795\n",
      "[LOG 20200511-10:23:43] epoch: 0, batch: 5238 train-loss: 1.9945425987243652\n",
      "[LOG 20200511-10:23:43] epoch: 0, batch: 5239 train-loss: 0.8532178401947021\n",
      "[LOG 20200511-10:23:43] epoch: 0, batch: 5240 train-loss: 0.6432409882545471\n",
      "[LOG 20200511-10:23:43] epoch: 0, batch: 5241 train-loss: 1.117306113243103\n",
      "[LOG 20200511-10:23:44] epoch: 0, batch: 5242 train-loss: 1.9538954496383667\n",
      "[LOG 20200511-10:23:44] epoch: 0, batch: 5243 train-loss: 0.8658368587493896\n",
      "[LOG 20200511-10:23:44] epoch: 0, batch: 5244 train-loss: 1.5950236320495605\n",
      "[LOG 20200511-10:23:44] epoch: 0, batch: 5245 train-loss: 0.6997437477111816\n",
      "[LOG 20200511-10:23:44] epoch: 0, batch: 5246 train-loss: 0.9977821111679077\n",
      "[LOG 20200511-10:23:44] epoch: 0, batch: 5247 train-loss: 1.0960313081741333\n",
      "[LOG 20200511-10:23:44] epoch: 0, batch: 5248 train-loss: 1.2964155673980713\n",
      "[LOG 20200511-10:23:45] epoch: 0, batch: 5249 train-loss: 0.906129002571106\n",
      "[LOG 20200511-10:23:45] epoch: 0, batch: 5250 train-loss: 1.8045403957366943\n",
      "[LOG 20200511-10:23:45] epoch: 0, batch: 5251 train-loss: 1.1838774681091309\n",
      "[LOG 20200511-10:23:45] epoch: 0, batch: 5252 train-loss: 0.8006182312965393\n",
      "[LOG 20200511-10:23:45] epoch: 0, batch: 5253 train-loss: 1.65004301071167\n",
      "[LOG 20200511-10:23:45] epoch: 0, batch: 5254 train-loss: 0.6434860229492188\n",
      "[LOG 20200511-10:23:45] epoch: 0, batch: 5255 train-loss: 1.2432548999786377\n",
      "[LOG 20200511-10:23:46] epoch: 0, batch: 5256 train-loss: 2.0251026153564453\n",
      "[LOG 20200511-10:23:46] epoch: 0, batch: 5257 train-loss: 2.6873834133148193\n",
      "[LOG 20200511-10:23:46] epoch: 0, batch: 5258 train-loss: 1.403097152709961\n",
      "[LOG 20200511-10:23:46] epoch: 0, batch: 5259 train-loss: 1.8332277536392212\n",
      "[LOG 20200511-10:23:46] epoch: 0, batch: 5260 train-loss: 1.8474164009094238\n",
      "[LOG 20200511-10:23:46] epoch: 0, batch: 5261 train-loss: 1.682246208190918\n",
      "[LOG 20200511-10:23:46] epoch: 0, batch: 5262 train-loss: 0.9936231970787048\n",
      "[LOG 20200511-10:23:47] epoch: 0, batch: 5263 train-loss: 1.9923611879348755\n",
      "[LOG 20200511-10:23:47] epoch: 0, batch: 5264 train-loss: 1.7796882390975952\n",
      "[LOG 20200511-10:23:47] epoch: 0, batch: 5265 train-loss: 1.233365535736084\n",
      "[LOG 20200511-10:23:47] epoch: 0, batch: 5266 train-loss: 1.0487089157104492\n",
      "[LOG 20200511-10:23:47] epoch: 0, batch: 5267 train-loss: 1.7161835432052612\n",
      "[LOG 20200511-10:23:47] epoch: 0, batch: 5268 train-loss: 3.0724716186523438\n",
      "[LOG 20200511-10:23:47] epoch: 0, batch: 5269 train-loss: 1.4934766292572021\n",
      "[LOG 20200511-10:23:48] epoch: 0, batch: 5270 train-loss: 1.0352212190628052\n",
      "[LOG 20200511-10:23:48] epoch: 0, batch: 5271 train-loss: 1.3583236932754517\n",
      "[LOG 20200511-10:23:48] epoch: 0, batch: 5272 train-loss: 2.0949478149414062\n",
      "[LOG 20200511-10:23:48] epoch: 0, batch: 5273 train-loss: 1.838625192642212\n",
      "[LOG 20200511-10:23:48] epoch: 0, batch: 5274 train-loss: 1.7733774185180664\n",
      "[LOG 20200511-10:23:48] epoch: 0, batch: 5275 train-loss: 1.4246392250061035\n",
      "[LOG 20200511-10:23:48] epoch: 0, batch: 5276 train-loss: 1.5940570831298828\n",
      "[LOG 20200511-10:23:49] epoch: 0, batch: 5277 train-loss: 2.3614449501037598\n",
      "[LOG 20200511-10:23:49] epoch: 0, batch: 5278 train-loss: 1.1729415655136108\n",
      "[LOG 20200511-10:23:49] epoch: 0, batch: 5279 train-loss: 1.9847197532653809\n",
      "[LOG 20200511-10:23:49] epoch: 0, batch: 5280 train-loss: 0.6269831657409668\n",
      "[LOG 20200511-10:23:49] epoch: 0, batch: 5281 train-loss: 0.9629052877426147\n",
      "[LOG 20200511-10:23:49] epoch: 0, batch: 5282 train-loss: 1.5251340866088867\n",
      "[LOG 20200511-10:23:50] epoch: 0, batch: 5283 train-loss: 1.360478162765503\n",
      "[LOG 20200511-10:23:50] epoch: 0, batch: 5284 train-loss: 1.489294171333313\n",
      "[LOG 20200511-10:23:50] epoch: 0, batch: 5285 train-loss: 1.50087571144104\n",
      "[LOG 20200511-10:23:50] epoch: 0, batch: 5286 train-loss: 1.629016399383545\n",
      "[LOG 20200511-10:23:50] epoch: 0, batch: 5287 train-loss: 1.12082040309906\n",
      "[LOG 20200511-10:23:50] epoch: 0, batch: 5288 train-loss: 2.160625696182251\n",
      "[LOG 20200511-10:23:50] epoch: 0, batch: 5289 train-loss: 1.1328963041305542\n",
      "[LOG 20200511-10:23:51] epoch: 0, batch: 5290 train-loss: 2.157771110534668\n",
      "[LOG 20200511-10:23:51] epoch: 0, batch: 5291 train-loss: 1.2311545610427856\n",
      "[LOG 20200511-10:23:51] epoch: 0, batch: 5292 train-loss: 1.9551564455032349\n",
      "[LOG 20200511-10:23:51] epoch: 0, batch: 5293 train-loss: 1.8074183464050293\n",
      "[LOG 20200511-10:23:51] epoch: 0, batch: 5294 train-loss: 0.9766254425048828\n",
      "[LOG 20200511-10:23:51] epoch: 0, batch: 5295 train-loss: 1.3381216526031494\n",
      "[LOG 20200511-10:23:51] epoch: 0, batch: 5296 train-loss: 1.2053476572036743\n",
      "[LOG 20200511-10:23:52] epoch: 0, batch: 5297 train-loss: 1.5508060455322266\n",
      "[LOG 20200511-10:23:52] epoch: 0, batch: 5298 train-loss: 1.0571460723876953\n",
      "[LOG 20200511-10:23:52] epoch: 0, batch: 5299 train-loss: 0.8424170613288879\n",
      "[LOG 20200511-10:23:52] epoch: 0, batch: 5300 train-loss: 0.8912668228149414\n",
      "[LOG 20200511-10:23:52] epoch: 0, batch: 5301 train-loss: 1.4052664041519165\n",
      "[LOG 20200511-10:23:52] epoch: 0, batch: 5302 train-loss: 1.762955665588379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:23:52] epoch: 0, batch: 5303 train-loss: 0.9159317016601562\n",
      "[LOG 20200511-10:23:53] epoch: 0, batch: 5304 train-loss: 0.6355893015861511\n",
      "[LOG 20200511-10:23:53] epoch: 0, batch: 5305 train-loss: 1.4448307752609253\n",
      "[LOG 20200511-10:23:53] epoch: 0, batch: 5306 train-loss: 1.1429153680801392\n",
      "[LOG 20200511-10:23:53] epoch: 0, batch: 5307 train-loss: 0.689610481262207\n",
      "[LOG 20200511-10:23:53] epoch: 0, batch: 5308 train-loss: 1.134529948234558\n",
      "[LOG 20200511-10:23:53] epoch: 0, batch: 5309 train-loss: 0.9560891389846802\n",
      "[LOG 20200511-10:23:53] epoch: 0, batch: 5310 train-loss: 2.153336524963379\n",
      "[LOG 20200511-10:23:54] epoch: 0, batch: 5311 train-loss: 3.4333553314208984\n",
      "[LOG 20200511-10:23:54] epoch: 0, batch: 5312 train-loss: 1.8596216440200806\n",
      "[LOG 20200511-10:23:54] epoch: 0, batch: 5313 train-loss: 2.2765350341796875\n",
      "[LOG 20200511-10:23:54] epoch: 0, batch: 5314 train-loss: 1.3549250364303589\n",
      "[LOG 20200511-10:23:54] epoch: 0, batch: 5315 train-loss: 1.0936086177825928\n",
      "[LOG 20200511-10:23:54] epoch: 0, batch: 5316 train-loss: 1.0298101902008057\n",
      "[LOG 20200511-10:23:54] epoch: 0, batch: 5317 train-loss: 1.768951654434204\n",
      "[LOG 20200511-10:23:55] epoch: 0, batch: 5318 train-loss: 1.447680950164795\n",
      "[LOG 20200511-10:23:55] epoch: 0, batch: 5319 train-loss: 1.0941029787063599\n",
      "[LOG 20200511-10:23:55] epoch: 0, batch: 5320 train-loss: 2.793196201324463\n",
      "[LOG 20200511-10:23:55] epoch: 0, batch: 5321 train-loss: 2.1851046085357666\n",
      "[LOG 20200511-10:23:55] epoch: 0, batch: 5322 train-loss: 1.3588943481445312\n",
      "[LOG 20200511-10:23:55] epoch: 0, batch: 5323 train-loss: 1.0119454860687256\n",
      "[LOG 20200511-10:23:55] epoch: 0, batch: 5324 train-loss: 0.8610097765922546\n",
      "[LOG 20200511-10:23:55] epoch: 0, batch: 5325 train-loss: 1.2486252784729004\n",
      "[LOG 20200511-10:23:56] epoch: 0, batch: 5326 train-loss: 1.1465530395507812\n",
      "[LOG 20200511-10:23:56] epoch: 0, batch: 5327 train-loss: 0.8517309427261353\n",
      "[LOG 20200511-10:23:56] epoch: 0, batch: 5328 train-loss: 0.9115990996360779\n",
      "[LOG 20200511-10:23:56] epoch: 0, batch: 5329 train-loss: 0.7182572484016418\n",
      "[LOG 20200511-10:23:56] epoch: 0, batch: 5330 train-loss: 0.5550066232681274\n",
      "[LOG 20200511-10:23:56] epoch: 0, batch: 5331 train-loss: 1.7223098278045654\n",
      "[LOG 20200511-10:23:56] epoch: 0, batch: 5332 train-loss: 2.3547656536102295\n",
      "[LOG 20200511-10:23:57] epoch: 0, batch: 5333 train-loss: 0.8277417421340942\n",
      "[LOG 20200511-10:23:57] epoch: 0, batch: 5334 train-loss: 0.4243316054344177\n",
      "[LOG 20200511-10:23:57] epoch: 0, batch: 5335 train-loss: 0.8653075695037842\n",
      "[LOG 20200511-10:23:57] epoch: 0, batch: 5336 train-loss: 1.965864896774292\n",
      "[LOG 20200511-10:23:57] epoch: 0, batch: 5337 train-loss: 1.3450531959533691\n",
      "[LOG 20200511-10:23:57] epoch: 0, batch: 5338 train-loss: 1.5050873756408691\n",
      "[LOG 20200511-10:23:57] epoch: 0, batch: 5339 train-loss: 1.0576081275939941\n",
      "[LOG 20200511-10:23:58] epoch: 0, batch: 5340 train-loss: 0.5172251462936401\n",
      "[LOG 20200511-10:23:58] epoch: 0, batch: 5341 train-loss: 1.652266025543213\n",
      "[LOG 20200511-10:23:58] epoch: 0, batch: 5342 train-loss: 1.2354671955108643\n",
      "[LOG 20200511-10:23:58] epoch: 0, batch: 5343 train-loss: 1.009568452835083\n",
      "[LOG 20200511-10:23:58] epoch: 0, batch: 5344 train-loss: 0.92645263671875\n",
      "[LOG 20200511-10:23:58] epoch: 0, batch: 5345 train-loss: 1.7404319047927856\n",
      "[LOG 20200511-10:23:58] epoch: 0, batch: 5346 train-loss: 1.6039681434631348\n",
      "[LOG 20200511-10:23:58] epoch: 0, batch: 5347 train-loss: 1.25222909450531\n",
      "[LOG 20200511-10:23:59] epoch: 0, batch: 5348 train-loss: 1.4703871011734009\n",
      "[LOG 20200511-10:23:59] epoch: 0, batch: 5349 train-loss: 1.024349570274353\n",
      "[LOG 20200511-10:23:59] epoch: 0, batch: 5350 train-loss: 1.6262986660003662\n",
      "[LOG 20200511-10:23:59] epoch: 0, batch: 5351 train-loss: 0.700532853603363\n",
      "[LOG 20200511-10:23:59] epoch: 0, batch: 5352 train-loss: 1.0706557035446167\n",
      "[LOG 20200511-10:23:59] epoch: 0, batch: 5353 train-loss: 1.854675531387329\n",
      "[LOG 20200511-10:23:59] epoch: 0, batch: 5354 train-loss: 1.4760737419128418\n",
      "[LOG 20200511-10:23:59] epoch: 0, batch: 5355 train-loss: 1.082294225692749\n",
      "[LOG 20200511-10:24:00] epoch: 0, batch: 5356 train-loss: 1.3803350925445557\n",
      "[LOG 20200511-10:24:00] epoch: 0, batch: 5357 train-loss: 1.9986238479614258\n",
      "[LOG 20200511-10:24:00] epoch: 0, batch: 5358 train-loss: 0.8785904049873352\n",
      "[LOG 20200511-10:24:00] epoch: 0, batch: 5359 train-loss: 1.812019944190979\n",
      "[LOG 20200511-10:24:00] epoch: 0, batch: 5360 train-loss: 1.267017126083374\n",
      "[LOG 20200511-10:24:00] epoch: 0, batch: 5361 train-loss: 0.8385387063026428\n",
      "[LOG 20200511-10:24:00] epoch: 0, batch: 5362 train-loss: 2.043192148208618\n",
      "[LOG 20200511-10:24:01] epoch: 0, batch: 5363 train-loss: 1.6977345943450928\n",
      "[LOG 20200511-10:24:01] epoch: 0, batch: 5364 train-loss: 1.8831162452697754\n",
      "[LOG 20200511-10:24:01] epoch: 0, batch: 5365 train-loss: 2.659364700317383\n",
      "[LOG 20200511-10:24:01] epoch: 0, batch: 5366 train-loss: 1.2474286556243896\n",
      "[LOG 20200511-10:24:01] epoch: 0, batch: 5367 train-loss: 0.9421313405036926\n",
      "[LOG 20200511-10:24:01] epoch: 0, batch: 5368 train-loss: 1.7398566007614136\n",
      "[LOG 20200511-10:24:01] epoch: 0, batch: 5369 train-loss: 1.1047804355621338\n",
      "[LOG 20200511-10:24:01] epoch: 0, batch: 5370 train-loss: 1.4139115810394287\n",
      "[LOG 20200511-10:24:02] epoch: 0, batch: 5371 train-loss: 0.9750828742980957\n",
      "[LOG 20200511-10:24:02] epoch: 0, batch: 5372 train-loss: 1.5126309394836426\n",
      "[LOG 20200511-10:24:02] epoch: 0, batch: 5373 train-loss: 1.023439884185791\n",
      "[LOG 20200511-10:24:02] epoch: 0, batch: 5374 train-loss: 1.4089640378952026\n",
      "[LOG 20200511-10:24:02] epoch: 0, batch: 5375 train-loss: 1.9735145568847656\n",
      "[LOG 20200511-10:24:02] epoch: 0, batch: 5376 train-loss: 1.0376373529434204\n",
      "[LOG 20200511-10:24:02] epoch: 0, batch: 5377 train-loss: 0.8699877262115479\n",
      "[LOG 20200511-10:24:03] epoch: 0, batch: 5378 train-loss: 1.6147830486297607\n",
      "[LOG 20200511-10:24:03] epoch: 0, batch: 5379 train-loss: 1.975754976272583\n",
      "[LOG 20200511-10:24:03] epoch: 0, batch: 5380 train-loss: 2.0472750663757324\n",
      "[LOG 20200511-10:24:03] epoch: 0, batch: 5381 train-loss: 1.2839601039886475\n",
      "[LOG 20200511-10:24:03] epoch: 0, batch: 5382 train-loss: 1.651942253112793\n",
      "[LOG 20200511-10:24:03] epoch: 0, batch: 5383 train-loss: 1.4485914707183838\n",
      "[LOG 20200511-10:24:03] epoch: 0, batch: 5384 train-loss: 1.32378089427948\n",
      "[LOG 20200511-10:24:03] epoch: 0, batch: 5385 train-loss: 1.8903863430023193\n",
      "[LOG 20200511-10:24:04] epoch: 0, batch: 5386 train-loss: 1.1438244581222534\n",
      "[LOG 20200511-10:24:04] epoch: 0, batch: 5387 train-loss: 1.739060640335083\n",
      "[LOG 20200511-10:24:04] epoch: 0, batch: 5388 train-loss: 0.8224743008613586\n",
      "[LOG 20200511-10:24:04] epoch: 0, batch: 5389 train-loss: 1.9317386150360107\n",
      "[LOG 20200511-10:24:04] epoch: 0, batch: 5390 train-loss: 0.7811762094497681\n",
      "[LOG 20200511-10:24:04] epoch: 0, batch: 5391 train-loss: 0.9615962505340576\n",
      "[LOG 20200511-10:24:04] epoch: 0, batch: 5392 train-loss: 0.8497452735900879\n",
      "[LOG 20200511-10:24:04] epoch: 0, batch: 5393 train-loss: 2.160223960876465\n",
      "[LOG 20200511-10:24:04] epoch: 0, batch: 5394 train-loss: 1.1117768287658691\n",
      "[LOG 20200511-10:24:05] epoch: 0, batch: 5395 train-loss: 2.5507991313934326\n",
      "[LOG 20200511-10:24:05] epoch: 0, batch: 5396 train-loss: 1.6617738008499146\n",
      "[LOG 20200511-10:24:05] epoch: 0, batch: 5397 train-loss: 1.0280251502990723\n",
      "[LOG 20200511-10:24:05] epoch: 0, batch: 5398 train-loss: 1.1124131679534912\n",
      "[LOG 20200511-10:24:05] epoch: 0, batch: 5399 train-loss: 3.238692045211792\n",
      "[LOG 20200511-10:24:05] epoch: 0, batch: 5400 train-loss: 1.5179471969604492\n",
      "[LOG 20200511-10:24:05] epoch: 0, batch: 5401 train-loss: 0.7829347848892212\n",
      "[LOG 20200511-10:24:05] epoch: 0, batch: 5402 train-loss: 1.3739522695541382\n",
      "[LOG 20200511-10:24:05] epoch: 0, batch: 5403 train-loss: 1.2916512489318848\n",
      "[LOG 20200511-10:24:06] epoch: 0, batch: 5404 train-loss: 1.4760860204696655\n",
      "[LOG 20200511-10:24:06] epoch: 0, batch: 5405 train-loss: 1.653303623199463\n",
      "[LOG 20200511-10:24:06] epoch: 0, batch: 5406 train-loss: 0.8932541608810425\n",
      "[LOG 20200511-10:24:06] epoch: 0, batch: 5407 train-loss: 1.362602949142456\n",
      "[LOG 20200511-10:24:06] epoch: 0, batch: 5408 train-loss: 1.247312068939209\n",
      "[LOG 20200511-10:24:06] epoch: 0, batch: 5409 train-loss: 1.1339101791381836\n",
      "[LOG 20200511-10:24:06] epoch: 0, batch: 5410 train-loss: 0.9086564779281616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:24:07] epoch: 0, batch: 5411 train-loss: 1.7236638069152832\n",
      "[LOG 20200511-10:24:07] epoch: 0, batch: 5412 train-loss: 1.8850221633911133\n",
      "[LOG 20200511-10:24:07] epoch: 0, batch: 5413 train-loss: 1.5798901319503784\n",
      "[LOG 20200511-10:24:07] epoch: 0, batch: 5414 train-loss: 1.3276340961456299\n",
      "[LOG 20200511-10:24:07] epoch: 0, batch: 5415 train-loss: 0.7798081040382385\n",
      "[LOG 20200511-10:24:07] epoch: 0, batch: 5416 train-loss: 1.938421607017517\n",
      "[LOG 20200511-10:24:07] epoch: 0, batch: 5417 train-loss: 0.5281766057014465\n",
      "[LOG 20200511-10:24:07] epoch: 0, batch: 5418 train-loss: 0.6602311730384827\n",
      "[LOG 20200511-10:24:08] epoch: 0, batch: 5419 train-loss: 0.8297773599624634\n",
      "[LOG 20200511-10:24:08] epoch: 0, batch: 5420 train-loss: 1.0760780572891235\n",
      "[LOG 20200511-10:24:08] epoch: 0, batch: 5421 train-loss: 1.5540987253189087\n",
      "[LOG 20200511-10:24:08] epoch: 0, batch: 5422 train-loss: 1.37563157081604\n",
      "[LOG 20200511-10:24:08] epoch: 0, batch: 5423 train-loss: 1.7705743312835693\n",
      "[LOG 20200511-10:24:08] epoch: 0, batch: 5424 train-loss: 0.8514248132705688\n",
      "[LOG 20200511-10:24:08] epoch: 0, batch: 5425 train-loss: 1.9255552291870117\n",
      "[LOG 20200511-10:24:08] epoch: 0, batch: 5426 train-loss: 1.243742823600769\n",
      "[LOG 20200511-10:24:09] epoch: 0, batch: 5427 train-loss: 1.7316335439682007\n",
      "[LOG 20200511-10:24:09] epoch: 0, batch: 5428 train-loss: 3.0174524784088135\n",
      "[LOG 20200511-10:24:09] epoch: 0, batch: 5429 train-loss: 1.6284571886062622\n",
      "[LOG 20200511-10:24:09] epoch: 0, batch: 5430 train-loss: 1.5144877433776855\n",
      "[LOG 20200511-10:24:09] epoch: 0, batch: 5431 train-loss: 1.3323121070861816\n",
      "[LOG 20200511-10:24:09] epoch: 0, batch: 5432 train-loss: 1.9110838174819946\n",
      "[LOG 20200511-10:24:09] epoch: 0, batch: 5433 train-loss: 1.2293946743011475\n",
      "[LOG 20200511-10:24:09] epoch: 0, batch: 5434 train-loss: 1.5204761028289795\n",
      "[LOG 20200511-10:24:10] epoch: 0, batch: 5435 train-loss: 1.6304081678390503\n",
      "[LOG 20200511-10:24:10] epoch: 0, batch: 5436 train-loss: 0.5369411110877991\n",
      "[LOG 20200511-10:24:10] epoch: 0, batch: 5437 train-loss: 1.1772289276123047\n",
      "[LOG 20200511-10:24:10] epoch: 0, batch: 5438 train-loss: 1.2653809785842896\n",
      "[LOG 20200511-10:24:10] epoch: 0, batch: 5439 train-loss: 1.1378051042556763\n",
      "[LOG 20200511-10:24:10] epoch: 0, batch: 5440 train-loss: 0.6424425840377808\n",
      "[LOG 20200511-10:24:10] epoch: 0, batch: 5441 train-loss: 1.52675461769104\n",
      "[LOG 20200511-10:24:10] epoch: 0, batch: 5442 train-loss: 1.622056007385254\n",
      "[LOG 20200511-10:24:11] epoch: 0, batch: 5443 train-loss: 1.154413104057312\n",
      "[LOG 20200511-10:24:11] epoch: 0, batch: 5444 train-loss: 1.8092374801635742\n",
      "[LOG 20200511-10:24:11] epoch: 0, batch: 5445 train-loss: 1.0300543308258057\n",
      "[LOG 20200511-10:24:11] epoch: 0, batch: 5446 train-loss: 0.7301244139671326\n",
      "[LOG 20200511-10:24:11] epoch: 0, batch: 5447 train-loss: 1.386065125465393\n",
      "[LOG 20200511-10:24:11] epoch: 0, batch: 5448 train-loss: 0.9556414484977722\n",
      "[LOG 20200511-10:24:11] epoch: 0, batch: 5449 train-loss: 1.1742557287216187\n",
      "[LOG 20200511-10:24:12] epoch: 0, batch: 5450 train-loss: 1.0529005527496338\n",
      "[LOG 20200511-10:24:12] epoch: 0, batch: 5451 train-loss: 0.9105874300003052\n",
      "[LOG 20200511-10:24:12] epoch: 0, batch: 5452 train-loss: 1.044690489768982\n",
      "[LOG 20200511-10:24:12] epoch: 0, batch: 5453 train-loss: 0.6580902338027954\n",
      "[LOG 20200511-10:24:12] epoch: 0, batch: 5454 train-loss: 1.6201649904251099\n",
      "[LOG 20200511-10:24:12] epoch: 0, batch: 5455 train-loss: 0.6243770718574524\n",
      "[LOG 20200511-10:24:12] epoch: 0, batch: 5456 train-loss: 0.3005499243736267\n",
      "[LOG 20200511-10:24:13] epoch: 0, batch: 5457 train-loss: 2.3048884868621826\n",
      "[LOG 20200511-10:24:13] epoch: 0, batch: 5458 train-loss: 1.136592984199524\n",
      "[LOG 20200511-10:24:13] epoch: 0, batch: 5459 train-loss: 1.0243340730667114\n",
      "[LOG 20200511-10:24:13] epoch: 0, batch: 5460 train-loss: 1.449857473373413\n",
      "[LOG 20200511-10:24:13] epoch: 0, batch: 5461 train-loss: 1.0341832637786865\n",
      "[LOG 20200511-10:24:13] epoch: 0, batch: 5462 train-loss: 1.9533952474594116\n",
      "[LOG 20200511-10:24:13] epoch: 0, batch: 5463 train-loss: 1.6771254539489746\n",
      "[LOG 20200511-10:24:13] epoch: 0, batch: 5464 train-loss: 0.741163969039917\n",
      "[LOG 20200511-10:24:14] epoch: 0, batch: 5465 train-loss: 1.4102250337600708\n",
      "[LOG 20200511-10:24:14] epoch: 0, batch: 5466 train-loss: 1.0204108953475952\n",
      "[LOG 20200511-10:24:14] epoch: 0, batch: 5467 train-loss: 1.0642821788787842\n",
      "[LOG 20200511-10:24:14] epoch: 0, batch: 5468 train-loss: 0.4631307125091553\n",
      "[LOG 20200511-10:24:14] epoch: 0, batch: 5469 train-loss: 1.0895918607711792\n",
      "[LOG 20200511-10:24:14] epoch: 0, batch: 5470 train-loss: 0.9437465071678162\n",
      "[LOG 20200511-10:24:14] epoch: 0, batch: 5471 train-loss: 0.9449858069419861\n",
      "[LOG 20200511-10:24:14] epoch: 0, batch: 5472 train-loss: 1.0842962265014648\n",
      "[LOG 20200511-10:24:15] epoch: 0, batch: 5473 train-loss: 0.5929352045059204\n",
      "[LOG 20200511-10:24:15] epoch: 0, batch: 5474 train-loss: 2.4973175525665283\n",
      "[LOG 20200511-10:24:15] epoch: 0, batch: 5475 train-loss: 1.9136766195297241\n",
      "[LOG 20200511-10:24:15] epoch: 0, batch: 5476 train-loss: 1.3028061389923096\n",
      "[LOG 20200511-10:24:15] epoch: 0, batch: 5477 train-loss: 0.7777343392372131\n",
      "[LOG 20200511-10:24:15] epoch: 0, batch: 5478 train-loss: 1.5945501327514648\n",
      "[LOG 20200511-10:24:15] epoch: 0, batch: 5479 train-loss: 1.6556249856948853\n",
      "[LOG 20200511-10:24:15] epoch: 0, batch: 5480 train-loss: 1.4486082792282104\n",
      "[LOG 20200511-10:24:15] epoch: 0, batch: 5481 train-loss: 0.46203839778900146\n",
      "[LOG 20200511-10:24:16] epoch: 0, batch: 5482 train-loss: 1.050701379776001\n",
      "[LOG 20200511-10:24:16] epoch: 0, batch: 5483 train-loss: 1.7242004871368408\n",
      "[LOG 20200511-10:24:16] epoch: 0, batch: 5484 train-loss: 0.9929879307746887\n",
      "[LOG 20200511-10:24:16] epoch: 0, batch: 5485 train-loss: 1.075465202331543\n",
      "[LOG 20200511-10:24:16] epoch: 0, batch: 5486 train-loss: 1.2277030944824219\n",
      "[LOG 20200511-10:24:16] epoch: 0, batch: 5487 train-loss: 0.8639823794364929\n",
      "[LOG 20200511-10:24:16] epoch: 0, batch: 5488 train-loss: 0.5904419422149658\n",
      "[LOG 20200511-10:24:16] epoch: 0, batch: 5489 train-loss: 1.0896742343902588\n",
      "[LOG 20200511-10:24:16] epoch: 0, batch: 5490 train-loss: 2.03659725189209\n",
      "[LOG 20200511-10:24:17] epoch: 0, batch: 5491 train-loss: 0.6006147861480713\n",
      "[LOG 20200511-10:24:17] epoch: 0, batch: 5492 train-loss: 1.257582426071167\n",
      "[LOG 20200511-10:24:17] epoch: 0, batch: 5493 train-loss: 0.8914263844490051\n",
      "[LOG 20200511-10:24:17] epoch: 0, batch: 5494 train-loss: 0.4966129958629608\n",
      "[LOG 20200511-10:24:17] epoch: 0, batch: 5495 train-loss: 0.9128657579421997\n",
      "[LOG 20200511-10:24:17] epoch: 0, batch: 5496 train-loss: 1.5781745910644531\n",
      "[LOG 20200511-10:24:17] epoch: 0, batch: 5497 train-loss: 1.898081660270691\n",
      "[LOG 20200511-10:24:17] epoch: 0, batch: 5498 train-loss: 1.1367971897125244\n",
      "[LOG 20200511-10:24:18] epoch: 0, batch: 5499 train-loss: 0.4665319323539734\n",
      "[LOG 20200511-10:24:18] epoch: 0, batch: 5500 train-loss: 1.3070613145828247\n",
      "[LOG 20200511-10:24:18] epoch: 0, batch: 5501 train-loss: 1.6376757621765137\n",
      "[LOG 20200511-10:24:18] epoch: 0, batch: 5502 train-loss: 1.696150302886963\n",
      "[LOG 20200511-10:24:18] epoch: 0, batch: 5503 train-loss: 0.7214789390563965\n",
      "[LOG 20200511-10:24:18] epoch: 0, batch: 5504 train-loss: 1.6569724082946777\n",
      "[LOG 20200511-10:24:18] epoch: 0, batch: 5505 train-loss: 2.2603373527526855\n",
      "[LOG 20200511-10:24:18] epoch: 0, batch: 5506 train-loss: 3.2103500366210938\n",
      "[LOG 20200511-10:24:19] epoch: 0, batch: 5507 train-loss: 1.890259027481079\n",
      "[LOG 20200511-10:24:19] epoch: 0, batch: 5508 train-loss: 1.4220178127288818\n",
      "[LOG 20200511-10:24:19] epoch: 0, batch: 5509 train-loss: 0.9544610381126404\n",
      "[LOG 20200511-10:24:19] epoch: 0, batch: 5510 train-loss: 1.0933032035827637\n",
      "[LOG 20200511-10:24:19] epoch: 0, batch: 5511 train-loss: 1.012937068939209\n",
      "[LOG 20200511-10:24:19] epoch: 0, batch: 5512 train-loss: 1.7582746744155884\n",
      "[LOG 20200511-10:24:19] epoch: 0, batch: 5513 train-loss: 1.6512818336486816\n",
      "[LOG 20200511-10:24:20] epoch: 0, batch: 5514 train-loss: 2.5483505725860596\n",
      "[LOG 20200511-10:24:20] epoch: 0, batch: 5515 train-loss: 0.7835116386413574\n",
      "[LOG 20200511-10:24:20] epoch: 0, batch: 5516 train-loss: 2.538153648376465\n",
      "[LOG 20200511-10:24:20] epoch: 0, batch: 5517 train-loss: 1.6443601846694946\n",
      "[LOG 20200511-10:24:20] epoch: 0, batch: 5518 train-loss: 1.393078088760376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:24:20] epoch: 0, batch: 5519 train-loss: 1.867243766784668\n",
      "[LOG 20200511-10:24:20] epoch: 0, batch: 5520 train-loss: 1.0061328411102295\n",
      "[LOG 20200511-10:24:21] epoch: 0, batch: 5521 train-loss: 1.0439527034759521\n",
      "[LOG 20200511-10:24:21] epoch: 0, batch: 5522 train-loss: 1.4803400039672852\n",
      "[LOG 20200511-10:24:21] epoch: 0, batch: 5523 train-loss: 2.0841846466064453\n",
      "[LOG 20200511-10:24:21] epoch: 0, batch: 5524 train-loss: 1.5211637020111084\n",
      "[LOG 20200511-10:24:21] epoch: 0, batch: 5525 train-loss: 0.8893587589263916\n",
      "[LOG 20200511-10:24:21] epoch: 0, batch: 5526 train-loss: 0.7053030729293823\n",
      "[LOG 20200511-10:24:21] epoch: 0, batch: 5527 train-loss: 1.160752773284912\n",
      "[LOG 20200511-10:24:22] epoch: 0, batch: 5528 train-loss: 1.7635047435760498\n",
      "[LOG 20200511-10:24:22] epoch: 0, batch: 5529 train-loss: 0.9853227734565735\n",
      "[LOG 20200511-10:24:22] epoch: 0, batch: 5530 train-loss: 0.8242956399917603\n",
      "[LOG 20200511-10:24:22] epoch: 0, batch: 5531 train-loss: 0.9767947196960449\n",
      "[LOG 20200511-10:24:22] epoch: 0, batch: 5532 train-loss: 0.8091822862625122\n",
      "[LOG 20200511-10:24:22] epoch: 0, batch: 5533 train-loss: 0.7058345079421997\n",
      "[LOG 20200511-10:24:22] epoch: 0, batch: 5534 train-loss: 0.617114782333374\n",
      "[LOG 20200511-10:24:22] epoch: 0, batch: 5535 train-loss: 1.9727880954742432\n",
      "[LOG 20200511-10:24:23] epoch: 0, batch: 5536 train-loss: 1.1006885766983032\n",
      "[LOG 20200511-10:24:23] epoch: 0, batch: 5537 train-loss: 2.124044895172119\n",
      "[LOG 20200511-10:24:23] epoch: 0, batch: 5538 train-loss: 0.6525277495384216\n",
      "[LOG 20200511-10:24:23] epoch: 0, batch: 5539 train-loss: 1.5341289043426514\n",
      "[LOG 20200511-10:24:23] epoch: 0, batch: 5540 train-loss: 1.301183819770813\n",
      "[LOG 20200511-10:24:23] epoch: 0, batch: 5541 train-loss: 0.8417889475822449\n",
      "[LOG 20200511-10:24:23] epoch: 0, batch: 5542 train-loss: 1.831854224205017\n",
      "[LOG 20200511-10:24:24] epoch: 0, batch: 5543 train-loss: 1.384278416633606\n",
      "[LOG 20200511-10:24:24] epoch: 0, batch: 5544 train-loss: 0.695326030254364\n",
      "[LOG 20200511-10:24:24] epoch: 0, batch: 5545 train-loss: 1.4256709814071655\n",
      "[LOG 20200511-10:24:24] epoch: 0, batch: 5546 train-loss: 0.8807514905929565\n",
      "[LOG 20200511-10:24:24] epoch: 0, batch: 5547 train-loss: 1.4449349641799927\n",
      "[LOG 20200511-10:24:24] epoch: 0, batch: 5548 train-loss: 0.82961106300354\n",
      "[LOG 20200511-10:24:24] epoch: 0, batch: 5549 train-loss: 1.03130304813385\n",
      "[LOG 20200511-10:24:24] epoch: 0, batch: 5550 train-loss: 1.8210958242416382\n",
      "[LOG 20200511-10:24:25] epoch: 0, batch: 5551 train-loss: 1.0891809463500977\n",
      "[LOG 20200511-10:24:25] epoch: 0, batch: 5552 train-loss: 1.8082265853881836\n",
      "[LOG 20200511-10:24:25] epoch: 0, batch: 5553 train-loss: 0.9050407409667969\n",
      "[LOG 20200511-10:24:25] epoch: 0, batch: 5554 train-loss: 1.0426709651947021\n",
      "[LOG 20200511-10:24:25] epoch: 0, batch: 5555 train-loss: 1.3364908695220947\n",
      "[LOG 20200511-10:24:25] epoch: 0, batch: 5556 train-loss: 0.9915379285812378\n",
      "[LOG 20200511-10:24:25] epoch: 0, batch: 5557 train-loss: 0.8666013479232788\n",
      "[LOG 20200511-10:24:25] epoch: 0, batch: 5558 train-loss: 1.9293280839920044\n",
      "[LOG 20200511-10:24:25] epoch: 0, batch: 5559 train-loss: 1.6362332105636597\n",
      "[LOG 20200511-10:24:26] epoch: 0, batch: 5560 train-loss: 2.5140135288238525\n",
      "[LOG 20200511-10:24:26] epoch: 0, batch: 5561 train-loss: 1.8373992443084717\n",
      "[LOG 20200511-10:24:26] epoch: 0, batch: 5562 train-loss: 1.0326886177062988\n",
      "[LOG 20200511-10:24:26] epoch: 0, batch: 5563 train-loss: 1.4824845790863037\n",
      "[LOG 20200511-10:24:26] epoch: 0, batch: 5564 train-loss: 1.075960636138916\n",
      "[LOG 20200511-10:24:26] epoch: 0, batch: 5565 train-loss: 1.2661070823669434\n",
      "[LOG 20200511-10:24:26] epoch: 0, batch: 5566 train-loss: 1.9543918371200562\n",
      "[LOG 20200511-10:24:26] epoch: 0, batch: 5567 train-loss: 1.1218335628509521\n",
      "[LOG 20200511-10:24:26] epoch: 0, batch: 5568 train-loss: 1.1380834579467773\n",
      "[LOG 20200511-10:24:27] epoch: 0, batch: 5569 train-loss: 1.3768213987350464\n",
      "[LOG 20200511-10:24:27] epoch: 0, batch: 5570 train-loss: 2.016986608505249\n",
      "[LOG 20200511-10:24:27] epoch: 0, batch: 5571 train-loss: 1.6067006587982178\n",
      "[LOG 20200511-10:24:27] epoch: 0, batch: 5572 train-loss: 0.8540061712265015\n",
      "[LOG 20200511-10:24:27] epoch: 0, batch: 5573 train-loss: 1.0850419998168945\n",
      "[LOG 20200511-10:24:27] epoch: 0, batch: 5574 train-loss: 0.6572990417480469\n",
      "[LOG 20200511-10:24:27] epoch: 0, batch: 5575 train-loss: 1.6801773309707642\n",
      "[LOG 20200511-10:24:27] epoch: 0, batch: 5576 train-loss: 0.976561963558197\n",
      "[LOG 20200511-10:24:27] epoch: 0, batch: 5577 train-loss: 0.5422186851501465\n",
      "[LOG 20200511-10:24:28] epoch: 0, batch: 5578 train-loss: 1.7734503746032715\n",
      "[LOG 20200511-10:24:28] epoch: 0, batch: 5579 train-loss: 1.431358814239502\n",
      "[LOG 20200511-10:24:28] epoch: 0, batch: 5580 train-loss: 1.3040127754211426\n",
      "[LOG 20200511-10:24:28] epoch: 0, batch: 5581 train-loss: 1.510063648223877\n",
      "[LOG 20200511-10:24:28] epoch: 0, batch: 5582 train-loss: 0.9854514598846436\n",
      "[LOG 20200511-10:24:28] epoch: 0, batch: 5583 train-loss: 1.9731553792953491\n",
      "[LOG 20200511-10:24:28] epoch: 0, batch: 5584 train-loss: 1.7102628946304321\n",
      "[LOG 20200511-10:24:28] epoch: 0, batch: 5585 train-loss: 1.8446599245071411\n",
      "[LOG 20200511-10:24:28] epoch: 0, batch: 5586 train-loss: 1.339039921760559\n",
      "[LOG 20200511-10:24:29] epoch: 0, batch: 5587 train-loss: 1.6192641258239746\n",
      "[LOG 20200511-10:24:29] epoch: 0, batch: 5588 train-loss: 2.2009029388427734\n",
      "[LOG 20200511-10:24:29] epoch: 0, batch: 5589 train-loss: 1.260317087173462\n",
      "[LOG 20200511-10:24:29] epoch: 0, batch: 5590 train-loss: 2.023515462875366\n",
      "[LOG 20200511-10:24:29] epoch: 0, batch: 5591 train-loss: 2.9961304664611816\n",
      "[LOG 20200511-10:24:29] epoch: 0, batch: 5592 train-loss: 1.2530651092529297\n",
      "[LOG 20200511-10:24:29] epoch: 0, batch: 5593 train-loss: 1.7549610137939453\n",
      "[LOG 20200511-10:24:29] epoch: 0, batch: 5594 train-loss: 1.796569585800171\n",
      "[LOG 20200511-10:24:30] epoch: 0, batch: 5595 train-loss: 1.3490684032440186\n",
      "[LOG 20200511-10:24:30] epoch: 0, batch: 5596 train-loss: 3.335888147354126\n",
      "[LOG 20200511-10:24:30] epoch: 0, batch: 5597 train-loss: 1.1734176874160767\n",
      "[LOG 20200511-10:24:30] epoch: 0, batch: 5598 train-loss: 1.9457703828811646\n",
      "[LOG 20200511-10:24:30] epoch: 0, batch: 5599 train-loss: 1.6424695253372192\n",
      "[LOG 20200511-10:24:30] epoch: 0, batch: 5600 train-loss: 1.2038567066192627\n",
      "[LOG 20200511-10:24:30] epoch: 0, batch: 5601 train-loss: 1.8068368434906006\n",
      "[LOG 20200511-10:24:30] epoch: 0, batch: 5602 train-loss: 1.5454590320587158\n",
      "[LOG 20200511-10:24:30] epoch: 0, batch: 5603 train-loss: 0.9441170692443848\n",
      "[LOG 20200511-10:24:31] epoch: 0, batch: 5604 train-loss: 0.5182374119758606\n",
      "[LOG 20200511-10:24:31] epoch: 0, batch: 5605 train-loss: 1.2817550897598267\n",
      "[LOG 20200511-10:24:31] epoch: 0, batch: 5606 train-loss: 1.4229326248168945\n",
      "[LOG 20200511-10:24:31] epoch: 0, batch: 5607 train-loss: 1.5992543697357178\n",
      "[LOG 20200511-10:24:31] epoch: 0, batch: 5608 train-loss: 1.444582462310791\n",
      "[LOG 20200511-10:24:31] epoch: 0, batch: 5609 train-loss: 1.5174850225448608\n",
      "[LOG 20200511-10:24:31] epoch: 0, batch: 5610 train-loss: 0.8445212244987488\n",
      "[LOG 20200511-10:24:31] epoch: 0, batch: 5611 train-loss: 0.9182587265968323\n",
      "[LOG 20200511-10:24:31] epoch: 0, batch: 5612 train-loss: 1.317744493484497\n",
      "[LOG 20200511-10:24:32] epoch: 0, batch: 5613 train-loss: 1.413078784942627\n",
      "[LOG 20200511-10:24:32] epoch: 0, batch: 5614 train-loss: 0.5456690192222595\n",
      "[LOG 20200511-10:24:32] epoch: 0, batch: 5615 train-loss: 1.560368537902832\n",
      "[LOG 20200511-10:24:32] epoch: 0, batch: 5616 train-loss: 1.2200936079025269\n",
      "[LOG 20200511-10:24:32] epoch: 0, batch: 5617 train-loss: 1.0745872259140015\n",
      "[LOG 20200511-10:24:32] epoch: 0, batch: 5618 train-loss: 0.8219393491744995\n",
      "[LOG 20200511-10:24:32] epoch: 0, batch: 5619 train-loss: 1.7834678888320923\n",
      "[LOG 20200511-10:24:32] epoch: 0, batch: 5620 train-loss: 0.9540403485298157\n",
      "[LOG 20200511-10:24:32] epoch: 0, batch: 5621 train-loss: 0.9743123650550842\n",
      "[LOG 20200511-10:24:33] epoch: 0, batch: 5622 train-loss: 0.6296948194503784\n",
      "[LOG 20200511-10:24:33] epoch: 0, batch: 5623 train-loss: 1.4050475358963013\n",
      "[LOG 20200511-10:24:33] epoch: 0, batch: 5624 train-loss: 2.299443006515503\n",
      "[LOG 20200511-10:24:33] epoch: 0, batch: 5625 train-loss: 1.8238950967788696\n",
      "[LOG 20200511-10:24:33] epoch: 0, batch: 5626 train-loss: 1.0771516561508179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:24:33] epoch: 0, batch: 5627 train-loss: 2.1663999557495117\n",
      "[LOG 20200511-10:24:33] epoch: 0, batch: 5628 train-loss: 0.886701762676239\n",
      "[LOG 20200511-10:24:33] epoch: 0, batch: 5629 train-loss: 0.9750304222106934\n",
      "[LOG 20200511-10:24:33] epoch: 0, batch: 5630 train-loss: 1.3147337436676025\n",
      "[LOG 20200511-10:24:34] epoch: 0, batch: 5631 train-loss: 1.442488670349121\n",
      "[LOG 20200511-10:24:34] epoch: 0, batch: 5632 train-loss: 2.164250373840332\n",
      "[LOG 20200511-10:24:34] epoch: 0, batch: 5633 train-loss: 1.5779528617858887\n",
      "[LOG 20200511-10:24:34] epoch: 0, batch: 5634 train-loss: 1.8180347681045532\n",
      "[LOG 20200511-10:24:34] epoch: 0, batch: 5635 train-loss: 2.381873369216919\n",
      "[LOG 20200511-10:24:34] epoch: 0, batch: 5636 train-loss: 0.8742496967315674\n",
      "[LOG 20200511-10:24:34] epoch: 0, batch: 5637 train-loss: 1.53617525100708\n",
      "[LOG 20200511-10:24:34] epoch: 0, batch: 5638 train-loss: 0.5672899484634399\n",
      "[LOG 20200511-10:24:34] epoch: 0, batch: 5639 train-loss: 0.7889803051948547\n",
      "[LOG 20200511-10:24:35] epoch: 0, batch: 5640 train-loss: 0.9131731986999512\n",
      "[LOG 20200511-10:24:35] epoch: 0, batch: 5641 train-loss: 2.1121695041656494\n",
      "[LOG 20200511-10:24:35] epoch: 0, batch: 5642 train-loss: 1.1039637327194214\n",
      "[LOG 20200511-10:24:35] epoch: 0, batch: 5643 train-loss: 1.1384810209274292\n",
      "[LOG 20200511-10:24:35] epoch: 0, batch: 5644 train-loss: 1.6439603567123413\n",
      "[LOG 20200511-10:24:35] epoch: 0, batch: 5645 train-loss: 1.7345960140228271\n",
      "[LOG 20200511-10:24:35] epoch: 0, batch: 5646 train-loss: 1.4117703437805176\n",
      "[LOG 20200511-10:24:35] epoch: 0, batch: 5647 train-loss: 0.7733809947967529\n",
      "[LOG 20200511-10:24:36] epoch: 0, batch: 5648 train-loss: 2.4203431606292725\n",
      "[LOG 20200511-10:24:36] epoch: 0, batch: 5649 train-loss: 1.3258047103881836\n",
      "[LOG 20200511-10:24:36] epoch: 0, batch: 5650 train-loss: 2.29457688331604\n",
      "[LOG 20200511-10:24:36] epoch: 0, batch: 5651 train-loss: 1.01149582862854\n",
      "[LOG 20200511-10:24:36] epoch: 0, batch: 5652 train-loss: 1.5366836786270142\n",
      "[LOG 20200511-10:24:36] epoch: 0, batch: 5653 train-loss: 2.0582761764526367\n",
      "[LOG 20200511-10:24:36] epoch: 0, batch: 5654 train-loss: 1.182507872581482\n",
      "[LOG 20200511-10:24:36] epoch: 0, batch: 5655 train-loss: 1.8705395460128784\n",
      "[LOG 20200511-10:24:37] epoch: 0, batch: 5656 train-loss: 1.3084765672683716\n",
      "[LOG 20200511-10:24:37] epoch: 0, batch: 5657 train-loss: 1.7421472072601318\n",
      "[LOG 20200511-10:24:37] epoch: 0, batch: 5658 train-loss: 1.0514565706253052\n",
      "[LOG 20200511-10:24:37] epoch: 0, batch: 5659 train-loss: 1.705514669418335\n",
      "[LOG 20200511-10:24:37] epoch: 0, batch: 5660 train-loss: 1.3990989923477173\n",
      "[LOG 20200511-10:24:37] epoch: 0, batch: 5661 train-loss: 1.1953847408294678\n",
      "[LOG 20200511-10:24:37] epoch: 0, batch: 5662 train-loss: 1.766521692276001\n",
      "[LOG 20200511-10:24:37] epoch: 0, batch: 5663 train-loss: 1.9792935848236084\n",
      "[LOG 20200511-10:24:37] epoch: 0, batch: 5664 train-loss: 2.247818946838379\n",
      "[LOG 20200511-10:24:38] epoch: 0, batch: 5665 train-loss: 1.0506761074066162\n",
      "[LOG 20200511-10:24:38] epoch: 0, batch: 5666 train-loss: 1.410231351852417\n",
      "[LOG 20200511-10:24:38] epoch: 0, batch: 5667 train-loss: 1.976294755935669\n",
      "[LOG 20200511-10:24:38] epoch: 0, batch: 5668 train-loss: 1.5872100591659546\n",
      "[LOG 20200511-10:24:38] epoch: 0, batch: 5669 train-loss: 1.338935375213623\n",
      "[LOG 20200511-10:24:38] epoch: 0, batch: 5670 train-loss: 0.4178401231765747\n",
      "[LOG 20200511-10:24:38] epoch: 0, batch: 5671 train-loss: 1.2091130018234253\n",
      "[LOG 20200511-10:24:38] epoch: 0, batch: 5672 train-loss: 1.1690375804901123\n",
      "[LOG 20200511-10:24:38] epoch: 0, batch: 5673 train-loss: 1.59469735622406\n",
      "[LOG 20200511-10:24:39] epoch: 0, batch: 5674 train-loss: 1.4532307386398315\n",
      "[LOG 20200511-10:24:39] epoch: 0, batch: 5675 train-loss: 0.7873536944389343\n",
      "[LOG 20200511-10:24:39] epoch: 0, batch: 5676 train-loss: 2.015244483947754\n",
      "[LOG 20200511-10:24:39] epoch: 0, batch: 5677 train-loss: 1.1840784549713135\n",
      "[LOG 20200511-10:24:39] epoch: 0, batch: 5678 train-loss: 1.3162513971328735\n",
      "[LOG 20200511-10:24:39] epoch: 0, batch: 5679 train-loss: 1.6438387632369995\n",
      "[LOG 20200511-10:24:39] epoch: 0, batch: 5680 train-loss: 1.0450297594070435\n",
      "[LOG 20200511-10:24:40] epoch: 0, batch: 5681 train-loss: 1.0050050020217896\n",
      "[LOG 20200511-10:24:40] epoch: 0, batch: 5682 train-loss: 1.0958095788955688\n",
      "[LOG 20200511-10:24:40] epoch: 0, batch: 5683 train-loss: 1.2827423810958862\n",
      "[LOG 20200511-10:24:40] epoch: 0, batch: 5684 train-loss: 1.1022849082946777\n",
      "[LOG 20200511-10:24:40] epoch: 0, batch: 5685 train-loss: 0.3718584179878235\n",
      "[LOG 20200511-10:24:40] epoch: 0, batch: 5686 train-loss: 1.8960402011871338\n",
      "[LOG 20200511-10:24:40] epoch: 0, batch: 5687 train-loss: 1.517477035522461\n",
      "[LOG 20200511-10:24:40] epoch: 0, batch: 5688 train-loss: 0.908820390701294\n",
      "[LOG 20200511-10:24:40] epoch: 0, batch: 5689 train-loss: 2.3349218368530273\n",
      "[LOG 20200511-10:24:41] epoch: 0, batch: 5690 train-loss: 1.0873106718063354\n",
      "[LOG 20200511-10:24:41] epoch: 0, batch: 5691 train-loss: 0.5127567052841187\n",
      "[LOG 20200511-10:24:41] epoch: 0, batch: 5692 train-loss: 0.9014499187469482\n",
      "[LOG 20200511-10:24:41] epoch: 0, batch: 5693 train-loss: 2.3655641078948975\n",
      "[LOG 20200511-10:24:41] epoch: 0, batch: 5694 train-loss: 1.130143165588379\n",
      "[LOG 20200511-10:24:41] epoch: 0, batch: 5695 train-loss: 1.2154115438461304\n",
      "[LOG 20200511-10:24:41] epoch: 0, batch: 5696 train-loss: 1.721945881843567\n",
      "[LOG 20200511-10:24:41] epoch: 0, batch: 5697 train-loss: 0.8321110606193542\n",
      "[LOG 20200511-10:24:41] epoch: 0, batch: 5698 train-loss: 1.2552155256271362\n",
      "[LOG 20200511-10:24:42] epoch: 0, batch: 5699 train-loss: 2.0449836254119873\n",
      "[LOG 20200511-10:24:42] epoch: 0, batch: 5700 train-loss: 2.0494987964630127\n",
      "[LOG 20200511-10:24:42] epoch: 0, batch: 5701 train-loss: 1.5618033409118652\n",
      "[LOG 20200511-10:24:42] epoch: 0, batch: 5702 train-loss: 1.5498042106628418\n",
      "[LOG 20200511-10:24:42] epoch: 0, batch: 5703 train-loss: 0.6981516480445862\n",
      "[LOG 20200511-10:24:42] epoch: 0, batch: 5704 train-loss: 1.027945876121521\n",
      "[LOG 20200511-10:24:42] epoch: 0, batch: 5705 train-loss: 1.1166908740997314\n",
      "[LOG 20200511-10:24:42] epoch: 0, batch: 5706 train-loss: 1.0658904314041138\n",
      "[LOG 20200511-10:24:43] epoch: 0, batch: 5707 train-loss: 0.8486734628677368\n",
      "[LOG 20200511-10:24:43] epoch: 0, batch: 5708 train-loss: 0.7826252579689026\n",
      "[LOG 20200511-10:24:43] epoch: 0, batch: 5709 train-loss: 0.2835248112678528\n",
      "[LOG 20200511-10:24:43] epoch: 0, batch: 5710 train-loss: 1.204666256904602\n",
      "[LOG 20200511-10:24:43] epoch: 0, batch: 5711 train-loss: 0.5584620833396912\n",
      "[LOG 20200511-10:24:43] epoch: 0, batch: 5712 train-loss: 0.9213846921920776\n",
      "[LOG 20200511-10:24:43] epoch: 0, batch: 5713 train-loss: 0.9752353429794312\n",
      "[LOG 20200511-10:24:43] epoch: 0, batch: 5714 train-loss: 2.2260255813598633\n",
      "[LOG 20200511-10:24:43] epoch: 0, batch: 5715 train-loss: 2.499055862426758\n",
      "[LOG 20200511-10:24:44] epoch: 0, batch: 5716 train-loss: 1.3438363075256348\n",
      "[LOG 20200511-10:24:44] epoch: 0, batch: 5717 train-loss: 1.0391674041748047\n",
      "[LOG 20200511-10:24:44] epoch: 0, batch: 5718 train-loss: 2.2861523628234863\n",
      "[LOG 20200511-10:24:44] epoch: 0, batch: 5719 train-loss: 1.017857313156128\n",
      "[LOG 20200511-10:24:44] epoch: 0, batch: 5720 train-loss: 2.3435120582580566\n",
      "[LOG 20200511-10:24:44] epoch: 0, batch: 5721 train-loss: 1.6377208232879639\n",
      "[LOG 20200511-10:24:44] epoch: 0, batch: 5722 train-loss: 1.9174230098724365\n",
      "[LOG 20200511-10:24:45] epoch: 0, batch: 5723 train-loss: 2.099745273590088\n",
      "[LOG 20200511-10:24:45] epoch: 0, batch: 5724 train-loss: 1.1643457412719727\n",
      "[LOG 20200511-10:24:45] epoch: 0, batch: 5725 train-loss: 0.6013982892036438\n",
      "[LOG 20200511-10:24:45] epoch: 0, batch: 5726 train-loss: 1.6668800115585327\n",
      "[LOG 20200511-10:24:45] epoch: 0, batch: 5727 train-loss: 1.228863000869751\n",
      "[LOG 20200511-10:24:45] epoch: 0, batch: 5728 train-loss: 1.8526867628097534\n",
      "[LOG 20200511-10:24:45] epoch: 0, batch: 5729 train-loss: 1.5028711557388306\n",
      "[LOG 20200511-10:24:45] epoch: 0, batch: 5730 train-loss: 1.3962886333465576\n",
      "[LOG 20200511-10:24:45] epoch: 0, batch: 5731 train-loss: 1.9049357175827026\n",
      "[LOG 20200511-10:24:46] epoch: 0, batch: 5732 train-loss: 1.1134848594665527\n",
      "[LOG 20200511-10:24:46] epoch: 0, batch: 5733 train-loss: 0.976151704788208\n",
      "[LOG 20200511-10:24:46] epoch: 0, batch: 5734 train-loss: 1.1411583423614502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:24:46] epoch: 0, batch: 5735 train-loss: 1.052703857421875\n",
      "[LOG 20200511-10:24:46] epoch: 0, batch: 5736 train-loss: 1.096720576286316\n",
      "[LOG 20200511-10:24:46] epoch: 0, batch: 5737 train-loss: 1.2246192693710327\n",
      "[LOG 20200511-10:24:46] epoch: 0, batch: 5738 train-loss: 2.1051132678985596\n",
      "[LOG 20200511-10:24:46] epoch: 0, batch: 5739 train-loss: 1.3233318328857422\n",
      "[LOG 20200511-10:24:46] epoch: 0, batch: 5740 train-loss: 0.8061046600341797\n",
      "[LOG 20200511-10:24:47] epoch: 0, batch: 5741 train-loss: 0.9906683564186096\n",
      "[LOG 20200511-10:24:47] epoch: 0, batch: 5742 train-loss: 1.8557771444320679\n",
      "[LOG 20200511-10:24:47] epoch: 0, batch: 5743 train-loss: 1.904144048690796\n",
      "[LOG 20200511-10:24:47] epoch: 0, batch: 5744 train-loss: 1.5682249069213867\n",
      "[LOG 20200511-10:24:47] epoch: 0, batch: 5745 train-loss: 2.3499596118927\n",
      "[LOG 20200511-10:24:47] epoch: 0, batch: 5746 train-loss: 1.8398261070251465\n",
      "[LOG 20200511-10:24:47] epoch: 0, batch: 5747 train-loss: 2.1487631797790527\n",
      "[LOG 20200511-10:24:47] epoch: 0, batch: 5748 train-loss: 3.1448190212249756\n",
      "[LOG 20200511-10:24:47] epoch: 0, batch: 5749 train-loss: 1.3057968616485596\n",
      "[LOG 20200511-10:24:48] epoch: 0, batch: 5750 train-loss: 0.4893941283226013\n",
      "[LOG 20200511-10:24:48] epoch: 0, batch: 5751 train-loss: 0.7928158044815063\n",
      "[LOG 20200511-10:24:48] epoch: 0, batch: 5752 train-loss: 1.0289088487625122\n",
      "[LOG 20200511-10:24:48] epoch: 0, batch: 5753 train-loss: 1.7292652130126953\n",
      "[LOG 20200511-10:24:48] epoch: 0, batch: 5754 train-loss: 1.935011386871338\n",
      "[LOG 20200511-10:24:48] epoch: 0, batch: 5755 train-loss: 1.4869872331619263\n",
      "[LOG 20200511-10:24:48] epoch: 0, batch: 5756 train-loss: 2.213796377182007\n",
      "[LOG 20200511-10:24:48] epoch: 0, batch: 5757 train-loss: 1.7466819286346436\n",
      "[LOG 20200511-10:24:49] epoch: 0, batch: 5758 train-loss: 1.4521923065185547\n",
      "[LOG 20200511-10:24:49] epoch: 0, batch: 5759 train-loss: 1.3095823526382446\n",
      "[LOG 20200511-10:24:49] epoch: 0, batch: 5760 train-loss: 1.6691604852676392\n",
      "[LOG 20200511-10:24:49] epoch: 0, batch: 5761 train-loss: 1.462792158126831\n",
      "[LOG 20200511-10:24:49] epoch: 0, batch: 5762 train-loss: 1.8019343614578247\n",
      "[LOG 20200511-10:24:49] epoch: 0, batch: 5763 train-loss: 1.5775392055511475\n",
      "[LOG 20200511-10:24:49] epoch: 0, batch: 5764 train-loss: 2.853736400604248\n",
      "[LOG 20200511-10:24:49] epoch: 0, batch: 5765 train-loss: 1.215330958366394\n",
      "[LOG 20200511-10:24:49] epoch: 0, batch: 5766 train-loss: 2.185715436935425\n",
      "[LOG 20200511-10:24:50] epoch: 0, batch: 5767 train-loss: 0.8862199783325195\n",
      "[LOG 20200511-10:24:50] epoch: 0, batch: 5768 train-loss: 2.297970771789551\n",
      "[LOG 20200511-10:24:50] epoch: 0, batch: 5769 train-loss: 1.0051146745681763\n",
      "[LOG 20200511-10:24:50] epoch: 0, batch: 5770 train-loss: 1.197161316871643\n",
      "[LOG 20200511-10:24:50] epoch: 0, batch: 5771 train-loss: 1.1948782205581665\n",
      "[LOG 20200511-10:24:50] epoch: 0, batch: 5772 train-loss: 1.5535595417022705\n",
      "[LOG 20200511-10:24:50] epoch: 0, batch: 5773 train-loss: 1.2490211725234985\n",
      "[LOG 20200511-10:24:50] epoch: 0, batch: 5774 train-loss: 1.4340636730194092\n",
      "[LOG 20200511-10:24:50] epoch: 0, batch: 5775 train-loss: 1.9967849254608154\n",
      "[LOG 20200511-10:24:51] epoch: 0, batch: 5776 train-loss: 1.0230724811553955\n",
      "[LOG 20200511-10:24:51] epoch: 0, batch: 5777 train-loss: 1.180674433708191\n",
      "[LOG 20200511-10:24:51] epoch: 0, batch: 5778 train-loss: 1.4907463788986206\n",
      "[LOG 20200511-10:24:51] epoch: 0, batch: 5779 train-loss: 0.9956210851669312\n",
      "[LOG 20200511-10:24:51] epoch: 0, batch: 5780 train-loss: 2.141684055328369\n",
      "[LOG 20200511-10:24:51] epoch: 0, batch: 5781 train-loss: 2.212872266769409\n",
      "[LOG 20200511-10:24:51] epoch: 0, batch: 5782 train-loss: 1.4901375770568848\n",
      "[LOG 20200511-10:24:51] epoch: 0, batch: 5783 train-loss: 1.6433088779449463\n",
      "[LOG 20200511-10:24:51] epoch: 0, batch: 5784 train-loss: 1.460585117340088\n",
      "[LOG 20200511-10:24:52] epoch: 0, batch: 5785 train-loss: 1.155195951461792\n",
      "[LOG 20200511-10:24:52] epoch: 0, batch: 5786 train-loss: 1.243277907371521\n",
      "[LOG 20200511-10:24:52] epoch: 0, batch: 5787 train-loss: 1.2769347429275513\n",
      "[LOG 20200511-10:24:52] epoch: 0, batch: 5788 train-loss: 1.4895696640014648\n",
      "[LOG 20200511-10:24:52] epoch: 0, batch: 5789 train-loss: 2.063692331314087\n",
      "[LOG 20200511-10:24:52] epoch: 0, batch: 5790 train-loss: 2.200486660003662\n",
      "[LOG 20200511-10:24:52] epoch: 0, batch: 5791 train-loss: 2.151283025741577\n",
      "[LOG 20200511-10:24:52] epoch: 0, batch: 5792 train-loss: 1.7673715353012085\n",
      "[LOG 20200511-10:24:53] epoch: 0, batch: 5793 train-loss: 1.1910943984985352\n",
      "[LOG 20200511-10:24:53] epoch: 0, batch: 5794 train-loss: 1.2259764671325684\n",
      "[LOG 20200511-10:24:53] epoch: 0, batch: 5795 train-loss: 1.1891916990280151\n",
      "[LOG 20200511-10:24:53] epoch: 0, batch: 5796 train-loss: 1.904667615890503\n",
      "[LOG 20200511-10:24:53] epoch: 0, batch: 5797 train-loss: 1.0179929733276367\n",
      "[LOG 20200511-10:24:53] epoch: 0, batch: 5798 train-loss: 1.5645182132720947\n",
      "[LOG 20200511-10:24:53] epoch: 0, batch: 5799 train-loss: 0.7470602989196777\n",
      "[LOG 20200511-10:24:53] epoch: 0, batch: 5800 train-loss: 0.9241511821746826\n",
      "[LOG 20200511-10:24:53] epoch: 0, batch: 5801 train-loss: 0.9495634436607361\n",
      "[LOG 20200511-10:24:54] epoch: 0, batch: 5802 train-loss: 2.0877199172973633\n",
      "[LOG 20200511-10:24:54] epoch: 0, batch: 5803 train-loss: 2.2403903007507324\n",
      "[LOG 20200511-10:24:54] epoch: 0, batch: 5804 train-loss: 1.492599606513977\n",
      "[LOG 20200511-10:24:54] epoch: 0, batch: 5805 train-loss: 1.084779143333435\n",
      "[LOG 20200511-10:24:54] epoch: 0, batch: 5806 train-loss: 1.2826999425888062\n",
      "[LOG 20200511-10:24:54] epoch: 0, batch: 5807 train-loss: 0.9336373805999756\n",
      "[LOG 20200511-10:24:54] epoch: 0, batch: 5808 train-loss: 1.684713363647461\n",
      "[LOG 20200511-10:24:54] epoch: 0, batch: 5809 train-loss: 1.7881823778152466\n",
      "[LOG 20200511-10:24:55] epoch: 0, batch: 5810 train-loss: 0.9865682125091553\n",
      "[LOG 20200511-10:24:55] epoch: 0, batch: 5811 train-loss: 1.1783733367919922\n",
      "[LOG 20200511-10:24:55] epoch: 0, batch: 5812 train-loss: 1.0795793533325195\n",
      "[LOG 20200511-10:24:55] epoch: 0, batch: 5813 train-loss: 0.9817993640899658\n",
      "[LOG 20200511-10:24:55] epoch: 0, batch: 5814 train-loss: 0.39942798018455505\n",
      "[LOG 20200511-10:24:55] epoch: 0, batch: 5815 train-loss: 2.1612603664398193\n",
      "[LOG 20200511-10:24:55] epoch: 0, batch: 5816 train-loss: 1.4960949420928955\n",
      "[LOG 20200511-10:24:55] epoch: 0, batch: 5817 train-loss: 2.124723434448242\n",
      "[LOG 20200511-10:24:55] epoch: 0, batch: 5818 train-loss: 0.6223433017730713\n",
      "[LOG 20200511-10:24:56] epoch: 0, batch: 5819 train-loss: 1.941725492477417\n",
      "[LOG 20200511-10:24:56] epoch: 0, batch: 5820 train-loss: 0.6612699031829834\n",
      "[LOG 20200511-10:24:56] epoch: 0, batch: 5821 train-loss: 0.9503544569015503\n",
      "[LOG 20200511-10:24:56] epoch: 0, batch: 5822 train-loss: 0.7639658451080322\n",
      "[LOG 20200511-10:24:56] epoch: 0, batch: 5823 train-loss: 1.593761682510376\n",
      "[LOG 20200511-10:24:56] epoch: 0, batch: 5824 train-loss: 0.406245619058609\n",
      "[LOG 20200511-10:24:56] epoch: 0, batch: 5825 train-loss: 1.9173446893692017\n",
      "[LOG 20200511-10:24:56] epoch: 0, batch: 5826 train-loss: 1.404022455215454\n",
      "[LOG 20200511-10:24:56] epoch: 0, batch: 5827 train-loss: 1.9283970594406128\n",
      "[LOG 20200511-10:24:57] epoch: 0, batch: 5828 train-loss: 2.097527503967285\n",
      "[LOG 20200511-10:24:57] epoch: 0, batch: 5829 train-loss: 1.1238960027694702\n",
      "[LOG 20200511-10:24:57] epoch: 0, batch: 5830 train-loss: 1.2614820003509521\n",
      "[LOG 20200511-10:24:57] epoch: 0, batch: 5831 train-loss: 1.3743326663970947\n",
      "[LOG 20200511-10:24:57] epoch: 0, batch: 5832 train-loss: 1.7016714811325073\n",
      "[LOG 20200511-10:24:57] epoch: 0, batch: 5833 train-loss: 1.2409052848815918\n",
      "[LOG 20200511-10:24:57] epoch: 0, batch: 5834 train-loss: 1.4614858627319336\n",
      "[LOG 20200511-10:24:57] epoch: 0, batch: 5835 train-loss: 1.078693151473999\n",
      "[LOG 20200511-10:24:57] epoch: 0, batch: 5836 train-loss: 1.7689497470855713\n",
      "[LOG 20200511-10:24:58] epoch: 0, batch: 5837 train-loss: 0.7571791410446167\n",
      "[LOG 20200511-10:24:58] epoch: 0, batch: 5838 train-loss: 1.443535566329956\n",
      "[LOG 20200511-10:24:58] epoch: 0, batch: 5839 train-loss: 1.0325255393981934\n",
      "[LOG 20200511-10:24:58] epoch: 0, batch: 5840 train-loss: 1.240643858909607\n",
      "[LOG 20200511-10:24:58] epoch: 0, batch: 5841 train-loss: 1.9069041013717651\n",
      "[LOG 20200511-10:24:58] epoch: 0, batch: 5842 train-loss: 1.3473511934280396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:24:58] epoch: 0, batch: 5843 train-loss: 0.9320629835128784\n",
      "[LOG 20200511-10:24:58] epoch: 0, batch: 5844 train-loss: 1.7269725799560547\n",
      "[LOG 20200511-10:24:58] epoch: 0, batch: 5845 train-loss: 2.041337490081787\n",
      "[LOG 20200511-10:24:59] epoch: 0, batch: 5846 train-loss: 0.8578885793685913\n",
      "[LOG 20200511-10:24:59] epoch: 0, batch: 5847 train-loss: 0.6244173645973206\n",
      "[LOG 20200511-10:24:59] epoch: 0, batch: 5848 train-loss: 0.784786581993103\n",
      "[LOG 20200511-10:24:59] epoch: 0, batch: 5849 train-loss: 1.2503741979599\n",
      "[LOG 20200511-10:24:59] epoch: 0, batch: 5850 train-loss: 1.117680549621582\n",
      "[LOG 20200511-10:24:59] epoch: 0, batch: 5851 train-loss: 0.36560314893722534\n",
      "[LOG 20200511-10:24:59] epoch: 0, batch: 5852 train-loss: 0.815868616104126\n",
      "[LOG 20200511-10:24:59] epoch: 0, batch: 5853 train-loss: 1.2328410148620605\n",
      "[LOG 20200511-10:24:59] epoch: 0, batch: 5854 train-loss: 0.6364412307739258\n",
      "[LOG 20200511-10:25:00] epoch: 0, batch: 5855 train-loss: 1.8777964115142822\n",
      "[LOG 20200511-10:25:00] epoch: 0, batch: 5856 train-loss: 0.8303362131118774\n",
      "[LOG 20200511-10:25:00] epoch: 0, batch: 5857 train-loss: 1.1058093309402466\n",
      "[LOG 20200511-10:25:00] epoch: 0, batch: 5858 train-loss: 1.093299150466919\n",
      "[LOG 20200511-10:25:00] epoch: 0, batch: 5859 train-loss: 0.6110270619392395\n",
      "[LOG 20200511-10:25:00] epoch: 0, batch: 5860 train-loss: 2.327056884765625\n",
      "[LOG 20200511-10:25:00] epoch: 0, batch: 5861 train-loss: 1.8344372510910034\n",
      "[LOG 20200511-10:25:00] epoch: 0, batch: 5862 train-loss: 1.397589087486267\n",
      "[LOG 20200511-10:25:00] epoch: 0, batch: 5863 train-loss: 2.1400208473205566\n",
      "[LOG 20200511-10:25:01] epoch: 0, batch: 5864 train-loss: 1.1784566640853882\n",
      "[LOG 20200511-10:25:01] epoch: 0, batch: 5865 train-loss: 1.2480602264404297\n",
      "[LOG 20200511-10:25:01] epoch: 0, batch: 5866 train-loss: 1.082019567489624\n",
      "[LOG 20200511-10:25:01] epoch: 0, batch: 5867 train-loss: 1.2136328220367432\n",
      "[LOG 20200511-10:25:01] epoch: 0, batch: 5868 train-loss: 0.7070617079734802\n",
      "[LOG 20200511-10:25:01] epoch: 0, batch: 5869 train-loss: 1.3295834064483643\n",
      "[LOG 20200511-10:25:01] epoch: 0, batch: 5870 train-loss: 2.22255802154541\n",
      "[LOG 20200511-10:25:01] epoch: 0, batch: 5871 train-loss: 0.9460645914077759\n",
      "[LOG 20200511-10:25:01] epoch: 0, batch: 5872 train-loss: 1.9692949056625366\n",
      "[LOG 20200511-10:25:02] epoch: 0, batch: 5873 train-loss: 0.8750059604644775\n",
      "[LOG 20200511-10:25:02] epoch: 0, batch: 5874 train-loss: 1.3297350406646729\n",
      "[LOG 20200511-10:25:02] epoch: 0, batch: 5875 train-loss: 1.2663002014160156\n",
      "[LOG 20200511-10:25:02] epoch: 0, batch: 5876 train-loss: 2.1178388595581055\n",
      "[LOG 20200511-10:25:02] epoch: 0, batch: 5877 train-loss: 0.9060791730880737\n",
      "[LOG 20200511-10:25:02] epoch: 0, batch: 5878 train-loss: 1.411935806274414\n",
      "[LOG 20200511-10:25:02] epoch: 0, batch: 5879 train-loss: 1.5081971883773804\n",
      "[LOG 20200511-10:25:02] epoch: 0, batch: 5880 train-loss: 1.489386796951294\n",
      "[LOG 20200511-10:25:02] epoch: 0, batch: 5881 train-loss: 0.4765965938568115\n",
      "[LOG 20200511-10:25:03] epoch: 0, batch: 5882 train-loss: 1.4726694822311401\n",
      "[LOG 20200511-10:25:03] epoch: 0, batch: 5883 train-loss: 1.1080158948898315\n",
      "[LOG 20200511-10:25:03] epoch: 0, batch: 5884 train-loss: 1.2809040546417236\n",
      "[LOG 20200511-10:25:03] epoch: 0, batch: 5885 train-loss: 0.84665447473526\n",
      "[LOG 20200511-10:25:03] epoch: 0, batch: 5886 train-loss: 2.8419547080993652\n",
      "[LOG 20200511-10:25:03] epoch: 0, batch: 5887 train-loss: 1.5665754079818726\n",
      "[LOG 20200511-10:25:03] epoch: 0, batch: 5888 train-loss: 1.4641278982162476\n",
      "[LOG 20200511-10:25:03] epoch: 0, batch: 5889 train-loss: 1.172987461090088\n",
      "[LOG 20200511-10:25:03] epoch: 0, batch: 5890 train-loss: 0.9353758096694946\n",
      "[LOG 20200511-10:25:04] epoch: 0, batch: 5891 train-loss: 2.018247127532959\n",
      "[LOG 20200511-10:25:04] epoch: 0, batch: 5892 train-loss: 1.073094367980957\n",
      "[LOG 20200511-10:25:04] epoch: 0, batch: 5893 train-loss: 1.3456449508666992\n",
      "[LOG 20200511-10:25:04] epoch: 0, batch: 5894 train-loss: 1.6152551174163818\n",
      "[LOG 20200511-10:25:04] epoch: 0, batch: 5895 train-loss: 0.6934438347816467\n",
      "[LOG 20200511-10:25:04] epoch: 0, batch: 5896 train-loss: 1.448425054550171\n",
      "[LOG 20200511-10:25:04] epoch: 0, batch: 5897 train-loss: 0.5401935577392578\n",
      "[LOG 20200511-10:25:04] epoch: 0, batch: 5898 train-loss: 1.0421640872955322\n",
      "[LOG 20200511-10:25:04] epoch: 0, batch: 5899 train-loss: 1.3793377876281738\n",
      "[LOG 20200511-10:25:05] epoch: 0, batch: 5900 train-loss: 0.6639870405197144\n",
      "[LOG 20200511-10:25:05] epoch: 0, batch: 5901 train-loss: 1.5207124948501587\n",
      "[LOG 20200511-10:25:05] epoch: 0, batch: 5902 train-loss: 1.4994947910308838\n",
      "[LOG 20200511-10:25:05] epoch: 0, batch: 5903 train-loss: 0.8286327719688416\n",
      "[LOG 20200511-10:25:05] epoch: 0, batch: 5904 train-loss: 1.536400556564331\n",
      "[LOG 20200511-10:25:05] epoch: 0, batch: 5905 train-loss: 1.6747081279754639\n",
      "[LOG 20200511-10:25:05] epoch: 0, batch: 5906 train-loss: 1.301687240600586\n",
      "[LOG 20200511-10:25:05] epoch: 0, batch: 5907 train-loss: 0.9456781148910522\n",
      "[LOG 20200511-10:25:05] epoch: 0, batch: 5908 train-loss: 1.550929069519043\n",
      "[LOG 20200511-10:25:06] epoch: 0, batch: 5909 train-loss: 0.877873420715332\n",
      "[LOG 20200511-10:25:06] epoch: 0, batch: 5910 train-loss: 0.3422170877456665\n",
      "[LOG 20200511-10:25:06] epoch: 0, batch: 5911 train-loss: 0.7866325974464417\n",
      "[LOG 20200511-10:25:06] epoch: 0, batch: 5912 train-loss: 1.3852603435516357\n",
      "[LOG 20200511-10:25:06] epoch: 0, batch: 5913 train-loss: 0.7937104105949402\n",
      "[LOG 20200511-10:25:06] epoch: 0, batch: 5914 train-loss: 2.148426055908203\n",
      "[LOG 20200511-10:25:06] epoch: 0, batch: 5915 train-loss: 0.7587566375732422\n",
      "[LOG 20200511-10:25:06] epoch: 0, batch: 5916 train-loss: 1.4843451976776123\n",
      "[LOG 20200511-10:25:06] epoch: 0, batch: 5917 train-loss: 0.820580244064331\n",
      "[LOG 20200511-10:25:07] epoch: 0, batch: 5918 train-loss: 1.0853060483932495\n",
      "[LOG 20200511-10:25:07] epoch: 0, batch: 5919 train-loss: 1.3168063163757324\n",
      "[LOG 20200511-10:25:07] epoch: 0, batch: 5920 train-loss: 1.1162543296813965\n",
      "[LOG 20200511-10:25:07] epoch: 0, batch: 5921 train-loss: 0.9598284959793091\n",
      "[LOG 20200511-10:25:07] epoch: 0, batch: 5922 train-loss: 1.2703253030776978\n",
      "[LOG 20200511-10:25:07] epoch: 0, batch: 5923 train-loss: 1.7963758707046509\n",
      "[LOG 20200511-10:25:07] epoch: 0, batch: 5924 train-loss: 1.2677289247512817\n",
      "[LOG 20200511-10:25:07] epoch: 0, batch: 5925 train-loss: 1.6925427913665771\n",
      "[LOG 20200511-10:25:07] epoch: 0, batch: 5926 train-loss: 1.202390432357788\n",
      "[LOG 20200511-10:25:08] epoch: 0, batch: 5927 train-loss: 1.0437108278274536\n",
      "[LOG 20200511-10:25:08] epoch: 0, batch: 5928 train-loss: 2.128690481185913\n",
      "[LOG 20200511-10:25:08] epoch: 0, batch: 5929 train-loss: 1.7638800144195557\n",
      "[LOG 20200511-10:25:08] epoch: 0, batch: 5930 train-loss: 0.7869086861610413\n",
      "[LOG 20200511-10:25:08] epoch: 0, batch: 5931 train-loss: 1.024147868156433\n",
      "[LOG 20200511-10:25:08] epoch: 0, batch: 5932 train-loss: 0.5125153064727783\n",
      "[LOG 20200511-10:25:08] epoch: 0, batch: 5933 train-loss: 1.6373916864395142\n",
      "[LOG 20200511-10:25:08] epoch: 0, batch: 5934 train-loss: 0.9703012704849243\n",
      "[LOG 20200511-10:25:08] epoch: 0, batch: 5935 train-loss: 1.8428869247436523\n",
      "[LOG 20200511-10:25:09] epoch: 0, batch: 5936 train-loss: 2.8093152046203613\n",
      "[LOG 20200511-10:25:09] epoch: 0, batch: 5937 train-loss: 1.602848768234253\n",
      "[LOG 20200511-10:25:09] epoch: 0, batch: 5938 train-loss: 1.9445223808288574\n",
      "[LOG 20200511-10:25:09] epoch: 0, batch: 5939 train-loss: 0.7913134098052979\n",
      "[LOG 20200511-10:25:09] epoch: 0, batch: 5940 train-loss: 1.17931067943573\n",
      "[LOG 20200511-10:25:09] epoch: 0, batch: 5941 train-loss: 1.0658459663391113\n",
      "[LOG 20200511-10:25:09] epoch: 0, batch: 5942 train-loss: 1.67118501663208\n",
      "[LOG 20200511-10:25:09] epoch: 0, batch: 5943 train-loss: 1.1166774034500122\n",
      "[LOG 20200511-10:25:09] epoch: 0, batch: 5944 train-loss: 1.4613800048828125\n",
      "[LOG 20200511-10:25:10] epoch: 0, batch: 5945 train-loss: 0.8026900291442871\n",
      "[LOG 20200511-10:25:10] epoch: 0, batch: 5946 train-loss: 1.315248966217041\n",
      "[LOG 20200511-10:25:10] epoch: 0, batch: 5947 train-loss: 0.7385317087173462\n",
      "[LOG 20200511-10:25:10] epoch: 0, batch: 5948 train-loss: 1.6754720211029053\n",
      "[LOG 20200511-10:25:10] epoch: 0, batch: 5949 train-loss: 0.8244537711143494\n",
      "[LOG 20200511-10:25:10] epoch: 0, batch: 5950 train-loss: 2.366520881652832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:25:10] epoch: 0, batch: 5951 train-loss: 1.448337197303772\n",
      "[LOG 20200511-10:25:10] epoch: 0, batch: 5952 train-loss: 1.5248857736587524\n",
      "[LOG 20200511-10:25:10] epoch: 0, batch: 5953 train-loss: 2.156846523284912\n",
      "[LOG 20200511-10:25:11] epoch: 0, batch: 5954 train-loss: 0.6903862357139587\n",
      "[LOG 20200511-10:25:11] epoch: 0, batch: 5955 train-loss: 1.3186053037643433\n",
      "[LOG 20200511-10:25:11] epoch: 0, batch: 5956 train-loss: 2.1964011192321777\n",
      "[LOG 20200511-10:25:11] epoch: 0, batch: 5957 train-loss: 1.4592351913452148\n",
      "[LOG 20200511-10:25:11] epoch: 0, batch: 5958 train-loss: 1.553879737854004\n",
      "[LOG 20200511-10:25:11] epoch: 0, batch: 5959 train-loss: 1.4151923656463623\n",
      "[LOG 20200511-10:25:11] epoch: 0, batch: 5960 train-loss: 1.2306162118911743\n",
      "[LOG 20200511-10:25:11] epoch: 0, batch: 5961 train-loss: 1.0305283069610596\n",
      "[LOG 20200511-10:25:11] epoch: 0, batch: 5962 train-loss: 1.5772967338562012\n",
      "[LOG 20200511-10:25:12] epoch: 0, batch: 5963 train-loss: 1.8535224199295044\n",
      "[LOG 20200511-10:25:12] epoch: 0, batch: 5964 train-loss: 0.8009825348854065\n",
      "[LOG 20200511-10:25:12] epoch: 0, batch: 5965 train-loss: 1.5932245254516602\n",
      "[LOG 20200511-10:25:12] epoch: 0, batch: 5966 train-loss: 1.8097596168518066\n",
      "[LOG 20200511-10:25:12] epoch: 0, batch: 5967 train-loss: 1.006169080734253\n",
      "[LOG 20200511-10:25:12] epoch: 0, batch: 5968 train-loss: 1.317522406578064\n",
      "[LOG 20200511-10:25:12] epoch: 0, batch: 5969 train-loss: 1.2726423740386963\n",
      "[LOG 20200511-10:25:12] epoch: 0, batch: 5970 train-loss: 0.6782475709915161\n",
      "[LOG 20200511-10:25:12] epoch: 0, batch: 5971 train-loss: 1.645174264907837\n",
      "[LOG 20200511-10:25:13] epoch: 0, batch: 5972 train-loss: 1.2718393802642822\n",
      "[LOG 20200511-10:25:13] epoch: 0, batch: 5973 train-loss: 1.570826530456543\n",
      "[LOG 20200511-10:25:13] epoch: 0, batch: 5974 train-loss: 1.9213567972183228\n",
      "[LOG 20200511-10:25:13] epoch: 0, batch: 5975 train-loss: 0.7129637002944946\n",
      "[LOG 20200511-10:25:13] epoch: 0, batch: 5976 train-loss: 1.4763754606246948\n",
      "[LOG 20200511-10:25:13] epoch: 0, batch: 5977 train-loss: 0.48920682072639465\n",
      "[LOG 20200511-10:25:13] epoch: 0, batch: 5978 train-loss: 0.41237664222717285\n",
      "[LOG 20200511-10:25:13] epoch: 0, batch: 5979 train-loss: 0.9241429567337036\n",
      "[LOG 20200511-10:25:13] epoch: 0, batch: 5980 train-loss: 1.5202547311782837\n",
      "[LOG 20200511-10:25:14] epoch: 0, batch: 5981 train-loss: 0.901458740234375\n",
      "[LOG 20200511-10:25:14] epoch: 0, batch: 5982 train-loss: 1.1732853651046753\n",
      "[LOG 20200511-10:25:14] epoch: 0, batch: 5983 train-loss: 1.2915573120117188\n",
      "[LOG 20200511-10:25:14] epoch: 0, batch: 5984 train-loss: 1.3778223991394043\n",
      "[LOG 20200511-10:25:14] epoch: 0, batch: 5985 train-loss: 1.283104658126831\n",
      "[LOG 20200511-10:25:14] epoch: 0, batch: 5986 train-loss: 1.3100682497024536\n",
      "[LOG 20200511-10:25:14] epoch: 0, batch: 5987 train-loss: 0.38977137207984924\n",
      "[LOG 20200511-10:25:14] epoch: 0, batch: 5988 train-loss: 1.4198429584503174\n",
      "[LOG 20200511-10:25:14] epoch: 0, batch: 5989 train-loss: 1.0898796319961548\n",
      "[LOG 20200511-10:25:15] epoch: 0, batch: 5990 train-loss: 0.8818392157554626\n",
      "[LOG 20200511-10:25:15] epoch: 0, batch: 5991 train-loss: 1.596405267715454\n",
      "[LOG 20200511-10:25:15] epoch: 0, batch: 5992 train-loss: 1.4589643478393555\n",
      "[LOG 20200511-10:25:15] epoch: 0, batch: 5993 train-loss: 1.6043665409088135\n",
      "[LOG 20200511-10:25:15] epoch: 0, batch: 5994 train-loss: 1.495286226272583\n",
      "[LOG 20200511-10:25:15] epoch: 0, batch: 5995 train-loss: 2.00441312789917\n",
      "[LOG 20200511-10:25:15] epoch: 0, batch: 5996 train-loss: 1.9136030673980713\n",
      "[LOG 20200511-10:25:15] epoch: 0, batch: 5997 train-loss: 0.6304964423179626\n",
      "[LOG 20200511-10:25:15] epoch: 0, batch: 5998 train-loss: 1.3562099933624268\n",
      "[LOG 20200511-10:25:16] epoch: 0, batch: 5999 train-loss: 0.5210919976234436\n",
      "[LOG 20200511-10:25:16] epoch: 0, batch: 6000 train-loss: 1.916281819343567\n",
      "[LOG 20200511-10:25:16] epoch: 0, batch: 6001 train-loss: 1.4968947172164917\n",
      "[LOG 20200511-10:25:16] epoch: 0, batch: 6002 train-loss: 1.1830811500549316\n",
      "[LOG 20200511-10:25:16] epoch: 0, batch: 6003 train-loss: 0.7247076034545898\n",
      "[LOG 20200511-10:25:16] epoch: 0, batch: 6004 train-loss: 1.9359033107757568\n",
      "[LOG 20200511-10:25:16] epoch: 0, batch: 6005 train-loss: 2.3712520599365234\n",
      "[LOG 20200511-10:25:16] epoch: 0, batch: 6006 train-loss: 1.0810630321502686\n",
      "[LOG 20200511-10:25:17] epoch: 0, batch: 6007 train-loss: 1.1627769470214844\n",
      "[LOG 20200511-10:25:17] epoch: 0, batch: 6008 train-loss: 1.4032008647918701\n",
      "[LOG 20200511-10:25:17] epoch: 0, batch: 6009 train-loss: 1.4072136878967285\n",
      "[LOG 20200511-10:25:17] epoch: 0, batch: 6010 train-loss: 1.4812791347503662\n",
      "[LOG 20200511-10:25:17] epoch: 0, batch: 6011 train-loss: 1.332343339920044\n",
      "[LOG 20200511-10:25:17] epoch: 0, batch: 6012 train-loss: 1.0301907062530518\n",
      "[LOG 20200511-10:25:17] epoch: 0, batch: 6013 train-loss: 0.9343500733375549\n",
      "[LOG 20200511-10:25:17] epoch: 0, batch: 6014 train-loss: 0.5649937391281128\n",
      "[LOG 20200511-10:25:17] epoch: 0, batch: 6015 train-loss: 1.7822917699813843\n",
      "[LOG 20200511-10:25:18] epoch: 0, batch: 6016 train-loss: 0.7056909799575806\n",
      "[LOG 20200511-10:25:18] epoch: 0, batch: 6017 train-loss: 1.0094828605651855\n",
      "[LOG 20200511-10:25:18] epoch: 0, batch: 6018 train-loss: 0.888887345790863\n",
      "[LOG 20200511-10:25:18] epoch: 0, batch: 6019 train-loss: 1.6545674800872803\n",
      "[LOG 20200511-10:25:18] epoch: 0, batch: 6020 train-loss: 1.5427656173706055\n",
      "[LOG 20200511-10:25:18] epoch: 0, batch: 6021 train-loss: 1.6738945245742798\n",
      "[LOG 20200511-10:25:18] epoch: 0, batch: 6022 train-loss: 1.1981995105743408\n",
      "[LOG 20200511-10:25:18] epoch: 0, batch: 6023 train-loss: 1.6570582389831543\n",
      "[LOG 20200511-10:25:19] epoch: 0, batch: 6024 train-loss: 0.5626411437988281\n",
      "[LOG 20200511-10:25:19] epoch: 0, batch: 6025 train-loss: 1.8959016799926758\n",
      "[LOG 20200511-10:25:19] epoch: 0, batch: 6026 train-loss: 1.6545871496200562\n",
      "[LOG 20200511-10:25:19] epoch: 0, batch: 6027 train-loss: 1.2945024967193604\n",
      "[LOG 20200511-10:25:19] epoch: 0, batch: 6028 train-loss: 1.0006294250488281\n",
      "[LOG 20200511-10:25:19] epoch: 0, batch: 6029 train-loss: 1.820159673690796\n",
      "[LOG 20200511-10:25:19] epoch: 0, batch: 6030 train-loss: 0.5623568296432495\n",
      "[LOG 20200511-10:25:19] epoch: 0, batch: 6031 train-loss: 1.821210265159607\n",
      "[LOG 20200511-10:25:20] epoch: 0, batch: 6032 train-loss: 1.9622877836227417\n",
      "[LOG 20200511-10:25:20] epoch: 0, batch: 6033 train-loss: 1.3907281160354614\n",
      "[LOG 20200511-10:25:20] epoch: 0, batch: 6034 train-loss: 0.5649564266204834\n",
      "[LOG 20200511-10:25:20] epoch: 0, batch: 6035 train-loss: 1.5616426467895508\n",
      "[LOG 20200511-10:25:20] epoch: 0, batch: 6036 train-loss: 2.028242826461792\n",
      "[LOG 20200511-10:25:20] epoch: 0, batch: 6037 train-loss: 1.4072849750518799\n",
      "[LOG 20200511-10:25:20] epoch: 0, batch: 6038 train-loss: 0.596632719039917\n",
      "[LOG 20200511-10:25:20] epoch: 0, batch: 6039 train-loss: 1.4026985168457031\n",
      "[LOG 20200511-10:25:21] epoch: 0, batch: 6040 train-loss: 1.5785412788391113\n",
      "[LOG 20200511-10:25:21] epoch: 0, batch: 6041 train-loss: 1.6601685285568237\n",
      "[LOG 20200511-10:25:21] epoch: 0, batch: 6042 train-loss: 1.9983596801757812\n",
      "[LOG 20200511-10:25:21] epoch: 0, batch: 6043 train-loss: 2.152574062347412\n",
      "[LOG 20200511-10:25:21] epoch: 0, batch: 6044 train-loss: 1.2027275562286377\n",
      "[LOG 20200511-10:25:21] epoch: 0, batch: 6045 train-loss: 1.3131235837936401\n",
      "[LOG 20200511-10:25:21] epoch: 0, batch: 6046 train-loss: 1.1207977533340454\n",
      "[LOG 20200511-10:25:21] epoch: 0, batch: 6047 train-loss: 1.3452110290527344\n",
      "[LOG 20200511-10:25:22] epoch: 0, batch: 6048 train-loss: 1.5238944292068481\n",
      "[LOG 20200511-10:25:22] epoch: 0, batch: 6049 train-loss: 0.8631748557090759\n",
      "[LOG 20200511-10:25:22] epoch: 0, batch: 6050 train-loss: 0.7490001916885376\n",
      "[LOG 20200511-10:25:22] epoch: 0, batch: 6051 train-loss: 1.8656017780303955\n",
      "[LOG 20200511-10:25:22] epoch: 0, batch: 6052 train-loss: 1.795534372329712\n",
      "[LOG 20200511-10:25:22] epoch: 0, batch: 6053 train-loss: 1.034757137298584\n",
      "[LOG 20200511-10:25:22] epoch: 0, batch: 6054 train-loss: 1.9877816438674927\n",
      "[LOG 20200511-10:25:22] epoch: 0, batch: 6055 train-loss: 1.6489918231964111\n",
      "[LOG 20200511-10:25:23] epoch: 0, batch: 6056 train-loss: 0.9256988763809204\n",
      "[LOG 20200511-10:25:23] epoch: 0, batch: 6057 train-loss: 1.45986008644104\n",
      "[LOG 20200511-10:25:23] epoch: 0, batch: 6058 train-loss: 0.9747840166091919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:25:23] epoch: 0, batch: 6059 train-loss: 1.0341510772705078\n",
      "[LOG 20200511-10:25:23] epoch: 0, batch: 6060 train-loss: 1.8426737785339355\n",
      "[LOG 20200511-10:25:23] epoch: 0, batch: 6061 train-loss: 0.7202214598655701\n",
      "[LOG 20200511-10:25:23] epoch: 0, batch: 6062 train-loss: 0.7512282133102417\n",
      "[LOG 20200511-10:25:23] epoch: 0, batch: 6063 train-loss: 1.4100449085235596\n",
      "[LOG 20200511-10:25:24] epoch: 0, batch: 6064 train-loss: 1.3095157146453857\n",
      "[LOG 20200511-10:25:24] epoch: 0, batch: 6065 train-loss: 1.008479356765747\n",
      "[LOG 20200511-10:25:24] epoch: 0, batch: 6066 train-loss: 2.0202198028564453\n",
      "[LOG 20200511-10:25:24] epoch: 0, batch: 6067 train-loss: 1.7943391799926758\n",
      "[LOG 20200511-10:25:24] epoch: 0, batch: 6068 train-loss: 1.5271453857421875\n",
      "[LOG 20200511-10:25:24] epoch: 0, batch: 6069 train-loss: 1.9490654468536377\n",
      "[LOG 20200511-10:25:24] epoch: 0, batch: 6070 train-loss: 2.0269646644592285\n",
      "[LOG 20200511-10:25:24] epoch: 0, batch: 6071 train-loss: 1.7314774990081787\n",
      "[LOG 20200511-10:25:25] epoch: 0, batch: 6072 train-loss: 1.2393885850906372\n",
      "[LOG 20200511-10:25:25] epoch: 0, batch: 6073 train-loss: 1.4032894372940063\n",
      "[LOG 20200511-10:25:25] epoch: 0, batch: 6074 train-loss: 1.9838438034057617\n",
      "[LOG 20200511-10:25:25] epoch: 0, batch: 6075 train-loss: 2.610381841659546\n",
      "[LOG 20200511-10:25:25] epoch: 0, batch: 6076 train-loss: 1.5434660911560059\n",
      "[LOG 20200511-10:25:25] epoch: 0, batch: 6077 train-loss: 0.7844290733337402\n",
      "[LOG 20200511-10:25:25] epoch: 0, batch: 6078 train-loss: 1.374477505683899\n",
      "[LOG 20200511-10:25:25] epoch: 0, batch: 6079 train-loss: 1.0712499618530273\n",
      "[LOG 20200511-10:25:26] epoch: 0, batch: 6080 train-loss: 1.2885167598724365\n",
      "[LOG 20200511-10:25:26] epoch: 0, batch: 6081 train-loss: 1.903835415840149\n",
      "[LOG 20200511-10:25:26] epoch: 0, batch: 6082 train-loss: 0.8151488900184631\n",
      "[LOG 20200511-10:25:26] epoch: 0, batch: 6083 train-loss: 0.7793990969657898\n",
      "[LOG 20200511-10:25:26] epoch: 0, batch: 6084 train-loss: 1.1256515979766846\n",
      "[LOG 20200511-10:25:26] epoch: 0, batch: 6085 train-loss: 1.9661786556243896\n",
      "[LOG 20200511-10:25:26] epoch: 0, batch: 6086 train-loss: 1.1085008382797241\n",
      "[LOG 20200511-10:25:26] epoch: 0, batch: 6087 train-loss: 1.0029878616333008\n",
      "[LOG 20200511-10:25:27] epoch: 0, batch: 6088 train-loss: 0.7349150776863098\n",
      "[LOG 20200511-10:25:27] epoch: 0, batch: 6089 train-loss: 2.5039329528808594\n",
      "[LOG 20200511-10:25:27] epoch: 0, batch: 6090 train-loss: 1.4607014656066895\n",
      "[LOG 20200511-10:25:27] epoch: 0, batch: 6091 train-loss: 2.273866891860962\n",
      "[LOG 20200511-10:25:27] epoch: 0, batch: 6092 train-loss: 1.1188244819641113\n",
      "[LOG 20200511-10:25:27] epoch: 0, batch: 6093 train-loss: 0.8330004215240479\n",
      "[LOG 20200511-10:25:27] epoch: 0, batch: 6094 train-loss: 0.914040207862854\n",
      "[LOG 20200511-10:25:27] epoch: 0, batch: 6095 train-loss: 1.061960220336914\n",
      "[LOG 20200511-10:25:27] epoch: 0, batch: 6096 train-loss: 1.1514731645584106\n",
      "[LOG 20200511-10:25:28] epoch: 0, batch: 6097 train-loss: 1.713425874710083\n",
      "[LOG 20200511-10:25:28] epoch: 0, batch: 6098 train-loss: 1.0396062135696411\n",
      "[LOG 20200511-10:25:28] epoch: 0, batch: 6099 train-loss: 0.9960023164749146\n",
      "[LOG 20200511-10:25:28] epoch: 0, batch: 6100 train-loss: 1.5735220909118652\n",
      "[LOG 20200511-10:25:28] epoch: 0, batch: 6101 train-loss: 1.9882923364639282\n",
      "[LOG 20200511-10:25:28] epoch: 0, batch: 6102 train-loss: 1.2325561046600342\n",
      "[LOG 20200511-10:25:28] epoch: 0, batch: 6103 train-loss: 1.052014708518982\n",
      "[LOG 20200511-10:25:28] epoch: 0, batch: 6104 train-loss: 1.5671055316925049\n",
      "[LOG 20200511-10:25:29] epoch: 0, batch: 6105 train-loss: 0.6129069924354553\n",
      "[LOG 20200511-10:25:29] epoch: 0, batch: 6106 train-loss: 0.9525554180145264\n",
      "[LOG 20200511-10:25:29] epoch: 0, batch: 6107 train-loss: 1.0639399290084839\n",
      "[LOG 20200511-10:25:29] epoch: 0, batch: 6108 train-loss: 1.3608496189117432\n",
      "[LOG 20200511-10:25:29] epoch: 0, batch: 6109 train-loss: 0.9092202186584473\n",
      "[LOG 20200511-10:25:29] epoch: 0, batch: 6110 train-loss: 1.496569275856018\n",
      "[LOG 20200511-10:25:29] epoch: 0, batch: 6111 train-loss: 1.400749683380127\n",
      "[LOG 20200511-10:25:29] epoch: 0, batch: 6112 train-loss: 0.5160663723945618\n",
      "[LOG 20200511-10:25:30] epoch: 0, batch: 6113 train-loss: 1.2692956924438477\n",
      "[LOG 20200511-10:25:30] epoch: 0, batch: 6114 train-loss: 1.2464044094085693\n",
      "[LOG 20200511-10:25:30] epoch: 0, batch: 6115 train-loss: 2.7784645557403564\n",
      "[LOG 20200511-10:25:30] epoch: 0, batch: 6116 train-loss: 1.3472284078598022\n",
      "[LOG 20200511-10:25:30] epoch: 0, batch: 6117 train-loss: 1.2771422863006592\n",
      "[LOG 20200511-10:25:30] epoch: 0, batch: 6118 train-loss: 0.8341576457023621\n",
      "[LOG 20200511-10:25:30] epoch: 0, batch: 6119 train-loss: 1.8449188470840454\n",
      "[LOG 20200511-10:25:30] epoch: 0, batch: 6120 train-loss: 1.00552499294281\n",
      "[LOG 20200511-10:25:31] epoch: 0, batch: 6121 train-loss: 1.6578030586242676\n",
      "[LOG 20200511-10:25:31] epoch: 0, batch: 6122 train-loss: 0.598452091217041\n",
      "[LOG 20200511-10:25:31] epoch: 0, batch: 6123 train-loss: 1.4794288873672485\n",
      "[LOG 20200511-10:25:31] epoch: 0, batch: 6124 train-loss: 1.6594982147216797\n",
      "[LOG 20200511-10:25:31] epoch: 0, batch: 6125 train-loss: 1.4536933898925781\n",
      "[LOG 20200511-10:25:31] epoch: 0, batch: 6126 train-loss: 2.2411186695098877\n",
      "[LOG 20200511-10:25:31] epoch: 0, batch: 6127 train-loss: 1.149566411972046\n",
      "[LOG 20200511-10:25:31] epoch: 0, batch: 6128 train-loss: 1.5474497079849243\n",
      "[LOG 20200511-10:25:31] epoch: 0, batch: 6129 train-loss: 0.697345495223999\n",
      "[LOG 20200511-10:25:32] epoch: 0, batch: 6130 train-loss: 2.834354877471924\n",
      "[LOG 20200511-10:25:32] epoch: 0, batch: 6131 train-loss: 1.6166768074035645\n",
      "[LOG 20200511-10:25:32] epoch: 0, batch: 6132 train-loss: 0.9783828258514404\n",
      "[LOG 20200511-10:25:32] epoch: 0, batch: 6133 train-loss: 2.2061939239501953\n",
      "[LOG 20200511-10:25:32] epoch: 0, batch: 6134 train-loss: 1.4236476421356201\n",
      "[LOG 20200511-10:25:32] epoch: 0, batch: 6135 train-loss: 1.323586106300354\n",
      "[LOG 20200511-10:25:32] epoch: 0, batch: 6136 train-loss: 1.2105463743209839\n",
      "[LOG 20200511-10:25:32] epoch: 0, batch: 6137 train-loss: 1.008697271347046\n",
      "[LOG 20200511-10:25:33] epoch: 0, batch: 6138 train-loss: 0.9569157361984253\n",
      "[LOG 20200511-10:25:33] epoch: 0, batch: 6139 train-loss: 0.6584240198135376\n",
      "[LOG 20200511-10:25:33] epoch: 0, batch: 6140 train-loss: 1.300191879272461\n",
      "[LOG 20200511-10:25:33] epoch: 0, batch: 6141 train-loss: 1.132275938987732\n",
      "[LOG 20200511-10:25:33] epoch: 0, batch: 6142 train-loss: 1.4361766576766968\n",
      "[LOG 20200511-10:25:33] epoch: 0, batch: 6143 train-loss: 0.9105644822120667\n",
      "[LOG 20200511-10:25:33] epoch: 0, batch: 6144 train-loss: 0.5725008249282837\n",
      "[LOG 20200511-10:25:33] epoch: 0, batch: 6145 train-loss: 2.699517011642456\n",
      "[LOG 20200511-10:25:34] epoch: 0, batch: 6146 train-loss: 1.086020827293396\n",
      "[LOG 20200511-10:25:34] epoch: 0, batch: 6147 train-loss: 1.3129312992095947\n",
      "[LOG 20200511-10:25:34] epoch: 0, batch: 6148 train-loss: 2.537421226501465\n",
      "[LOG 20200511-10:25:34] epoch: 0, batch: 6149 train-loss: 1.1626622676849365\n",
      "[LOG 20200511-10:25:34] epoch: 0, batch: 6150 train-loss: 1.5577536821365356\n",
      "[LOG 20200511-10:25:34] epoch: 0, batch: 6151 train-loss: 1.6196421384811401\n",
      "[LOG 20200511-10:25:34] epoch: 0, batch: 6152 train-loss: 1.4305654764175415\n",
      "[LOG 20200511-10:25:34] epoch: 0, batch: 6153 train-loss: 1.2578428983688354\n",
      "[LOG 20200511-10:25:34] epoch: 0, batch: 6154 train-loss: 1.3863343000411987\n",
      "[LOG 20200511-10:25:35] epoch: 0, batch: 6155 train-loss: 1.5737740993499756\n",
      "[LOG 20200511-10:25:35] epoch: 0, batch: 6156 train-loss: 1.083731770515442\n",
      "[LOG 20200511-10:25:35] epoch: 0, batch: 6157 train-loss: 1.294697880744934\n",
      "[LOG 20200511-10:25:35] epoch: 0, batch: 6158 train-loss: 1.2847976684570312\n",
      "[LOG 20200511-10:25:35] epoch: 0, batch: 6159 train-loss: 1.2275058031082153\n",
      "[LOG 20200511-10:25:35] epoch: 0, batch: 6160 train-loss: 1.6490073204040527\n",
      "[LOG 20200511-10:25:35] epoch: 0, batch: 6161 train-loss: 1.8941278457641602\n",
      "[LOG 20200511-10:25:35] epoch: 0, batch: 6162 train-loss: 1.2187473773956299\n",
      "[LOG 20200511-10:25:36] epoch: 0, batch: 6163 train-loss: 1.2494995594024658\n",
      "[LOG 20200511-10:25:36] epoch: 0, batch: 6164 train-loss: 1.0911535024642944\n",
      "[LOG 20200511-10:25:36] epoch: 0, batch: 6165 train-loss: 0.7440319061279297\n",
      "[LOG 20200511-10:25:36] epoch: 0, batch: 6166 train-loss: 1.3649389743804932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:25:36] epoch: 0, batch: 6167 train-loss: 1.2479795217514038\n",
      "[LOG 20200511-10:25:36] epoch: 0, batch: 6168 train-loss: 1.2466785907745361\n",
      "[LOG 20200511-10:25:36] epoch: 0, batch: 6169 train-loss: 0.9718499779701233\n",
      "[LOG 20200511-10:25:36] epoch: 0, batch: 6170 train-loss: 1.8188550472259521\n",
      "[LOG 20200511-10:25:36] epoch: 0, batch: 6171 train-loss: 0.8642551898956299\n",
      "[LOG 20200511-10:25:37] epoch: 0, batch: 6172 train-loss: 1.086475133895874\n",
      "[LOG 20200511-10:25:37] epoch: 0, batch: 6173 train-loss: 2.311964988708496\n",
      "[LOG 20200511-10:25:37] epoch: 0, batch: 6174 train-loss: 0.716942310333252\n",
      "[LOG 20200511-10:25:37] epoch: 0, batch: 6175 train-loss: 0.7156089544296265\n",
      "[LOG 20200511-10:25:37] epoch: 0, batch: 6176 train-loss: 1.810634970664978\n",
      "[LOG 20200511-10:25:37] epoch: 0, batch: 6177 train-loss: 1.1190744638442993\n",
      "[LOG 20200511-10:25:37] epoch: 0, batch: 6178 train-loss: 0.4789157509803772\n",
      "[LOG 20200511-10:25:37] epoch: 0, batch: 6179 train-loss: 2.2159781455993652\n",
      "[LOG 20200511-10:25:38] epoch: 0, batch: 6180 train-loss: 1.9034974575042725\n",
      "[LOG 20200511-10:25:38] epoch: 0, batch: 6181 train-loss: 0.9519616365432739\n",
      "[LOG 20200511-10:25:38] epoch: 0, batch: 6182 train-loss: 1.6422755718231201\n",
      "[LOG 20200511-10:25:38] epoch: 0, batch: 6183 train-loss: 1.1502405405044556\n",
      "[LOG 20200511-10:25:38] epoch: 0, batch: 6184 train-loss: 0.5556041598320007\n",
      "[LOG 20200511-10:25:38] epoch: 0, batch: 6185 train-loss: 1.2013684511184692\n",
      "[LOG 20200511-10:25:38] epoch: 0, batch: 6186 train-loss: 1.3870081901550293\n",
      "[LOG 20200511-10:25:38] epoch: 0, batch: 6187 train-loss: 0.996087908744812\n",
      "[LOG 20200511-10:25:38] epoch: 0, batch: 6188 train-loss: 1.6021983623504639\n",
      "[LOG 20200511-10:25:39] epoch: 0, batch: 6189 train-loss: 0.7854177355766296\n",
      "[LOG 20200511-10:25:39] epoch: 0, batch: 6190 train-loss: 1.2247339487075806\n",
      "[LOG 20200511-10:25:39] epoch: 0, batch: 6191 train-loss: 0.47080105543136597\n",
      "[LOG 20200511-10:25:39] epoch: 0, batch: 6192 train-loss: 1.174296259880066\n",
      "[LOG 20200511-10:25:39] epoch: 0, batch: 6193 train-loss: 1.2557969093322754\n",
      "[LOG 20200511-10:25:39] epoch: 0, batch: 6194 train-loss: 0.5081847906112671\n",
      "[LOG 20200511-10:25:39] epoch: 0, batch: 6195 train-loss: 0.6764907240867615\n",
      "[LOG 20200511-10:25:39] epoch: 0, batch: 6196 train-loss: 1.170548439025879\n",
      "[LOG 20200511-10:25:40] epoch: 0, batch: 6197 train-loss: 0.5006641745567322\n",
      "[LOG 20200511-10:25:40] epoch: 0, batch: 6198 train-loss: 0.939543604850769\n",
      "[LOG 20200511-10:25:40] epoch: 0, batch: 6199 train-loss: 1.1665959358215332\n",
      "[LOG 20200511-10:25:40] epoch: 0, batch: 6200 train-loss: 1.1276674270629883\n",
      "[LOG 20200511-10:25:40] epoch: 0, batch: 6201 train-loss: 1.7887300252914429\n",
      "[LOG 20200511-10:25:40] epoch: 0, batch: 6202 train-loss: 1.7635384798049927\n",
      "[LOG 20200511-10:25:40] epoch: 0, batch: 6203 train-loss: 1.0563673973083496\n",
      "[LOG 20200511-10:25:40] epoch: 0, batch: 6204 train-loss: 1.2362309694290161\n",
      "[LOG 20200511-10:25:41] epoch: 0, batch: 6205 train-loss: 1.0261106491088867\n",
      "[LOG 20200511-10:25:41] epoch: 0, batch: 6206 train-loss: 1.036475658416748\n",
      "[LOG 20200511-10:25:41] epoch: 0, batch: 6207 train-loss: 1.2497432231903076\n",
      "[LOG 20200511-10:25:41] epoch: 0, batch: 6208 train-loss: 1.437727451324463\n",
      "[LOG 20200511-10:25:41] epoch: 0, batch: 6209 train-loss: 1.0059146881103516\n",
      "[LOG 20200511-10:25:41] epoch: 0, batch: 6210 train-loss: 0.7899723649024963\n",
      "[LOG 20200511-10:25:41] epoch: 0, batch: 6211 train-loss: 0.4436196982860565\n",
      "[LOG 20200511-10:25:41] epoch: 0, batch: 6212 train-loss: 0.9661707878112793\n",
      "[LOG 20200511-10:25:42] epoch: 0, batch: 6213 train-loss: 0.7209736704826355\n",
      "[LOG 20200511-10:25:42] epoch: 0, batch: 6214 train-loss: 1.8680875301361084\n",
      "[LOG 20200511-10:25:42] epoch: 0, batch: 6215 train-loss: 0.5200208425521851\n",
      "[LOG 20200511-10:25:42] epoch: 0, batch: 6216 train-loss: 1.372725009918213\n",
      "[LOG 20200511-10:25:42] epoch: 0, batch: 6217 train-loss: 2.3084545135498047\n",
      "[LOG 20200511-10:25:42] epoch: 0, batch: 6218 train-loss: 1.4543789625167847\n",
      "[LOG 20200511-10:25:42] epoch: 0, batch: 6219 train-loss: 1.0847127437591553\n",
      "[LOG 20200511-10:25:42] epoch: 0, batch: 6220 train-loss: 1.8157191276550293\n",
      "[LOG 20200511-10:25:42] epoch: 0, batch: 6221 train-loss: 1.4346376657485962\n",
      "[LOG 20200511-10:25:43] epoch: 0, batch: 6222 train-loss: 1.2063905000686646\n",
      "[LOG 20200511-10:25:43] epoch: 0, batch: 6223 train-loss: 1.1270426511764526\n",
      "[LOG 20200511-10:25:43] epoch: 0, batch: 6224 train-loss: 1.9036214351654053\n",
      "[LOG 20200511-10:25:43] epoch: 0, batch: 6225 train-loss: 0.7288212776184082\n",
      "[LOG 20200511-10:25:43] epoch: 0, batch: 6226 train-loss: 1.9892585277557373\n",
      "[LOG 20200511-10:25:43] epoch: 0, batch: 6227 train-loss: 1.1584982872009277\n",
      "[LOG 20200511-10:25:43] epoch: 0, batch: 6228 train-loss: 1.2356765270233154\n",
      "[LOG 20200511-10:25:43] epoch: 0, batch: 6229 train-loss: 2.8276920318603516\n",
      "[LOG 20200511-10:25:43] epoch: 0, batch: 6230 train-loss: 0.5906793475151062\n",
      "[LOG 20200511-10:25:44] epoch: 0, batch: 6231 train-loss: 1.4672402143478394\n",
      "[LOG 20200511-10:25:44] epoch: 0, batch: 6232 train-loss: 1.1250886917114258\n",
      "[LOG 20200511-10:25:44] epoch: 0, batch: 6233 train-loss: 1.6662596464157104\n",
      "[LOG 20200511-10:25:44] epoch: 0, batch: 6234 train-loss: 1.09293794631958\n",
      "[LOG 20200511-10:25:44] epoch: 0, batch: 6235 train-loss: 1.4592758417129517\n",
      "[LOG 20200511-10:25:44] epoch: 0, batch: 6236 train-loss: 1.5102972984313965\n",
      "[LOG 20200511-10:25:44] epoch: 0, batch: 6237 train-loss: 2.3482096195220947\n",
      "[LOG 20200511-10:25:44] epoch: 0, batch: 6238 train-loss: 1.282041311264038\n",
      "[LOG 20200511-10:25:45] epoch: 0, batch: 6239 train-loss: 1.3394428491592407\n",
      "[LOG 20200511-10:25:45] epoch: 0, batch: 6240 train-loss: 2.59633207321167\n",
      "[LOG 20200511-10:25:45] epoch: 0, batch: 6241 train-loss: 1.4192020893096924\n",
      "[LOG 20200511-10:25:45] epoch: 0, batch: 6242 train-loss: 1.0022457838058472\n",
      "[LOG 20200511-10:25:45] epoch: 0, batch: 6243 train-loss: 1.1239925622940063\n",
      "[LOG 20200511-10:25:45] epoch: 0, batch: 6244 train-loss: 0.8911519050598145\n",
      "[LOG 20200511-10:25:45] epoch: 0, batch: 6245 train-loss: 0.8083572387695312\n",
      "[LOG 20200511-10:25:45] epoch: 0, batch: 6246 train-loss: 2.921325206756592\n",
      "[LOG 20200511-10:25:46] epoch: 0, batch: 6247 train-loss: 1.469733476638794\n",
      "[LOG 20200511-10:25:46] epoch: 0, batch: 6248 train-loss: 1.261540412902832\n",
      "[LOG 20200511-10:25:46] epoch: 0, batch: 6249 train-loss: 0.788989782333374\n",
      "[LOG 20200511-10:25:46] epoch: 0, batch: 6250 train-loss: 0.8567625284194946\n",
      "[LOG 20200511-10:25:46] epoch: 0, batch: 6251 train-loss: 2.0433199405670166\n",
      "[LOG 20200511-10:25:46] epoch: 0, batch: 6252 train-loss: 1.1895822286605835\n",
      "[LOG 20200511-10:25:46] epoch: 0, batch: 6253 train-loss: 0.8783293962478638\n",
      "[LOG 20200511-10:25:46] epoch: 0, batch: 6254 train-loss: 1.7016723155975342\n",
      "[LOG 20200511-10:25:46] epoch: 0, batch: 6255 train-loss: 1.9880149364471436\n",
      "[LOG 20200511-10:25:46] epoch: 0, batch: 6256 train-loss: 1.4225647449493408\n",
      "[LOG 20200511-10:25:47] epoch: 0, batch: 6257 train-loss: 1.6414875984191895\n",
      "[LOG 20200511-10:25:47] epoch: 0, batch: 6258 train-loss: 1.0208497047424316\n",
      "[LOG 20200511-10:25:47] epoch: 0, batch: 6259 train-loss: 0.7271140217781067\n",
      "[LOG 20200511-10:25:47] epoch: 0, batch: 6260 train-loss: 1.0159987211227417\n",
      "[LOG 20200511-10:25:47] epoch: 0, batch: 6261 train-loss: 0.37434130907058716\n",
      "[LOG 20200511-10:25:47] epoch: 0, batch: 6262 train-loss: 0.7117552161216736\n",
      "[LOG 20200511-10:25:47] epoch: 0, batch: 6263 train-loss: 1.2502102851867676\n",
      "[LOG 20200511-10:25:47] epoch: 0, batch: 6264 train-loss: 1.8880529403686523\n",
      "[LOG 20200511-10:25:47] epoch: 0, batch: 6265 train-loss: 1.5509448051452637\n",
      "[LOG 20200511-10:25:48] epoch: 0, batch: 6266 train-loss: 0.9087339639663696\n",
      "[LOG 20200511-10:25:48] epoch: 0, batch: 6267 train-loss: 2.1091697216033936\n",
      "[LOG 20200511-10:25:48] epoch: 0, batch: 6268 train-loss: 1.1269055604934692\n",
      "[LOG 20200511-10:25:48] epoch: 0, batch: 6269 train-loss: 1.1924105882644653\n",
      "[LOG 20200511-10:25:48] epoch: 0, batch: 6270 train-loss: 0.8469564914703369\n",
      "[LOG 20200511-10:25:48] epoch: 0, batch: 6271 train-loss: 2.156367778778076\n",
      "[LOG 20200511-10:25:48] epoch: 0, batch: 6272 train-loss: 1.2053196430206299\n",
      "[LOG 20200511-10:25:48] epoch: 0, batch: 6273 train-loss: 1.6530280113220215\n",
      "[LOG 20200511-10:25:48] epoch: 0, batch: 6274 train-loss: 1.772976040840149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:25:49] epoch: 0, batch: 6275 train-loss: 1.3469343185424805\n",
      "[LOG 20200511-10:25:49] epoch: 0, batch: 6276 train-loss: 1.0216217041015625\n",
      "[LOG 20200511-10:25:49] epoch: 0, batch: 6277 train-loss: 1.055021047592163\n",
      "[LOG 20200511-10:25:49] epoch: 0, batch: 6278 train-loss: 1.6099708080291748\n",
      "[LOG 20200511-10:25:49] epoch: 0, batch: 6279 train-loss: 1.4554526805877686\n",
      "[LOG 20200511-10:25:49] epoch: 0, batch: 6280 train-loss: 1.1182502508163452\n",
      "[LOG 20200511-10:25:49] epoch: 0, batch: 6281 train-loss: 0.9777923822402954\n",
      "[LOG 20200511-10:25:49] epoch: 0, batch: 6282 train-loss: 1.487642526626587\n",
      "[LOG 20200511-10:25:49] epoch: 0, batch: 6283 train-loss: 1.577789545059204\n",
      "[LOG 20200511-10:25:50] epoch: 0, batch: 6284 train-loss: 1.783210039138794\n",
      "[LOG 20200511-10:25:50] epoch: 0, batch: 6285 train-loss: 1.5613398551940918\n",
      "[LOG 20200511-10:25:50] epoch: 0, batch: 6286 train-loss: 2.138162136077881\n",
      "[LOG 20200511-10:25:50] epoch: 0, batch: 6287 train-loss: 0.36831390857696533\n",
      "[LOG 20200511-10:25:50] epoch: 0, batch: 6288 train-loss: 1.73151695728302\n",
      "[LOG 20200511-10:25:50] epoch: 0, batch: 6289 train-loss: 1.2287991046905518\n",
      "[LOG 20200511-10:25:50] epoch: 0, batch: 6290 train-loss: 1.3825597763061523\n",
      "[LOG 20200511-10:25:50] epoch: 0, batch: 6291 train-loss: 1.81198251247406\n",
      "[LOG 20200511-10:25:50] epoch: 0, batch: 6292 train-loss: 1.5264737606048584\n",
      "[LOG 20200511-10:25:51] epoch: 0, batch: 6293 train-loss: 1.848228931427002\n",
      "[LOG 20200511-10:25:51] epoch: 0, batch: 6294 train-loss: 1.315359354019165\n",
      "[LOG 20200511-10:25:51] epoch: 0, batch: 6295 train-loss: 1.6701531410217285\n",
      "[LOG 20200511-10:25:51] epoch: 0, batch: 6296 train-loss: 0.7323068976402283\n",
      "[LOG 20200511-10:25:51] epoch: 0, batch: 6297 train-loss: 1.4005976915359497\n",
      "[LOG 20200511-10:25:51] epoch: 0, batch: 6298 train-loss: 1.2092599868774414\n",
      "[LOG 20200511-10:25:51] epoch: 0, batch: 6299 train-loss: 1.6258699893951416\n",
      "[LOG 20200511-10:25:51] epoch: 0, batch: 6300 train-loss: 1.3760801553726196\n",
      "[LOG 20200511-10:25:51] epoch: 0, batch: 6301 train-loss: 2.1997556686401367\n",
      "[LOG 20200511-10:25:52] epoch: 0, batch: 6302 train-loss: 1.9607268571853638\n",
      "[LOG 20200511-10:25:52] epoch: 0, batch: 6303 train-loss: 1.1058017015457153\n",
      "[LOG 20200511-10:25:52] epoch: 0, batch: 6304 train-loss: 0.9324591159820557\n",
      "[LOG 20200511-10:25:52] epoch: 0, batch: 6305 train-loss: 2.064317464828491\n",
      "[LOG 20200511-10:25:52] epoch: 0, batch: 6306 train-loss: 1.2600507736206055\n",
      "[LOG 20200511-10:25:52] epoch: 0, batch: 6307 train-loss: 1.169822335243225\n",
      "[LOG 20200511-10:25:52] epoch: 0, batch: 6308 train-loss: 0.6446192860603333\n",
      "[LOG 20200511-10:25:52] epoch: 0, batch: 6309 train-loss: 0.6824779510498047\n",
      "[LOG 20200511-10:25:52] epoch: 0, batch: 6310 train-loss: 0.7708570957183838\n",
      "[LOG 20200511-10:25:53] epoch: 0, batch: 6311 train-loss: 1.0483523607254028\n",
      "[LOG 20200511-10:25:53] epoch: 0, batch: 6312 train-loss: 1.1656090021133423\n",
      "[LOG 20200511-10:25:53] epoch: 0, batch: 6313 train-loss: 1.4820600748062134\n",
      "[LOG 20200511-10:25:53] epoch: 0, batch: 6314 train-loss: 1.2314904928207397\n",
      "[LOG 20200511-10:25:53] epoch: 0, batch: 6315 train-loss: 1.542336106300354\n",
      "[LOG 20200511-10:25:53] epoch: 0, batch: 6316 train-loss: 0.8252462148666382\n",
      "[LOG 20200511-10:25:53] epoch: 0, batch: 6317 train-loss: 1.2258474826812744\n",
      "[LOG 20200511-10:25:53] epoch: 0, batch: 6318 train-loss: 1.0689681768417358\n",
      "[LOG 20200511-10:25:53] epoch: 0, batch: 6319 train-loss: 0.39577165246009827\n",
      "[LOG 20200511-10:25:54] epoch: 0, batch: 6320 train-loss: 2.0587410926818848\n",
      "[LOG 20200511-10:25:54] epoch: 0, batch: 6321 train-loss: 1.4461874961853027\n",
      "[LOG 20200511-10:25:54] epoch: 0, batch: 6322 train-loss: 0.5624598264694214\n",
      "[LOG 20200511-10:25:54] epoch: 0, batch: 6323 train-loss: 1.2829480171203613\n",
      "[LOG 20200511-10:25:54] epoch: 0, batch: 6324 train-loss: 1.2267231941223145\n",
      "[LOG 20200511-10:25:54] epoch: 0, batch: 6325 train-loss: 2.6093413829803467\n",
      "[LOG 20200511-10:25:54] epoch: 0, batch: 6326 train-loss: 0.1502177119255066\n",
      "[LOG 20200511-10:25:54] epoch: 0, batch: 6327 train-loss: 1.913921594619751\n",
      "[LOG 20200511-10:25:54] epoch: 0, batch: 6328 train-loss: 0.8745465874671936\n",
      "[LOG 20200511-10:25:55] epoch: 0, batch: 6329 train-loss: 1.4582598209381104\n",
      "[LOG 20200511-10:25:55] epoch: 0, batch: 6330 train-loss: 1.0636708736419678\n",
      "[LOG 20200511-10:25:55] epoch: 0, batch: 6331 train-loss: 1.201562523841858\n",
      "[LOG 20200511-10:25:55] epoch: 0, batch: 6332 train-loss: 0.19664746522903442\n",
      "[LOG 20200511-10:25:55] epoch: 0, batch: 6333 train-loss: 1.0734807252883911\n",
      "[LOG 20200511-10:25:55] epoch: 0, batch: 6334 train-loss: 1.106485366821289\n",
      "[LOG 20200511-10:25:55] epoch: 0, batch: 6335 train-loss: 2.5115966796875\n",
      "[LOG 20200511-10:25:55] epoch: 0, batch: 6336 train-loss: 1.466111421585083\n",
      "[LOG 20200511-10:25:55] epoch: 0, batch: 6337 train-loss: 1.6268739700317383\n",
      "[LOG 20200511-10:25:56] epoch: 0, batch: 6338 train-loss: 0.7454608082771301\n",
      "[LOG 20200511-10:25:56] epoch: 0, batch: 6339 train-loss: 1.9524118900299072\n",
      "[LOG 20200511-10:25:56] epoch: 0, batch: 6340 train-loss: 1.2210724353790283\n",
      "[LOG 20200511-10:25:56] epoch: 0, batch: 6341 train-loss: 0.9848838448524475\n",
      "[LOG 20200511-10:25:56] epoch: 0, batch: 6342 train-loss: 2.0522572994232178\n",
      "[LOG 20200511-10:25:56] epoch: 0, batch: 6343 train-loss: 0.9558608531951904\n",
      "[LOG 20200511-10:25:56] epoch: 0, batch: 6344 train-loss: 0.8574773073196411\n",
      "[LOG 20200511-10:25:56] epoch: 0, batch: 6345 train-loss: 1.3795208930969238\n",
      "[LOG 20200511-10:25:56] epoch: 0, batch: 6346 train-loss: 0.5431063771247864\n",
      "[LOG 20200511-10:25:57] epoch: 0, batch: 6347 train-loss: 0.8632341623306274\n",
      "[LOG 20200511-10:25:57] epoch: 0, batch: 6348 train-loss: 1.7128316164016724\n",
      "[LOG 20200511-10:25:57] epoch: 0, batch: 6349 train-loss: 1.088197946548462\n",
      "[LOG 20200511-10:25:57] epoch: 0, batch: 6350 train-loss: 0.7932940125465393\n",
      "[LOG 20200511-10:25:57] epoch: 0, batch: 6351 train-loss: 0.6039906144142151\n",
      "[LOG 20200511-10:25:57] epoch: 0, batch: 6352 train-loss: 0.6115545034408569\n",
      "[LOG 20200511-10:25:57] epoch: 0, batch: 6353 train-loss: 1.799530029296875\n",
      "[LOG 20200511-10:25:57] epoch: 0, batch: 6354 train-loss: 1.1719038486480713\n",
      "[LOG 20200511-10:25:57] epoch: 0, batch: 6355 train-loss: 1.5295741558074951\n",
      "[LOG 20200511-10:25:58] epoch: 0, batch: 6356 train-loss: 1.0682435035705566\n",
      "[LOG 20200511-10:25:58] epoch: 0, batch: 6357 train-loss: 1.32001531124115\n",
      "[LOG 20200511-10:25:58] epoch: 0, batch: 6358 train-loss: 1.2117211818695068\n",
      "[LOG 20200511-10:25:58] epoch: 0, batch: 6359 train-loss: 1.3685762882232666\n",
      "[LOG 20200511-10:25:58] epoch: 0, batch: 6360 train-loss: 0.6472698450088501\n",
      "[LOG 20200511-10:25:58] epoch: 0, batch: 6361 train-loss: 1.6529606580734253\n",
      "[LOG 20200511-10:25:58] epoch: 0, batch: 6362 train-loss: 1.0735061168670654\n",
      "[LOG 20200511-10:25:58] epoch: 0, batch: 6363 train-loss: 1.1276875734329224\n",
      "[LOG 20200511-10:25:59] epoch: 0, batch: 6364 train-loss: 1.9808095693588257\n",
      "[LOG 20200511-10:25:59] epoch: 0, batch: 6365 train-loss: 1.626143455505371\n",
      "[LOG 20200511-10:25:59] epoch: 0, batch: 6366 train-loss: 3.214291572570801\n",
      "[LOG 20200511-10:25:59] epoch: 0, batch: 6367 train-loss: 1.1505850553512573\n",
      "[LOG 20200511-10:25:59] epoch: 0, batch: 6368 train-loss: 1.0989187955856323\n",
      "[LOG 20200511-10:25:59] epoch: 0, batch: 6369 train-loss: 1.8751500844955444\n",
      "[LOG 20200511-10:25:59] epoch: 0, batch: 6370 train-loss: 1.4371063709259033\n",
      "[LOG 20200511-10:25:59] epoch: 0, batch: 6371 train-loss: 1.2837837934494019\n",
      "[LOG 20200511-10:25:59] epoch: 0, batch: 6372 train-loss: 1.0774896144866943\n",
      "[LOG 20200511-10:26:00] epoch: 0, batch: 6373 train-loss: 0.5849292278289795\n",
      "[LOG 20200511-10:26:00] epoch: 0, batch: 6374 train-loss: 0.8840509653091431\n",
      "[LOG 20200511-10:26:00] epoch: 0, batch: 6375 train-loss: 1.1384897232055664\n",
      "[LOG 20200511-10:26:00] epoch: 0, batch: 6376 train-loss: 1.3111639022827148\n",
      "[LOG 20200511-10:26:00] epoch: 0, batch: 6377 train-loss: 2.434516429901123\n",
      "[LOG 20200511-10:26:00] epoch: 0, batch: 6378 train-loss: 0.8948928117752075\n",
      "[LOG 20200511-10:26:00] epoch: 0, batch: 6379 train-loss: 0.6495645642280579\n",
      "[LOG 20200511-10:26:00] epoch: 0, batch: 6380 train-loss: 1.5034486055374146\n",
      "[LOG 20200511-10:26:00] epoch: 0, batch: 6381 train-loss: 1.2576395273208618\n",
      "[LOG 20200511-10:26:01] epoch: 0, batch: 6382 train-loss: 0.8528965711593628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:26:01] epoch: 0, batch: 6383 train-loss: 0.6219553351402283\n",
      "[LOG 20200511-10:26:01] epoch: 0, batch: 6384 train-loss: 0.9241639971733093\n",
      "[LOG 20200511-10:26:01] epoch: 0, batch: 6385 train-loss: 1.503929853439331\n",
      "[LOG 20200511-10:26:01] epoch: 0, batch: 6386 train-loss: 0.984513521194458\n",
      "[LOG 20200511-10:26:01] epoch: 0, batch: 6387 train-loss: 2.978816270828247\n",
      "[LOG 20200511-10:26:01] epoch: 0, batch: 6388 train-loss: 1.6028674840927124\n",
      "[LOG 20200511-10:26:01] epoch: 0, batch: 6389 train-loss: 1.4001970291137695\n",
      "[LOG 20200511-10:26:02] epoch: 0, batch: 6390 train-loss: 1.2333531379699707\n",
      "[LOG 20200511-10:26:02] epoch: 0, batch: 6391 train-loss: 0.9027583599090576\n",
      "[LOG 20200511-10:26:02] epoch: 0, batch: 6392 train-loss: 2.1557552814483643\n",
      "[LOG 20200511-10:26:02] epoch: 0, batch: 6393 train-loss: 1.0296611785888672\n",
      "[LOG 20200511-10:26:02] epoch: 0, batch: 6394 train-loss: 1.3438301086425781\n",
      "[LOG 20200511-10:26:02] epoch: 0, batch: 6395 train-loss: 1.3550359010696411\n",
      "[LOG 20200511-10:26:02] epoch: 0, batch: 6396 train-loss: 1.4764533042907715\n",
      "[LOG 20200511-10:26:02] epoch: 0, batch: 6397 train-loss: 2.743690013885498\n",
      "[LOG 20200511-10:26:03] epoch: 0, batch: 6398 train-loss: 1.1150257587432861\n",
      "[LOG 20200511-10:26:03] epoch: 0, batch: 6399 train-loss: 1.6837108135223389\n",
      "[LOG 20200511-10:26:03] epoch: 0, batch: 6400 train-loss: 1.3054444789886475\n",
      "[LOG 20200511-10:26:03] epoch: 0, batch: 6401 train-loss: 1.6584219932556152\n",
      "[LOG 20200511-10:26:03] epoch: 0, batch: 6402 train-loss: 1.2908544540405273\n",
      "[LOG 20200511-10:26:03] epoch: 0, batch: 6403 train-loss: 0.9177976846694946\n",
      "[LOG 20200511-10:26:03] epoch: 0, batch: 6404 train-loss: 0.4767322242259979\n",
      "[LOG 20200511-10:26:03] epoch: 0, batch: 6405 train-loss: 1.1626087427139282\n",
      "[LOG 20200511-10:26:03] epoch: 0, batch: 6406 train-loss: 1.5117075443267822\n",
      "[LOG 20200511-10:26:03] epoch: 0, batch: 6407 train-loss: 1.3567262887954712\n",
      "[LOG 20200511-10:26:04] epoch: 0, batch: 6408 train-loss: 0.5138525366783142\n",
      "[LOG 20200511-10:26:04] epoch: 0, batch: 6409 train-loss: 1.0281842947006226\n",
      "[LOG 20200511-10:26:04] epoch: 0, batch: 6410 train-loss: 1.9962279796600342\n",
      "[LOG 20200511-10:26:04] epoch: 0, batch: 6411 train-loss: 1.8045536279678345\n",
      "[LOG 20200511-10:26:04] epoch: 0, batch: 6412 train-loss: 1.105414867401123\n",
      "[LOG 20200511-10:26:04] epoch: 0, batch: 6413 train-loss: 1.1477348804473877\n",
      "[LOG 20200511-10:26:04] epoch: 0, batch: 6414 train-loss: 0.7018943428993225\n",
      "[LOG 20200511-10:26:04] epoch: 0, batch: 6415 train-loss: 1.3770240545272827\n",
      "[LOG 20200511-10:26:04] epoch: 0, batch: 6416 train-loss: 1.013964056968689\n",
      "[LOG 20200511-10:26:05] epoch: 0, batch: 6417 train-loss: 1.4707192182540894\n",
      "[LOG 20200511-10:26:05] epoch: 0, batch: 6418 train-loss: 1.0886762142181396\n",
      "[LOG 20200511-10:26:05] epoch: 0, batch: 6419 train-loss: 0.9244446754455566\n",
      "[LOG 20200511-10:26:05] epoch: 0, batch: 6420 train-loss: 2.075968027114868\n",
      "[LOG 20200511-10:26:05] epoch: 0, batch: 6421 train-loss: 1.7235838174819946\n",
      "[LOG 20200511-10:26:05] epoch: 0, batch: 6422 train-loss: 1.0118712186813354\n",
      "[LOG 20200511-10:26:05] epoch: 0, batch: 6423 train-loss: 1.2679210901260376\n",
      "[LOG 20200511-10:26:05] epoch: 0, batch: 6424 train-loss: 1.2206759452819824\n",
      "[LOG 20200511-10:26:05] epoch: 0, batch: 6425 train-loss: 0.9456385374069214\n",
      "[LOG 20200511-10:26:06] epoch: 0, batch: 6426 train-loss: 1.0738232135772705\n",
      "[LOG 20200511-10:26:06] epoch: 0, batch: 6427 train-loss: 1.110331654548645\n",
      "[LOG 20200511-10:26:06] epoch: 0, batch: 6428 train-loss: 1.7420909404754639\n",
      "[LOG 20200511-10:26:06] epoch: 0, batch: 6429 train-loss: 1.3924778699874878\n",
      "[LOG 20200511-10:26:06] epoch: 0, batch: 6430 train-loss: 1.0146666765213013\n",
      "[LOG 20200511-10:26:06] epoch: 0, batch: 6431 train-loss: 1.8359006643295288\n",
      "[LOG 20200511-10:26:06] epoch: 0, batch: 6432 train-loss: 1.4147350788116455\n",
      "[LOG 20200511-10:26:06] epoch: 0, batch: 6433 train-loss: 0.5953801870346069\n",
      "[LOG 20200511-10:26:06] epoch: 0, batch: 6434 train-loss: 0.9887850880622864\n",
      "[LOG 20200511-10:26:07] epoch: 0, batch: 6435 train-loss: 0.8577173352241516\n",
      "[LOG 20200511-10:26:07] epoch: 0, batch: 6436 train-loss: 1.3021141290664673\n",
      "[LOG 20200511-10:26:07] epoch: 0, batch: 6437 train-loss: 1.2382478713989258\n",
      "[LOG 20200511-10:26:07] epoch: 0, batch: 6438 train-loss: 2.0930845737457275\n",
      "[LOG 20200511-10:26:07] epoch: 0, batch: 6439 train-loss: 1.3149787187576294\n",
      "[LOG 20200511-10:26:07] epoch: 0, batch: 6440 train-loss: 1.9846789836883545\n",
      "[LOG 20200511-10:26:07] epoch: 0, batch: 6441 train-loss: 1.156341791152954\n",
      "[LOG 20200511-10:26:07] epoch: 0, batch: 6442 train-loss: 0.4055919945240021\n",
      "[LOG 20200511-10:26:07] epoch: 0, batch: 6443 train-loss: 1.1563894748687744\n",
      "[LOG 20200511-10:26:08] epoch: 0, batch: 6444 train-loss: 0.8889342546463013\n",
      "[LOG 20200511-10:26:08] epoch: 0, batch: 6445 train-loss: 1.2701847553253174\n",
      "[LOG 20200511-10:26:08] epoch: 0, batch: 6446 train-loss: 0.5296289324760437\n",
      "[LOG 20200511-10:26:08] epoch: 0, batch: 6447 train-loss: 1.0677649974822998\n",
      "[LOG 20200511-10:26:08] epoch: 0, batch: 6448 train-loss: 0.5743182897567749\n",
      "[LOG 20200511-10:26:08] epoch: 0, batch: 6449 train-loss: 1.1778600215911865\n",
      "[LOG 20200511-10:26:08] epoch: 0, batch: 6450 train-loss: 1.3702222108840942\n",
      "[LOG 20200511-10:26:08] epoch: 0, batch: 6451 train-loss: 2.0032429695129395\n",
      "[LOG 20200511-10:26:09] epoch: 0, batch: 6452 train-loss: 0.6586830019950867\n",
      "[LOG 20200511-10:26:09] epoch: 0, batch: 6453 train-loss: 1.105805516242981\n",
      "[LOG 20200511-10:26:09] epoch: 0, batch: 6454 train-loss: 1.0336968898773193\n",
      "[LOG 20200511-10:26:09] epoch: 0, batch: 6455 train-loss: 1.2022064924240112\n",
      "[LOG 20200511-10:26:09] epoch: 0, batch: 6456 train-loss: 1.3512725830078125\n",
      "[LOG 20200511-10:26:09] epoch: 0, batch: 6457 train-loss: 1.2578649520874023\n",
      "[LOG 20200511-10:26:09] epoch: 0, batch: 6458 train-loss: 1.0773835182189941\n",
      "[LOG 20200511-10:26:09] epoch: 0, batch: 6459 train-loss: 0.607545793056488\n",
      "[LOG 20200511-10:26:09] epoch: 0, batch: 6460 train-loss: 1.3731749057769775\n",
      "[LOG 20200511-10:26:10] epoch: 0, batch: 6461 train-loss: 1.5958013534545898\n",
      "[LOG 20200511-10:26:10] epoch: 0, batch: 6462 train-loss: 1.2273807525634766\n",
      "[LOG 20200511-10:26:10] epoch: 0, batch: 6463 train-loss: 1.7454242706298828\n",
      "[LOG 20200511-10:26:10] epoch: 0, batch: 6464 train-loss: 0.4008302092552185\n",
      "[LOG 20200511-10:26:10] epoch: 0, batch: 6465 train-loss: 0.6541821956634521\n",
      "[LOG 20200511-10:26:10] epoch: 0, batch: 6466 train-loss: 2.239441394805908\n",
      "[LOG 20200511-10:26:10] epoch: 0, batch: 6467 train-loss: 1.1227903366088867\n",
      "[LOG 20200511-10:26:10] epoch: 0, batch: 6468 train-loss: 2.5746684074401855\n",
      "[LOG 20200511-10:26:10] epoch: 0, batch: 6469 train-loss: 2.181551694869995\n",
      "[LOG 20200511-10:26:11] epoch: 0, batch: 6470 train-loss: 0.6779909133911133\n",
      "[LOG 20200511-10:26:11] epoch: 0, batch: 6471 train-loss: 1.1590625047683716\n",
      "[LOG 20200511-10:26:11] epoch: 0, batch: 6472 train-loss: 0.6959894895553589\n",
      "[LOG 20200511-10:26:11] epoch: 0, batch: 6473 train-loss: 0.7141333818435669\n",
      "[LOG 20200511-10:26:11] epoch: 0, batch: 6474 train-loss: 0.9165035486221313\n",
      "[LOG 20200511-10:26:11] epoch: 0, batch: 6475 train-loss: 0.7296963930130005\n",
      "[LOG 20200511-10:26:11] epoch: 0, batch: 6476 train-loss: 0.9499996900558472\n",
      "[LOG 20200511-10:26:11] epoch: 0, batch: 6477 train-loss: 0.5057330131530762\n",
      "[LOG 20200511-10:26:11] epoch: 0, batch: 6478 train-loss: 0.9245570302009583\n",
      "[LOG 20200511-10:26:12] epoch: 0, batch: 6479 train-loss: 0.7249643802642822\n",
      "[LOG 20200511-10:26:12] epoch: 0, batch: 6480 train-loss: 1.5042471885681152\n",
      "[LOG 20200511-10:26:12] epoch: 0, batch: 6481 train-loss: 1.3348801136016846\n",
      "[LOG 20200511-10:26:12] epoch: 0, batch: 6482 train-loss: 1.6809306144714355\n",
      "[LOG 20200511-10:26:12] epoch: 0, batch: 6483 train-loss: 1.2441154718399048\n",
      "[LOG 20200511-10:26:12] epoch: 0, batch: 6484 train-loss: 2.063528537750244\n",
      "[LOG 20200511-10:26:12] epoch: 0, batch: 6485 train-loss: 0.6871704459190369\n",
      "[LOG 20200511-10:26:12] epoch: 0, batch: 6486 train-loss: 1.0938589572906494\n",
      "[LOG 20200511-10:26:12] epoch: 0, batch: 6487 train-loss: 1.0997776985168457\n",
      "[LOG 20200511-10:26:13] epoch: 0, batch: 6488 train-loss: 1.8133552074432373\n",
      "[LOG 20200511-10:26:13] epoch: 0, batch: 6489 train-loss: 0.8946229219436646\n",
      "[LOG 20200511-10:26:13] epoch: 0, batch: 6490 train-loss: 1.4614819288253784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:26:13] epoch: 0, batch: 6491 train-loss: 1.577711582183838\n",
      "[LOG 20200511-10:26:13] epoch: 0, batch: 6492 train-loss: 0.6963527202606201\n",
      "[LOG 20200511-10:26:13] epoch: 0, batch: 6493 train-loss: 0.733810544013977\n",
      "[LOG 20200511-10:26:13] epoch: 0, batch: 6494 train-loss: 0.9155448079109192\n",
      "[LOG 20200511-10:26:13] epoch: 0, batch: 6495 train-loss: 1.2614364624023438\n",
      "[LOG 20200511-10:26:14] epoch: 0, batch: 6496 train-loss: 1.078436017036438\n",
      "[LOG 20200511-10:26:14] epoch: 0, batch: 6497 train-loss: 2.316014528274536\n",
      "[LOG 20200511-10:26:14] epoch: 0, batch: 6498 train-loss: 0.9298325181007385\n",
      "[LOG 20200511-10:26:14] epoch: 0, batch: 6499 train-loss: 0.7809773683547974\n",
      "[LOG 20200511-10:26:14] epoch: 0, batch: 6500 train-loss: 2.2565231323242188\n",
      "[LOG 20200511-10:26:14] epoch: 0, batch: 6501 train-loss: 1.456931471824646\n",
      "[LOG 20200511-10:26:14] epoch: 0, batch: 6502 train-loss: 1.4034205675125122\n",
      "[LOG 20200511-10:26:14] epoch: 0, batch: 6503 train-loss: 1.963130235671997\n",
      "[LOG 20200511-10:26:14] epoch: 0, batch: 6504 train-loss: 1.2603577375411987\n",
      "[LOG 20200511-10:26:15] epoch: 0, batch: 6505 train-loss: 0.5450075268745422\n",
      "[LOG 20200511-10:26:15] epoch: 0, batch: 6506 train-loss: 1.102599024772644\n",
      "[LOG 20200511-10:26:15] epoch: 0, batch: 6507 train-loss: 1.1616098880767822\n",
      "[LOG 20200511-10:26:15] epoch: 0, batch: 6508 train-loss: 1.2165751457214355\n",
      "[LOG 20200511-10:26:15] epoch: 0, batch: 6509 train-loss: 0.8328392505645752\n",
      "[LOG 20200511-10:26:15] epoch: 0, batch: 6510 train-loss: 2.1475162506103516\n",
      "[LOG 20200511-10:26:15] epoch: 0, batch: 6511 train-loss: 1.6892573833465576\n",
      "[LOG 20200511-10:26:15] epoch: 0, batch: 6512 train-loss: 1.6667349338531494\n",
      "[LOG 20200511-10:26:15] epoch: 0, batch: 6513 train-loss: 1.6901214122772217\n",
      "[LOG 20200511-10:26:16] epoch: 0, batch: 6514 train-loss: 1.4913921356201172\n",
      "[LOG 20200511-10:26:16] epoch: 0, batch: 6515 train-loss: 1.0076674222946167\n",
      "[LOG 20200511-10:26:16] epoch: 0, batch: 6516 train-loss: 0.6893187761306763\n",
      "[LOG 20200511-10:26:16] epoch: 0, batch: 6517 train-loss: 1.1873009204864502\n",
      "[LOG 20200511-10:26:16] epoch: 0, batch: 6518 train-loss: 1.4810703992843628\n",
      "[LOG 20200511-10:26:16] epoch: 0, batch: 6519 train-loss: 0.8363105654716492\n",
      "[LOG 20200511-10:26:16] epoch: 0, batch: 6520 train-loss: 1.8805665969848633\n",
      "[LOG 20200511-10:26:16] epoch: 0, batch: 6521 train-loss: 1.6211111545562744\n",
      "[LOG 20200511-10:26:17] epoch: 0, batch: 6522 train-loss: 1.2089440822601318\n",
      "[LOG 20200511-10:26:17] epoch: 0, batch: 6523 train-loss: 0.8777751922607422\n",
      "[LOG 20200511-10:26:17] epoch: 0, batch: 6524 train-loss: 2.0001165866851807\n",
      "[LOG 20200511-10:26:17] epoch: 0, batch: 6525 train-loss: 1.1797857284545898\n",
      "[LOG 20200511-10:26:17] epoch: 0, batch: 6526 train-loss: 1.0468711853027344\n",
      "[LOG 20200511-10:26:17] epoch: 0, batch: 6527 train-loss: 1.4322513341903687\n",
      "[LOG 20200511-10:26:17] epoch: 0, batch: 6528 train-loss: 0.767929196357727\n",
      "[LOG 20200511-10:26:17] epoch: 0, batch: 6529 train-loss: 0.8906903862953186\n",
      "[LOG 20200511-10:26:18] epoch: 0, batch: 6530 train-loss: 1.1591672897338867\n",
      "[LOG 20200511-10:26:18] epoch: 0, batch: 6531 train-loss: 1.6153886318206787\n",
      "[LOG 20200511-10:26:18] epoch: 0, batch: 6532 train-loss: 2.0283052921295166\n",
      "[LOG 20200511-10:26:18] epoch: 0, batch: 6533 train-loss: 1.5317318439483643\n",
      "[LOG 20200511-10:26:18] epoch: 0, batch: 6534 train-loss: 0.48429951071739197\n",
      "[LOG 20200511-10:26:18] epoch: 0, batch: 6535 train-loss: 0.43071094155311584\n",
      "[LOG 20200511-10:26:18] epoch: 0, batch: 6536 train-loss: 1.0734922885894775\n",
      "[LOG 20200511-10:26:18] epoch: 0, batch: 6537 train-loss: 1.4498823881149292\n",
      "[LOG 20200511-10:26:18] epoch: 0, batch: 6538 train-loss: 2.4192123413085938\n",
      "[LOG 20200511-10:26:19] epoch: 0, batch: 6539 train-loss: 0.9651892781257629\n",
      "[LOG 20200511-10:26:19] epoch: 0, batch: 6540 train-loss: 1.3360857963562012\n",
      "[LOG 20200511-10:26:19] epoch: 0, batch: 6541 train-loss: 1.416652798652649\n",
      "[LOG 20200511-10:26:19] epoch: 0, batch: 6542 train-loss: 0.8914200663566589\n",
      "[LOG 20200511-10:26:19] epoch: 0, batch: 6543 train-loss: 2.446627616882324\n",
      "[LOG 20200511-10:26:19] epoch: 0, batch: 6544 train-loss: 1.3831064701080322\n",
      "[LOG 20200511-10:26:19] epoch: 0, batch: 6545 train-loss: 0.9806009531021118\n",
      "[LOG 20200511-10:26:19] epoch: 0, batch: 6546 train-loss: 2.2714157104492188\n",
      "[LOG 20200511-10:26:19] epoch: 0, batch: 6547 train-loss: 0.50108402967453\n",
      "[LOG 20200511-10:26:20] epoch: 0, batch: 6548 train-loss: 0.6160716414451599\n",
      "[LOG 20200511-10:26:20] epoch: 0, batch: 6549 train-loss: 0.9817932844161987\n",
      "[LOG 20200511-10:26:20] epoch: 0, batch: 6550 train-loss: 3.050581455230713\n",
      "[LOG 20200511-10:26:20] epoch: 0, batch: 6551 train-loss: 1.372326135635376\n",
      "[LOG 20200511-10:26:20] epoch: 0, batch: 6552 train-loss: 0.9684814214706421\n",
      "[LOG 20200511-10:26:20] epoch: 0, batch: 6553 train-loss: 1.7713080644607544\n",
      "[LOG 20200511-10:26:20] epoch: 0, batch: 6554 train-loss: 0.9321051239967346\n",
      "[LOG 20200511-10:26:20] epoch: 0, batch: 6555 train-loss: 0.7080362439155579\n",
      "[LOG 20200511-10:26:21] epoch: 0, batch: 6556 train-loss: 1.5333120822906494\n",
      "[LOG 20200511-10:26:21] epoch: 0, batch: 6557 train-loss: 1.2798144817352295\n",
      "[LOG 20200511-10:26:21] epoch: 0, batch: 6558 train-loss: 0.5288527607917786\n",
      "[LOG 20200511-10:26:21] epoch: 0, batch: 6559 train-loss: 1.7521531581878662\n",
      "[LOG 20200511-10:26:21] epoch: 0, batch: 6560 train-loss: 0.8138721585273743\n",
      "[LOG 20200511-10:26:21] epoch: 0, batch: 6561 train-loss: 1.2577202320098877\n",
      "[LOG 20200511-10:26:21] epoch: 0, batch: 6562 train-loss: 1.5970131158828735\n",
      "[LOG 20200511-10:26:21] epoch: 0, batch: 6563 train-loss: 1.3672068119049072\n",
      "[LOG 20200511-10:26:21] epoch: 0, batch: 6564 train-loss: 1.340541958808899\n",
      "[LOG 20200511-10:26:22] epoch: 0, batch: 6565 train-loss: 1.6929903030395508\n",
      "[LOG 20200511-10:26:22] epoch: 0, batch: 6566 train-loss: 0.9100020527839661\n",
      "[LOG 20200511-10:26:22] epoch: 0, batch: 6567 train-loss: 0.6894909143447876\n",
      "[LOG 20200511-10:26:22] epoch: 0, batch: 6568 train-loss: 1.809674620628357\n",
      "[LOG 20200511-10:26:22] epoch: 0, batch: 6569 train-loss: 0.707465648651123\n",
      "[LOG 20200511-10:26:22] epoch: 0, batch: 6570 train-loss: 1.3625919818878174\n",
      "[LOG 20200511-10:26:22] epoch: 0, batch: 6571 train-loss: 1.2398781776428223\n",
      "[LOG 20200511-10:26:22] epoch: 0, batch: 6572 train-loss: 1.2966339588165283\n",
      "[LOG 20200511-10:26:22] epoch: 0, batch: 6573 train-loss: 1.920877456665039\n",
      "[LOG 20200511-10:26:23] epoch: 0, batch: 6574 train-loss: 1.064988374710083\n",
      "[LOG 20200511-10:26:23] epoch: 0, batch: 6575 train-loss: 0.6589187383651733\n",
      "[LOG 20200511-10:26:23] epoch: 0, batch: 6576 train-loss: 0.7662444710731506\n",
      "[LOG 20200511-10:26:23] epoch: 0, batch: 6577 train-loss: 0.8518332839012146\n",
      "[LOG 20200511-10:26:23] epoch: 0, batch: 6578 train-loss: 1.1207414865493774\n",
      "[LOG 20200511-10:26:23] epoch: 0, batch: 6579 train-loss: 1.5303531885147095\n",
      "[LOG 20200511-10:26:23] epoch: 0, batch: 6580 train-loss: 1.2407233715057373\n",
      "[LOG 20200511-10:26:23] epoch: 0, batch: 6581 train-loss: 1.5562900304794312\n",
      "[LOG 20200511-10:26:23] epoch: 0, batch: 6582 train-loss: 1.8415579795837402\n",
      "[LOG 20200511-10:26:24] epoch: 0, batch: 6583 train-loss: 1.2462977170944214\n",
      "[LOG 20200511-10:26:24] epoch: 0, batch: 6584 train-loss: 1.042693018913269\n",
      "[LOG 20200511-10:26:24] epoch: 0, batch: 6585 train-loss: 1.9299900531768799\n",
      "[LOG 20200511-10:26:24] epoch: 0, batch: 6586 train-loss: 0.7236512303352356\n",
      "[LOG 20200511-10:26:24] epoch: 0, batch: 6587 train-loss: 1.883495807647705\n",
      "[LOG 20200511-10:26:24] epoch: 0, batch: 6588 train-loss: 1.5831941366195679\n",
      "[LOG 20200511-10:26:24] epoch: 0, batch: 6589 train-loss: 2.3093316555023193\n",
      "[LOG 20200511-10:26:24] epoch: 0, batch: 6590 train-loss: 0.8767884969711304\n",
      "[LOG 20200511-10:26:24] epoch: 0, batch: 6591 train-loss: 0.6448258757591248\n",
      "[LOG 20200511-10:26:24] epoch: 0, batch: 6592 train-loss: 1.2000925540924072\n",
      "[LOG 20200511-10:26:25] epoch: 0, batch: 6593 train-loss: 0.9115650653839111\n",
      "[LOG 20200511-10:26:25] epoch: 0, batch: 6594 train-loss: 0.761527955532074\n",
      "[LOG 20200511-10:26:25] epoch: 0, batch: 6595 train-loss: 0.7152537107467651\n",
      "[LOG 20200511-10:26:25] epoch: 0, batch: 6596 train-loss: 1.4363101720809937\n",
      "[LOG 20200511-10:26:25] epoch: 0, batch: 6597 train-loss: 0.9439665675163269\n",
      "[LOG 20200511-10:26:25] epoch: 0, batch: 6598 train-loss: 0.773279070854187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:26:25] epoch: 0, batch: 6599 train-loss: 0.990113377571106\n",
      "[LOG 20200511-10:26:25] epoch: 0, batch: 6600 train-loss: 0.9719623327255249\n",
      "[LOG 20200511-10:26:25] epoch: 0, batch: 6601 train-loss: 1.2135000228881836\n",
      "[LOG 20200511-10:26:26] epoch: 0, batch: 6602 train-loss: 1.4536495208740234\n",
      "[LOG 20200511-10:26:26] epoch: 0, batch: 6603 train-loss: 1.200645089149475\n",
      "[LOG 20200511-10:26:26] epoch: 0, batch: 6604 train-loss: 0.5027307868003845\n",
      "[LOG 20200511-10:26:26] epoch: 0, batch: 6605 train-loss: 0.6922315359115601\n",
      "[LOG 20200511-10:26:26] epoch: 0, batch: 6606 train-loss: 0.3415667414665222\n",
      "[LOG 20200511-10:26:26] epoch: 0, batch: 6607 train-loss: 0.48764342069625854\n",
      "[LOG 20200511-10:26:26] epoch: 0, batch: 6608 train-loss: 0.7493181228637695\n",
      "[LOG 20200511-10:26:26] epoch: 0, batch: 6609 train-loss: 2.3060874938964844\n",
      "[LOG 20200511-10:26:26] epoch: 0, batch: 6610 train-loss: 0.9970502853393555\n",
      "[LOG 20200511-10:26:27] epoch: 0, batch: 6611 train-loss: 1.299285650253296\n",
      "[LOG 20200511-10:26:27] epoch: 0, batch: 6612 train-loss: 1.5477573871612549\n",
      "[LOG 20200511-10:26:27] epoch: 0, batch: 6613 train-loss: 0.6385247707366943\n",
      "[LOG 20200511-10:26:27] epoch: 0, batch: 6614 train-loss: 1.2827857732772827\n",
      "[LOG 20200511-10:26:27] epoch: 0, batch: 6615 train-loss: 1.771531581878662\n",
      "[LOG 20200511-10:26:27] epoch: 0, batch: 6616 train-loss: 2.564005136489868\n",
      "[LOG 20200511-10:26:27] epoch: 0, batch: 6617 train-loss: 0.9207651615142822\n",
      "[LOG 20200511-10:26:28] epoch: 0, batch: 6618 train-loss: 0.8618553876876831\n",
      "[LOG 20200511-10:26:28] epoch: 0, batch: 6619 train-loss: 1.7419344186782837\n",
      "[LOG 20200511-10:26:28] epoch: 0, batch: 6620 train-loss: 1.5392560958862305\n",
      "[LOG 20200511-10:26:28] epoch: 0, batch: 6621 train-loss: 0.8396648168563843\n",
      "[LOG 20200511-10:26:28] epoch: 0, batch: 6622 train-loss: 1.2812156677246094\n",
      "[LOG 20200511-10:26:28] epoch: 0, batch: 6623 train-loss: 1.1188174486160278\n",
      "[LOG 20200511-10:26:28] epoch: 0, batch: 6624 train-loss: 1.64519464969635\n",
      "[LOG 20200511-10:26:28] epoch: 0, batch: 6625 train-loss: 1.0932351350784302\n",
      "[LOG 20200511-10:26:28] epoch: 0, batch: 6626 train-loss: 0.4194638729095459\n",
      "[LOG 20200511-10:26:29] epoch: 0, batch: 6627 train-loss: 1.5948121547698975\n",
      "[LOG 20200511-10:26:29] epoch: 0, batch: 6628 train-loss: 1.9914278984069824\n",
      "[LOG 20200511-10:26:29] epoch: 0, batch: 6629 train-loss: 1.4986811876296997\n",
      "[LOG 20200511-10:26:29] epoch: 0, batch: 6630 train-loss: 2.497067451477051\n",
      "[LOG 20200511-10:26:29] epoch: 0, batch: 6631 train-loss: 2.1677191257476807\n",
      "[LOG 20200511-10:26:29] epoch: 0, batch: 6632 train-loss: 1.4314615726470947\n",
      "[LOG 20200511-10:26:29] epoch: 0, batch: 6633 train-loss: 1.8532601594924927\n",
      "[LOG 20200511-10:26:29] epoch: 0, batch: 6634 train-loss: 1.2215147018432617\n",
      "[LOG 20200511-10:26:30] epoch: 0, batch: 6635 train-loss: 2.248664379119873\n",
      "[LOG 20200511-10:26:30] epoch: 0, batch: 6636 train-loss: 1.5647997856140137\n",
      "[LOG 20200511-10:26:30] epoch: 0, batch: 6637 train-loss: 1.4816200733184814\n",
      "[LOG 20200511-10:26:30] epoch: 0, batch: 6638 train-loss: 2.147665023803711\n",
      "[LOG 20200511-10:26:30] epoch: 0, batch: 6639 train-loss: 0.8539508581161499\n",
      "[LOG 20200511-10:26:30] epoch: 0, batch: 6640 train-loss: 0.8075379729270935\n",
      "[LOG 20200511-10:26:30] epoch: 0, batch: 6641 train-loss: 1.4520576000213623\n",
      "[LOG 20200511-10:26:30] epoch: 0, batch: 6642 train-loss: 1.4672818183898926\n",
      "[LOG 20200511-10:26:30] epoch: 0, batch: 6643 train-loss: 1.7544348239898682\n",
      "[LOG 20200511-10:26:31] epoch: 0, batch: 6644 train-loss: 1.4386106729507446\n",
      "[LOG 20200511-10:26:31] epoch: 0, batch: 6645 train-loss: 1.8202626705169678\n",
      "[LOG 20200511-10:26:31] epoch: 0, batch: 6646 train-loss: 0.9846251606941223\n",
      "[LOG 20200511-10:26:31] epoch: 0, batch: 6647 train-loss: 1.8353290557861328\n",
      "[LOG 20200511-10:26:31] epoch: 0, batch: 6648 train-loss: 0.7628101110458374\n",
      "[LOG 20200511-10:26:31] epoch: 0, batch: 6649 train-loss: 1.3927626609802246\n",
      "[LOG 20200511-10:26:31] epoch: 0, batch: 6650 train-loss: 0.6730256676673889\n",
      "[LOG 20200511-10:26:31] epoch: 0, batch: 6651 train-loss: 0.9526153802871704\n",
      "[LOG 20200511-10:26:31] epoch: 0, batch: 6652 train-loss: 1.2982738018035889\n",
      "[LOG 20200511-10:26:32] epoch: 0, batch: 6653 train-loss: 0.6647481322288513\n",
      "[LOG 20200511-10:26:32] epoch: 0, batch: 6654 train-loss: 1.6493124961853027\n",
      "[LOG 20200511-10:26:32] epoch: 0, batch: 6655 train-loss: 1.4291329383850098\n",
      "[LOG 20200511-10:26:32] epoch: 0, batch: 6656 train-loss: 1.2476483583450317\n",
      "[LOG 20200511-10:26:32] epoch: 0, batch: 6657 train-loss: 1.1601226329803467\n",
      "[LOG 20200511-10:26:32] epoch: 0, batch: 6658 train-loss: 1.540593147277832\n",
      "[LOG 20200511-10:26:32] epoch: 0, batch: 6659 train-loss: 0.9730256795883179\n",
      "[LOG 20200511-10:26:32] epoch: 0, batch: 6660 train-loss: 0.27963367104530334\n",
      "[LOG 20200511-10:26:32] epoch: 0, batch: 6661 train-loss: 1.4843788146972656\n",
      "[LOG 20200511-10:26:32] epoch: 0, batch: 6662 train-loss: 1.2391855716705322\n",
      "[LOG 20200511-10:26:33] epoch: 0, batch: 6663 train-loss: 1.5120704174041748\n",
      "[LOG 20200511-10:26:33] epoch: 0, batch: 6664 train-loss: 1.0296133756637573\n",
      "[LOG 20200511-10:26:33] epoch: 0, batch: 6665 train-loss: 1.1099413633346558\n",
      "[LOG 20200511-10:26:33] epoch: 0, batch: 6666 train-loss: 2.239140510559082\n",
      "[LOG 20200511-10:26:33] epoch: 0, batch: 6667 train-loss: 1.8046526908874512\n",
      "[LOG 20200511-10:26:33] epoch: 0, batch: 6668 train-loss: 1.3715413808822632\n",
      "[LOG 20200511-10:26:33] epoch: 0, batch: 6669 train-loss: 1.9787366390228271\n",
      "[LOG 20200511-10:26:33] epoch: 0, batch: 6670 train-loss: 1.520406723022461\n",
      "[LOG 20200511-10:26:33] epoch: 0, batch: 6671 train-loss: 1.4999810457229614\n",
      "[LOG 20200511-10:26:34] epoch: 0, batch: 6672 train-loss: 1.888886570930481\n",
      "[LOG 20200511-10:26:34] epoch: 0, batch: 6673 train-loss: 1.677427053451538\n",
      "[LOG 20200511-10:26:34] epoch: 0, batch: 6674 train-loss: 1.3243956565856934\n",
      "[LOG 20200511-10:26:34] epoch: 0, batch: 6675 train-loss: 1.342963695526123\n",
      "[LOG 20200511-10:26:34] epoch: 0, batch: 6676 train-loss: 1.1299408674240112\n",
      "[LOG 20200511-10:26:34] epoch: 0, batch: 6677 train-loss: 1.0140246152877808\n",
      "[LOG 20200511-10:26:34] epoch: 0, batch: 6678 train-loss: 0.9568585157394409\n",
      "[LOG 20200511-10:26:34] epoch: 0, batch: 6679 train-loss: 1.3094240427017212\n",
      "[LOG 20200511-10:26:35] epoch: 0, batch: 6680 train-loss: 1.2731343507766724\n",
      "[LOG 20200511-10:26:35] epoch: 0, batch: 6681 train-loss: 0.5759934186935425\n",
      "[LOG 20200511-10:26:35] epoch: 0, batch: 6682 train-loss: 0.8451290130615234\n",
      "[LOG 20200511-10:26:35] epoch: 0, batch: 6683 train-loss: 0.9124644994735718\n",
      "[LOG 20200511-10:26:35] epoch: 0, batch: 6684 train-loss: 1.8372036218643188\n",
      "[LOG 20200511-10:26:35] epoch: 0, batch: 6685 train-loss: 0.6870295405387878\n",
      "[LOG 20200511-10:26:35] epoch: 0, batch: 6686 train-loss: 1.6992111206054688\n",
      "[LOG 20200511-10:26:35] epoch: 0, batch: 6687 train-loss: 2.608107328414917\n",
      "[LOG 20200511-10:26:35] epoch: 0, batch: 6688 train-loss: 3.136653423309326\n",
      "[LOG 20200511-10:26:36] epoch: 0, batch: 6689 train-loss: 0.4864519238471985\n",
      "[LOG 20200511-10:26:36] epoch: 0, batch: 6690 train-loss: 0.6276298761367798\n",
      "[LOG 20200511-10:26:36] epoch: 0, batch: 6691 train-loss: 1.2122353315353394\n",
      "[LOG 20200511-10:26:36] epoch: 0, batch: 6692 train-loss: 0.8912696838378906\n",
      "[LOG 20200511-10:26:36] epoch: 0, batch: 6693 train-loss: 1.1471147537231445\n",
      "[LOG 20200511-10:26:36] epoch: 0, batch: 6694 train-loss: 1.3380661010742188\n",
      "[LOG 20200511-10:26:36] epoch: 0, batch: 6695 train-loss: 2.3822455406188965\n",
      "[LOG 20200511-10:26:36] epoch: 0, batch: 6696 train-loss: 0.9690092206001282\n",
      "[LOG 20200511-10:26:36] epoch: 0, batch: 6697 train-loss: 0.95508873462677\n",
      "[LOG 20200511-10:26:37] epoch: 0, batch: 6698 train-loss: 0.8612135648727417\n",
      "[LOG 20200511-10:26:37] epoch: 0, batch: 6699 train-loss: 2.2304508686065674\n",
      "[LOG 20200511-10:26:37] epoch: 0, batch: 6700 train-loss: 2.301624298095703\n",
      "[LOG 20200511-10:26:37] epoch: 0, batch: 6701 train-loss: 0.7586382627487183\n",
      "[LOG 20200511-10:26:37] epoch: 0, batch: 6702 train-loss: 0.9346517324447632\n",
      "[LOG 20200511-10:26:37] epoch: 0, batch: 6703 train-loss: 1.4893380403518677\n",
      "[LOG 20200511-10:26:37] epoch: 0, batch: 6704 train-loss: 1.83461332321167\n",
      "[LOG 20200511-10:26:37] epoch: 0, batch: 6705 train-loss: 0.5837481021881104\n",
      "[LOG 20200511-10:26:37] epoch: 0, batch: 6706 train-loss: 1.3495595455169678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:26:38] epoch: 0, batch: 6707 train-loss: 1.9103350639343262\n",
      "[LOG 20200511-10:26:38] epoch: 0, batch: 6708 train-loss: 1.858810544013977\n",
      "[LOG 20200511-10:26:38] epoch: 0, batch: 6709 train-loss: 1.5895819664001465\n",
      "[LOG 20200511-10:26:38] epoch: 0, batch: 6710 train-loss: 2.035689115524292\n",
      "[LOG 20200511-10:26:38] epoch: 0, batch: 6711 train-loss: 0.639918327331543\n",
      "[LOG 20200511-10:26:38] epoch: 0, batch: 6712 train-loss: 1.492430329322815\n",
      "[LOG 20200511-10:26:38] epoch: 0, batch: 6713 train-loss: 0.5722498893737793\n",
      "[LOG 20200511-10:26:38] epoch: 0, batch: 6714 train-loss: 1.304979681968689\n",
      "[LOG 20200511-10:26:39] epoch: 0, batch: 6715 train-loss: 0.957931637763977\n",
      "[LOG 20200511-10:26:39] epoch: 0, batch: 6716 train-loss: 0.8156419992446899\n",
      "[LOG 20200511-10:26:39] epoch: 0, batch: 6717 train-loss: 0.9003961086273193\n",
      "[LOG 20200511-10:26:39] epoch: 0, batch: 6718 train-loss: 0.8278746604919434\n",
      "[LOG 20200511-10:26:39] epoch: 0, batch: 6719 train-loss: 1.8882904052734375\n",
      "[LOG 20200511-10:26:39] epoch: 0, batch: 6720 train-loss: 1.6628499031066895\n",
      "[LOG 20200511-10:26:39] epoch: 0, batch: 6721 train-loss: 0.8785316944122314\n",
      "[LOG 20200511-10:26:39] epoch: 0, batch: 6722 train-loss: 1.1730046272277832\n",
      "[LOG 20200511-10:26:39] epoch: 0, batch: 6723 train-loss: 1.550438642501831\n",
      "[LOG 20200511-10:26:40] epoch: 0, batch: 6724 train-loss: 0.9645453691482544\n",
      "[LOG 20200511-10:26:40] epoch: 0, batch: 6725 train-loss: 1.4432395696640015\n",
      "[LOG 20200511-10:26:40] epoch: 0, batch: 6726 train-loss: 0.9870085716247559\n",
      "[LOG 20200511-10:26:40] epoch: 0, batch: 6727 train-loss: 0.9460804462432861\n",
      "[LOG 20200511-10:26:40] epoch: 0, batch: 6728 train-loss: 0.9967750906944275\n",
      "[LOG 20200511-10:26:40] epoch: 0, batch: 6729 train-loss: 0.49164167046546936\n",
      "[LOG 20200511-10:26:40] epoch: 0, batch: 6730 train-loss: 0.41259095072746277\n",
      "[LOG 20200511-10:26:40] epoch: 0, batch: 6731 train-loss: 0.8173036575317383\n",
      "[LOG 20200511-10:26:41] epoch: 0, batch: 6732 train-loss: 0.6884108185768127\n",
      "[LOG 20200511-10:26:41] epoch: 0, batch: 6733 train-loss: 2.464966058731079\n",
      "[LOG 20200511-10:26:41] epoch: 0, batch: 6734 train-loss: 0.8002392053604126\n",
      "[LOG 20200511-10:26:41] epoch: 0, batch: 6735 train-loss: 1.2801778316497803\n",
      "[LOG 20200511-10:26:41] epoch: 0, batch: 6736 train-loss: 0.339496374130249\n",
      "[LOG 20200511-10:26:41] epoch: 0, batch: 6737 train-loss: 2.2294676303863525\n",
      "[LOG 20200511-10:26:41] epoch: 0, batch: 6738 train-loss: 0.41250813007354736\n",
      "[LOG 20200511-10:26:41] epoch: 0, batch: 6739 train-loss: 1.6667726039886475\n",
      "[LOG 20200511-10:26:42] epoch: 0, batch: 6740 train-loss: 2.2420544624328613\n",
      "[LOG 20200511-10:26:42] epoch: 0, batch: 6741 train-loss: 0.8590118288993835\n",
      "[LOG 20200511-10:26:42] epoch: 0, batch: 6742 train-loss: 0.9583870768547058\n",
      "[LOG 20200511-10:26:42] epoch: 0, batch: 6743 train-loss: 1.5202763080596924\n",
      "[LOG 20200511-10:26:42] epoch: 0, batch: 6744 train-loss: 0.7816599607467651\n",
      "[LOG 20200511-10:26:42] epoch: 0, batch: 6745 train-loss: 0.8310685753822327\n",
      "[LOG 20200511-10:26:42] epoch: 0, batch: 6746 train-loss: 2.4863710403442383\n",
      "[LOG 20200511-10:26:42] epoch: 0, batch: 6747 train-loss: 1.0297465324401855\n",
      "[LOG 20200511-10:26:42] epoch: 0, batch: 6748 train-loss: 2.6911909580230713\n",
      "[LOG 20200511-10:26:43] epoch: 0, batch: 6749 train-loss: 0.6353589296340942\n",
      "[LOG 20200511-10:26:43] epoch: 0, batch: 6750 train-loss: 1.0652167797088623\n",
      "[LOG 20200511-10:26:43] epoch: 0, batch: 6751 train-loss: 1.0176568031311035\n",
      "[LOG 20200511-10:26:43] epoch: 0, batch: 6752 train-loss: 2.7013111114501953\n",
      "[LOG 20200511-10:26:43] epoch: 0, batch: 6753 train-loss: 0.7527686953544617\n",
      "[LOG 20200511-10:26:43] epoch: 0, batch: 6754 train-loss: 1.2001025676727295\n",
      "[LOG 20200511-10:26:43] epoch: 0, batch: 6755 train-loss: 1.3569014072418213\n",
      "[LOG 20200511-10:26:43] epoch: 0, batch: 6756 train-loss: 0.9667492508888245\n",
      "[LOG 20200511-10:26:44] epoch: 0, batch: 6757 train-loss: 2.564270496368408\n",
      "[LOG 20200511-10:26:44] epoch: 0, batch: 6758 train-loss: 1.5886794328689575\n",
      "[LOG 20200511-10:26:44] epoch: 0, batch: 6759 train-loss: 1.3852063417434692\n",
      "[LOG 20200511-10:26:44] epoch: 0, batch: 6760 train-loss: 2.4353253841400146\n",
      "[LOG 20200511-10:26:44] epoch: 0, batch: 6761 train-loss: 2.0021238327026367\n",
      "[LOG 20200511-10:26:44] epoch: 0, batch: 6762 train-loss: 1.279035210609436\n",
      "[LOG 20200511-10:26:44] epoch: 0, batch: 6763 train-loss: 0.840157151222229\n",
      "[LOG 20200511-10:26:44] epoch: 0, batch: 6764 train-loss: 1.9361891746520996\n",
      "[LOG 20200511-10:26:44] epoch: 0, batch: 6765 train-loss: 2.061333417892456\n",
      "[LOG 20200511-10:26:45] epoch: 0, batch: 6766 train-loss: 1.3084397315979004\n",
      "[LOG 20200511-10:26:45] epoch: 0, batch: 6767 train-loss: 0.7209572792053223\n",
      "[LOG 20200511-10:26:45] epoch: 0, batch: 6768 train-loss: 1.8885219097137451\n",
      "[LOG 20200511-10:26:45] epoch: 0, batch: 6769 train-loss: 2.3090999126434326\n",
      "[LOG 20200511-10:26:45] epoch: 0, batch: 6770 train-loss: 1.9153454303741455\n",
      "[LOG 20200511-10:26:45] epoch: 0, batch: 6771 train-loss: 1.309458613395691\n",
      "[LOG 20200511-10:26:45] epoch: 0, batch: 6772 train-loss: 0.694876492023468\n",
      "[LOG 20200511-10:26:45] epoch: 0, batch: 6773 train-loss: 1.7234855890274048\n",
      "[LOG 20200511-10:26:46] epoch: 0, batch: 6774 train-loss: 0.35941535234451294\n",
      "[LOG 20200511-10:26:46] epoch: 0, batch: 6775 train-loss: 1.1427180767059326\n",
      "[LOG 20200511-10:26:46] epoch: 0, batch: 6776 train-loss: 1.5485072135925293\n",
      "[LOG 20200511-10:26:46] epoch: 0, batch: 6777 train-loss: 1.845523476600647\n",
      "[LOG 20200511-10:26:46] epoch: 0, batch: 6778 train-loss: 0.7856178283691406\n",
      "[LOG 20200511-10:26:46] epoch: 0, batch: 6779 train-loss: 1.8661308288574219\n",
      "[LOG 20200511-10:26:46] epoch: 0, batch: 6780 train-loss: 0.5390278100967407\n",
      "[LOG 20200511-10:26:46] epoch: 0, batch: 6781 train-loss: 1.0663913488388062\n",
      "[LOG 20200511-10:26:46] epoch: 0, batch: 6782 train-loss: 1.8888754844665527\n",
      "[LOG 20200511-10:26:47] epoch: 0, batch: 6783 train-loss: 0.7067471742630005\n",
      "[LOG 20200511-10:26:47] epoch: 0, batch: 6784 train-loss: 1.4635822772979736\n",
      "[LOG 20200511-10:26:47] epoch: 0, batch: 6785 train-loss: 1.7405879497528076\n",
      "[LOG 20200511-10:26:47] epoch: 0, batch: 6786 train-loss: 0.8903782367706299\n",
      "[LOG 20200511-10:26:47] epoch: 0, batch: 6787 train-loss: 0.44950002431869507\n",
      "[LOG 20200511-10:26:47] epoch: 0, batch: 6788 train-loss: 2.6147496700286865\n",
      "[LOG 20200511-10:26:47] epoch: 0, batch: 6789 train-loss: 1.8016310930252075\n",
      "[LOG 20200511-10:26:48] epoch: 0, batch: 6790 train-loss: 2.629006862640381\n",
      "[LOG 20200511-10:26:48] epoch: 0, batch: 6791 train-loss: 1.1688169240951538\n",
      "[LOG 20200511-10:26:48] epoch: 0, batch: 6792 train-loss: 0.967156171798706\n",
      "[LOG 20200511-10:26:48] epoch: 0, batch: 6793 train-loss: 1.6937527656555176\n",
      "[LOG 20200511-10:26:48] epoch: 0, batch: 6794 train-loss: 1.1253819465637207\n",
      "[LOG 20200511-10:26:48] epoch: 0, batch: 6795 train-loss: 1.2065694332122803\n",
      "[LOG 20200511-10:26:48] epoch: 0, batch: 6796 train-loss: 0.6060039401054382\n",
      "[LOG 20200511-10:26:48] epoch: 0, batch: 6797 train-loss: 2.443833827972412\n",
      "[LOG 20200511-10:26:48] epoch: 0, batch: 6798 train-loss: 1.1545251607894897\n",
      "[LOG 20200511-10:26:49] epoch: 0, batch: 6799 train-loss: 2.22322154045105\n",
      "[LOG 20200511-10:26:49] epoch: 0, batch: 6800 train-loss: 1.0466285943984985\n",
      "[LOG 20200511-10:26:49] epoch: 0, batch: 6801 train-loss: 1.0932728052139282\n",
      "[LOG 20200511-10:26:49] epoch: 0, batch: 6802 train-loss: 1.423377513885498\n",
      "[LOG 20200511-10:26:49] epoch: 0, batch: 6803 train-loss: 1.5398690700531006\n",
      "[LOG 20200511-10:26:49] epoch: 0, batch: 6804 train-loss: 1.4928514957427979\n",
      "[LOG 20200511-10:26:49] epoch: 0, batch: 6805 train-loss: 0.910767138004303\n",
      "[LOG 20200511-10:26:49] epoch: 0, batch: 6806 train-loss: 1.4112476110458374\n",
      "[LOG 20200511-10:26:50] epoch: 0, batch: 6807 train-loss: 1.7095792293548584\n",
      "[LOG 20200511-10:26:50] epoch: 0, batch: 6808 train-loss: 0.7215330600738525\n",
      "[LOG 20200511-10:26:50] epoch: 0, batch: 6809 train-loss: 1.6772894859313965\n",
      "[LOG 20200511-10:26:50] epoch: 0, batch: 6810 train-loss: 1.4474810361862183\n",
      "[LOG 20200511-10:26:50] epoch: 0, batch: 6811 train-loss: 1.1915603876113892\n",
      "[LOG 20200511-10:26:50] epoch: 0, batch: 6812 train-loss: 1.6925864219665527\n",
      "[LOG 20200511-10:26:50] epoch: 0, batch: 6813 train-loss: 1.8829349279403687\n",
      "[LOG 20200511-10:26:50] epoch: 0, batch: 6814 train-loss: 1.6752958297729492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:26:51] epoch: 0, batch: 6815 train-loss: 1.685110330581665\n",
      "[LOG 20200511-10:26:51] epoch: 0, batch: 6816 train-loss: 0.8658754229545593\n",
      "[LOG 20200511-10:26:51] epoch: 0, batch: 6817 train-loss: 1.600435733795166\n",
      "[LOG 20200511-10:26:51] epoch: 0, batch: 6818 train-loss: 0.7393511533737183\n",
      "[LOG 20200511-10:26:51] epoch: 0, batch: 6819 train-loss: 1.3535687923431396\n",
      "[LOG 20200511-10:26:51] epoch: 0, batch: 6820 train-loss: 0.8742227554321289\n",
      "[LOG 20200511-10:26:51] epoch: 0, batch: 6821 train-loss: 1.1683067083358765\n",
      "[LOG 20200511-10:26:51] epoch: 0, batch: 6822 train-loss: 2.0325427055358887\n",
      "[LOG 20200511-10:26:52] epoch: 0, batch: 6823 train-loss: 1.515096664428711\n",
      "[LOG 20200511-10:26:52] epoch: 0, batch: 6824 train-loss: 0.9693814516067505\n",
      "[LOG 20200511-10:26:52] epoch: 0, batch: 6825 train-loss: 1.2751843929290771\n",
      "[LOG 20200511-10:26:52] epoch: 0, batch: 6826 train-loss: 1.3101789951324463\n",
      "[LOG 20200511-10:26:52] epoch: 0, batch: 6827 train-loss: 1.2655911445617676\n",
      "[LOG 20200511-10:26:52] epoch: 0, batch: 6828 train-loss: 0.8888806104660034\n",
      "[LOG 20200511-10:26:52] epoch: 0, batch: 6829 train-loss: 0.6308601498603821\n",
      "[LOG 20200511-10:26:52] epoch: 0, batch: 6830 train-loss: 0.3809241056442261\n",
      "[LOG 20200511-10:26:52] epoch: 0, batch: 6831 train-loss: 0.8914573788642883\n",
      "[LOG 20200511-10:26:53] epoch: 0, batch: 6832 train-loss: 1.4208791255950928\n",
      "[LOG 20200511-10:26:53] epoch: 0, batch: 6833 train-loss: 1.4701288938522339\n",
      "[LOG 20200511-10:26:53] epoch: 0, batch: 6834 train-loss: 1.9990813732147217\n",
      "[LOG 20200511-10:26:53] epoch: 0, batch: 6835 train-loss: 1.5247282981872559\n",
      "[LOG 20200511-10:26:53] epoch: 0, batch: 6836 train-loss: 1.26213800907135\n",
      "[LOG 20200511-10:26:53] epoch: 0, batch: 6837 train-loss: 0.546518087387085\n",
      "[LOG 20200511-10:26:53] epoch: 0, batch: 6838 train-loss: 0.49340641498565674\n",
      "[LOG 20200511-10:26:53] epoch: 0, batch: 6839 train-loss: 1.9422193765640259\n",
      "[LOG 20200511-10:26:53] epoch: 0, batch: 6840 train-loss: 1.0674409866333008\n",
      "[LOG 20200511-10:26:54] epoch: 0, batch: 6841 train-loss: 1.3768258094787598\n",
      "[LOG 20200511-10:26:54] epoch: 0, batch: 6842 train-loss: 0.6146557331085205\n",
      "[LOG 20200511-10:26:54] epoch: 0, batch: 6843 train-loss: 1.0289875268936157\n",
      "[LOG 20200511-10:26:54] epoch: 0, batch: 6844 train-loss: 1.1546496152877808\n",
      "[LOG 20200511-10:26:54] epoch: 0, batch: 6845 train-loss: 0.8422722220420837\n",
      "[LOG 20200511-10:26:54] epoch: 0, batch: 6846 train-loss: 0.615748405456543\n",
      "[LOG 20200511-10:26:54] epoch: 0, batch: 6847 train-loss: 2.0373640060424805\n",
      "[LOG 20200511-10:26:54] epoch: 0, batch: 6848 train-loss: 1.3173027038574219\n",
      "[LOG 20200511-10:26:54] epoch: 0, batch: 6849 train-loss: 1.0234062671661377\n",
      "[LOG 20200511-10:26:55] epoch: 0, batch: 6850 train-loss: 0.7036951184272766\n",
      "[LOG 20200511-10:26:55] epoch: 0, batch: 6851 train-loss: 0.3297612965106964\n",
      "[LOG 20200511-10:26:55] epoch: 0, batch: 6852 train-loss: 0.59674471616745\n",
      "[LOG 20200511-10:26:55] epoch: 0, batch: 6853 train-loss: 1.3458130359649658\n",
      "[LOG 20200511-10:26:55] epoch: 0, batch: 6854 train-loss: 1.173355221748352\n",
      "[LOG 20200511-10:26:55] epoch: 0, batch: 6855 train-loss: 1.2597686052322388\n",
      "[LOG 20200511-10:26:55] epoch: 0, batch: 6856 train-loss: 1.7663726806640625\n",
      "[LOG 20200511-10:26:55] epoch: 0, batch: 6857 train-loss: 1.3508775234222412\n",
      "[LOG 20200511-10:26:56] epoch: 0, batch: 6858 train-loss: 0.8561664819717407\n",
      "[LOG 20200511-10:26:56] epoch: 0, batch: 6859 train-loss: 0.7993582487106323\n",
      "[LOG 20200511-10:26:56] epoch: 0, batch: 6860 train-loss: 1.5928089618682861\n",
      "[LOG 20200511-10:26:56] epoch: 0, batch: 6861 train-loss: 1.2343159914016724\n",
      "[LOG 20200511-10:26:56] epoch: 0, batch: 6862 train-loss: 2.1509320735931396\n",
      "[LOG 20200511-10:26:56] epoch: 0, batch: 6863 train-loss: 1.3002240657806396\n",
      "[LOG 20200511-10:26:56] epoch: 0, batch: 6864 train-loss: 1.2509925365447998\n",
      "[LOG 20200511-10:26:56] epoch: 0, batch: 6865 train-loss: 2.5318820476531982\n",
      "[LOG 20200511-10:26:57] epoch: 0, batch: 6866 train-loss: 2.4785921573638916\n",
      "[LOG 20200511-10:26:57] epoch: 0, batch: 6867 train-loss: 1.4068551063537598\n",
      "[LOG 20200511-10:26:57] epoch: 0, batch: 6868 train-loss: 0.5993683338165283\n",
      "[LOG 20200511-10:26:57] epoch: 0, batch: 6869 train-loss: 1.4946178197860718\n",
      "[LOG 20200511-10:26:57] epoch: 0, batch: 6870 train-loss: 1.0729899406433105\n",
      "[LOG 20200511-10:26:57] epoch: 0, batch: 6871 train-loss: 0.8967504501342773\n",
      "[LOG 20200511-10:26:57] epoch: 0, batch: 6872 train-loss: 1.3041473627090454\n",
      "[LOG 20200511-10:26:57] epoch: 0, batch: 6873 train-loss: 1.248225450515747\n",
      "[LOG 20200511-10:26:57] epoch: 0, batch: 6874 train-loss: 1.06740140914917\n",
      "[LOG 20200511-10:26:58] epoch: 0, batch: 6875 train-loss: 0.5595386028289795\n",
      "[LOG 20200511-10:26:58] epoch: 0, batch: 6876 train-loss: 1.7920682430267334\n",
      "[LOG 20200511-10:26:58] epoch: 0, batch: 6877 train-loss: 2.192204713821411\n",
      "[LOG 20200511-10:26:58] epoch: 0, batch: 6878 train-loss: 1.5759899616241455\n",
      "[LOG 20200511-10:26:58] epoch: 0, batch: 6879 train-loss: 1.1220464706420898\n",
      "[LOG 20200511-10:26:58] epoch: 0, batch: 6880 train-loss: 1.181301474571228\n",
      "[LOG 20200511-10:26:58] epoch: 0, batch: 6881 train-loss: 1.7086868286132812\n",
      "[LOG 20200511-10:26:58] epoch: 0, batch: 6882 train-loss: 0.3057592809200287\n",
      "[LOG 20200511-10:26:58] epoch: 0, batch: 6883 train-loss: 1.5252981185913086\n",
      "[LOG 20200511-10:26:59] epoch: 0, batch: 6884 train-loss: 1.2090163230895996\n",
      "[LOG 20200511-10:26:59] epoch: 0, batch: 6885 train-loss: 1.1011104583740234\n",
      "[LOG 20200511-10:26:59] epoch: 0, batch: 6886 train-loss: 1.1064987182617188\n",
      "[LOG 20200511-10:26:59] epoch: 0, batch: 6887 train-loss: 1.059536337852478\n",
      "[LOG 20200511-10:26:59] epoch: 0, batch: 6888 train-loss: 0.7561377286911011\n",
      "[LOG 20200511-10:26:59] epoch: 0, batch: 6889 train-loss: 1.593977689743042\n",
      "[LOG 20200511-10:26:59] epoch: 0, batch: 6890 train-loss: 1.8174830675125122\n",
      "[LOG 20200511-10:26:59] epoch: 0, batch: 6891 train-loss: 1.5835316181182861\n",
      "[LOG 20200511-10:26:59] epoch: 0, batch: 6892 train-loss: 1.1531977653503418\n",
      "[LOG 20200511-10:26:59] epoch: 0, batch: 6893 train-loss: 1.2715057134628296\n",
      "[LOG 20200511-10:27:00] epoch: 0, batch: 6894 train-loss: 1.7693469524383545\n",
      "[LOG 20200511-10:27:00] epoch: 0, batch: 6895 train-loss: 1.8820829391479492\n",
      "[LOG 20200511-10:27:00] epoch: 0, batch: 6896 train-loss: 0.9204220175743103\n",
      "[LOG 20200511-10:27:00] epoch: 0, batch: 6897 train-loss: 1.603108525276184\n",
      "[LOG 20200511-10:27:00] epoch: 0, batch: 6898 train-loss: 0.5781386494636536\n",
      "[LOG 20200511-10:27:00] epoch: 0, batch: 6899 train-loss: 0.9539663791656494\n",
      "[LOG 20200511-10:27:00] epoch: 0, batch: 6900 train-loss: 1.6110928058624268\n",
      "[LOG 20200511-10:27:00] epoch: 0, batch: 6901 train-loss: 1.4462461471557617\n",
      "[LOG 20200511-10:27:00] epoch: 0, batch: 6902 train-loss: 0.896484911441803\n",
      "[LOG 20200511-10:27:01] epoch: 0, batch: 6903 train-loss: 0.732047438621521\n",
      "[LOG 20200511-10:27:01] epoch: 0, batch: 6904 train-loss: 1.1618562936782837\n",
      "[LOG 20200511-10:27:01] epoch: 0, batch: 6905 train-loss: 1.3874672651290894\n",
      "[LOG 20200511-10:27:01] epoch: 0, batch: 6906 train-loss: 0.6198511123657227\n",
      "[LOG 20200511-10:27:01] epoch: 0, batch: 6907 train-loss: 2.020167827606201\n",
      "[LOG 20200511-10:27:01] epoch: 0, batch: 6908 train-loss: 1.3079910278320312\n",
      "[LOG 20200511-10:27:01] epoch: 0, batch: 6909 train-loss: 1.3100444078445435\n",
      "[LOG 20200511-10:27:01] epoch: 0, batch: 6910 train-loss: 1.5250325202941895\n",
      "[LOG 20200511-10:27:01] epoch: 0, batch: 6911 train-loss: 1.4741984605789185\n",
      "[LOG 20200511-10:27:01] epoch: 0, batch: 6912 train-loss: 1.3414909839630127\n",
      "[LOG 20200511-10:27:02] epoch: 0, batch: 6913 train-loss: 0.7107659578323364\n",
      "[LOG 20200511-10:27:02] epoch: 0, batch: 6914 train-loss: 1.456111192703247\n",
      "[LOG 20200511-10:27:02] epoch: 0, batch: 6915 train-loss: 1.1166903972625732\n",
      "[LOG 20200511-10:27:02] epoch: 0, batch: 6916 train-loss: 0.69775390625\n",
      "[LOG 20200511-10:27:02] epoch: 0, batch: 6917 train-loss: 1.2218568325042725\n",
      "[LOG 20200511-10:27:02] epoch: 0, batch: 6918 train-loss: 1.5874192714691162\n",
      "[LOG 20200511-10:27:02] epoch: 0, batch: 6919 train-loss: 1.3438410758972168\n",
      "[LOG 20200511-10:27:02] epoch: 0, batch: 6920 train-loss: 1.7147648334503174\n",
      "[LOG 20200511-10:27:02] epoch: 0, batch: 6921 train-loss: 1.3420647382736206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:27:03] epoch: 0, batch: 6922 train-loss: 0.9388132095336914\n",
      "[LOG 20200511-10:27:03] epoch: 0, batch: 6923 train-loss: 1.7887784242630005\n",
      "[LOG 20200511-10:27:03] epoch: 0, batch: 6924 train-loss: 0.37828749418258667\n",
      "[LOG 20200511-10:27:03] epoch: 0, batch: 6925 train-loss: 1.3154515027999878\n",
      "[LOG 20200511-10:27:03] epoch: 0, batch: 6926 train-loss: 1.0196343660354614\n",
      "[LOG 20200511-10:27:03] epoch: 0, batch: 6927 train-loss: 1.9583439826965332\n",
      "[LOG 20200511-10:27:03] epoch: 0, batch: 6928 train-loss: 1.2892615795135498\n",
      "[LOG 20200511-10:27:03] epoch: 0, batch: 6929 train-loss: 1.0082614421844482\n",
      "[LOG 20200511-10:27:03] epoch: 0, batch: 6930 train-loss: 1.4930599927902222\n",
      "[LOG 20200511-10:27:03] epoch: 0, batch: 6931 train-loss: 1.2043750286102295\n",
      "[LOG 20200511-10:27:04] epoch: 0, batch: 6932 train-loss: 1.029106616973877\n",
      "[LOG 20200511-10:27:04] epoch: 0, batch: 6933 train-loss: 2.1208689212799072\n",
      "[LOG 20200511-10:27:04] epoch: 0, batch: 6934 train-loss: 0.9802248477935791\n",
      "[LOG 20200511-10:27:04] epoch: 0, batch: 6935 train-loss: 1.8144092559814453\n",
      "[LOG 20200511-10:27:04] epoch: 0, batch: 6936 train-loss: 1.652523398399353\n",
      "[LOG 20200511-10:27:04] epoch: 0, batch: 6937 train-loss: 0.607463538646698\n",
      "[LOG 20200511-10:27:04] epoch: 0, batch: 6938 train-loss: 1.883410930633545\n",
      "[LOG 20200511-10:27:04] epoch: 0, batch: 6939 train-loss: 1.4150500297546387\n",
      "[LOG 20200511-10:27:04] epoch: 0, batch: 6940 train-loss: 1.6585938930511475\n",
      "[LOG 20200511-10:27:04] epoch: 0, batch: 6941 train-loss: 1.942766547203064\n",
      "[LOG 20200511-10:27:05] epoch: 0, batch: 6942 train-loss: 1.1208598613739014\n",
      "[LOG 20200511-10:27:05] epoch: 0, batch: 6943 train-loss: 1.0620498657226562\n",
      "[LOG 20200511-10:27:05] epoch: 0, batch: 6944 train-loss: 1.0439519882202148\n",
      "[LOG 20200511-10:27:05] epoch: 0, batch: 6945 train-loss: 0.7557673454284668\n",
      "[LOG 20200511-10:27:05] epoch: 0, batch: 6946 train-loss: 1.4505705833435059\n",
      "[LOG 20200511-10:27:05] epoch: 0, batch: 6947 train-loss: 1.6137135028839111\n",
      "[LOG 20200511-10:27:05] epoch: 0, batch: 6948 train-loss: 0.9965516328811646\n",
      "[LOG 20200511-10:27:05] epoch: 0, batch: 6949 train-loss: 1.0342328548431396\n",
      "[LOG 20200511-10:27:05] epoch: 0, batch: 6950 train-loss: 1.5017410516738892\n",
      "[LOG 20200511-10:27:06] epoch: 0, batch: 6951 train-loss: 1.045793056488037\n",
      "[LOG 20200511-10:27:06] epoch: 0, batch: 6952 train-loss: 1.2122650146484375\n",
      "[LOG 20200511-10:27:06] epoch: 0, batch: 6953 train-loss: 1.748199462890625\n",
      "[LOG 20200511-10:27:06] epoch: 0, batch: 6954 train-loss: 1.1502501964569092\n",
      "[LOG 20200511-10:27:06] epoch: 0, batch: 6955 train-loss: 1.268289566040039\n",
      "[LOG 20200511-10:27:06] epoch: 0, batch: 6956 train-loss: 0.8580369353294373\n",
      "[LOG 20200511-10:27:06] epoch: 0, batch: 6957 train-loss: 2.19254732131958\n",
      "[LOG 20200511-10:27:06] epoch: 0, batch: 6958 train-loss: 1.5249288082122803\n",
      "[LOG 20200511-10:27:06] epoch: 0, batch: 6959 train-loss: 0.9133554697036743\n",
      "[LOG 20200511-10:27:06] epoch: 0, batch: 6960 train-loss: 0.9229154586791992\n",
      "[LOG 20200511-10:27:07] epoch: 0, batch: 6961 train-loss: 1.5579674243927002\n",
      "[LOG 20200511-10:27:07] epoch: 0, batch: 6962 train-loss: 1.8191840648651123\n",
      "[LOG 20200511-10:27:07] epoch: 0, batch: 6963 train-loss: 0.4938507080078125\n",
      "[LOG 20200511-10:27:07] epoch: 0, batch: 6964 train-loss: 0.5617169141769409\n",
      "[LOG 20200511-10:27:07] epoch: 0, batch: 6965 train-loss: 1.9629238843917847\n",
      "[LOG 20200511-10:27:07] epoch: 0, batch: 6966 train-loss: 2.021625518798828\n",
      "[LOG 20200511-10:27:07] epoch: 0, batch: 6967 train-loss: 0.5977169871330261\n",
      "[LOG 20200511-10:27:07] epoch: 0, batch: 6968 train-loss: 2.0797576904296875\n",
      "[LOG 20200511-10:27:07] epoch: 0, batch: 6969 train-loss: 0.8201832175254822\n",
      "[LOG 20200511-10:27:08] epoch: 0, batch: 6970 train-loss: 0.9139436483383179\n",
      "[LOG 20200511-10:27:08] epoch: 0, batch: 6971 train-loss: 1.7828216552734375\n",
      "[LOG 20200511-10:27:08] epoch: 0, batch: 6972 train-loss: 2.399526357650757\n",
      "[LOG 20200511-10:27:08] epoch: 0, batch: 6973 train-loss: 2.389157295227051\n",
      "[LOG 20200511-10:27:08] epoch: 0, batch: 6974 train-loss: 1.0603528022766113\n",
      "[LOG 20200511-10:27:08] epoch: 0, batch: 6975 train-loss: 1.062917709350586\n",
      "[LOG 20200511-10:27:08] epoch: 0, batch: 6976 train-loss: 0.8583062291145325\n",
      "[LOG 20200511-10:27:08] epoch: 0, batch: 6977 train-loss: 1.2563459873199463\n",
      "[LOG 20200511-10:27:08] epoch: 0, batch: 6978 train-loss: 1.4041415452957153\n",
      "[LOG 20200511-10:27:09] epoch: 0, batch: 6979 train-loss: 0.5358884334564209\n",
      "[LOG 20200511-10:27:09] epoch: 0, batch: 6980 train-loss: 1.759975552558899\n",
      "[LOG 20200511-10:27:09] epoch: 0, batch: 6981 train-loss: 0.9873753786087036\n",
      "[LOG 20200511-10:27:09] epoch: 0, batch: 6982 train-loss: 0.8237626552581787\n",
      "[LOG 20200511-10:27:09] epoch: 0, batch: 6983 train-loss: 0.6676754355430603\n",
      "[LOG 20200511-10:27:09] epoch: 0, batch: 6984 train-loss: 1.1957790851593018\n",
      "[LOG 20200511-10:27:09] epoch: 0, batch: 6985 train-loss: 1.3221138715744019\n",
      "[LOG 20200511-10:27:09] epoch: 0, batch: 6986 train-loss: 1.1123037338256836\n",
      "[LOG 20200511-10:27:09] epoch: 0, batch: 6987 train-loss: 0.9110050797462463\n",
      "[LOG 20200511-10:27:10] epoch: 0, batch: 6988 train-loss: 1.1269534826278687\n",
      "[LOG 20200511-10:27:10] epoch: 0, batch: 6989 train-loss: 1.9561421871185303\n",
      "[LOG 20200511-10:27:10] epoch: 0, batch: 6990 train-loss: 1.065132975578308\n",
      "[LOG 20200511-10:27:10] epoch: 0, batch: 6991 train-loss: 1.7513127326965332\n",
      "[LOG 20200511-10:27:10] epoch: 0, batch: 6992 train-loss: 0.8514978289604187\n",
      "[LOG 20200511-10:27:10] epoch: 0, batch: 6993 train-loss: 2.1604862213134766\n",
      "[LOG 20200511-10:27:10] epoch: 0, batch: 6994 train-loss: 0.6855434775352478\n",
      "[LOG 20200511-10:27:10] epoch: 0, batch: 6995 train-loss: 0.8044267296791077\n",
      "[LOG 20200511-10:27:11] epoch: 0, batch: 6996 train-loss: 1.1903729438781738\n",
      "[LOG 20200511-10:27:11] epoch: 0, batch: 6997 train-loss: 1.283768892288208\n",
      "[LOG 20200511-10:27:11] epoch: 0, batch: 6998 train-loss: 1.9054092168807983\n",
      "[LOG 20200511-10:27:11] epoch: 0, batch: 6999 train-loss: 1.4250967502593994\n",
      "[LOG 20200511-10:27:11] epoch: 0, batch: 7000 train-loss: 0.8336515426635742\n",
      "[LOG 20200511-10:27:11] epoch: 0, batch: 7001 train-loss: 1.2041987180709839\n",
      "[LOG 20200511-10:27:11] epoch: 0, batch: 7002 train-loss: 0.6544206142425537\n",
      "[LOG 20200511-10:27:11] epoch: 0, batch: 7003 train-loss: 1.8231803178787231\n",
      "[LOG 20200511-10:27:11] epoch: 0, batch: 7004 train-loss: 1.0791987180709839\n",
      "[LOG 20200511-10:27:12] epoch: 0, batch: 7005 train-loss: 1.9618180990219116\n",
      "[LOG 20200511-10:27:12] epoch: 0, batch: 7006 train-loss: 2.046175003051758\n",
      "[LOG 20200511-10:27:12] epoch: 0, batch: 7007 train-loss: 0.9622020721435547\n",
      "[LOG 20200511-10:27:12] epoch: 0, batch: 7008 train-loss: 0.632325291633606\n",
      "[LOG 20200511-10:27:12] epoch: 0, batch: 7009 train-loss: 1.3097128868103027\n",
      "[LOG 20200511-10:27:12] epoch: 0, batch: 7010 train-loss: 1.5165835618972778\n",
      "[LOG 20200511-10:27:12] epoch: 0, batch: 7011 train-loss: 0.5343487858772278\n",
      "[LOG 20200511-10:27:12] epoch: 0, batch: 7012 train-loss: 1.7108144760131836\n",
      "[LOG 20200511-10:27:12] epoch: 0, batch: 7013 train-loss: 0.6397237777709961\n",
      "[LOG 20200511-10:27:12] epoch: 0, batch: 7014 train-loss: 1.2867090702056885\n",
      "[LOG 20200511-10:27:13] epoch: 0, batch: 7015 train-loss: 1.1640375852584839\n",
      "[LOG 20200511-10:27:13] epoch: 0, batch: 7016 train-loss: 1.0024960041046143\n",
      "[LOG 20200511-10:27:13] epoch: 0, batch: 7017 train-loss: 1.3814232349395752\n",
      "[LOG 20200511-10:27:13] epoch: 0, batch: 7018 train-loss: 0.9940162301063538\n",
      "[LOG 20200511-10:27:13] epoch: 0, batch: 7019 train-loss: 1.3779890537261963\n",
      "[LOG 20200511-10:27:13] epoch: 0, batch: 7020 train-loss: 0.7074337005615234\n",
      "[LOG 20200511-10:27:13] epoch: 0, batch: 7021 train-loss: 1.1722749471664429\n",
      "[LOG 20200511-10:27:13] epoch: 0, batch: 7022 train-loss: 1.6265904903411865\n",
      "[LOG 20200511-10:27:13] epoch: 0, batch: 7023 train-loss: 0.5783342719078064\n",
      "[LOG 20200511-10:27:14] epoch: 0, batch: 7024 train-loss: 1.3078534603118896\n",
      "[LOG 20200511-10:27:14] epoch: 0, batch: 7025 train-loss: 1.4965254068374634\n",
      "[LOG 20200511-10:27:14] epoch: 0, batch: 7026 train-loss: 1.0000532865524292\n",
      "[LOG 20200511-10:27:14] epoch: 0, batch: 7027 train-loss: 1.6974889039993286\n",
      "[LOG 20200511-10:27:14] epoch: 0, batch: 7028 train-loss: 1.3631672859191895\n",
      "[LOG 20200511-10:27:14] epoch: 0, batch: 7029 train-loss: 2.0190963745117188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:27:14] epoch: 0, batch: 7030 train-loss: 1.0350011587142944\n",
      "[LOG 20200511-10:27:14] epoch: 0, batch: 7031 train-loss: 1.1974562406539917\n",
      "[LOG 20200511-10:27:14] epoch: 0, batch: 7032 train-loss: 1.853601336479187\n",
      "[LOG 20200511-10:27:14] epoch: 0, batch: 7033 train-loss: 0.8954137563705444\n",
      "[LOG 20200511-10:27:15] epoch: 0, batch: 7034 train-loss: 0.4893001317977905\n",
      "[LOG 20200511-10:27:15] epoch: 0, batch: 7035 train-loss: 1.5767669677734375\n",
      "[LOG 20200511-10:27:15] epoch: 0, batch: 7036 train-loss: 0.6265967488288879\n",
      "[LOG 20200511-10:27:15] epoch: 0, batch: 7037 train-loss: 2.017956018447876\n",
      "[LOG 20200511-10:27:15] epoch: 0, batch: 7038 train-loss: 1.7633428573608398\n",
      "[LOG 20200511-10:27:15] epoch: 0, batch: 7039 train-loss: 1.0236890316009521\n",
      "[LOG 20200511-10:27:15] epoch: 0, batch: 7040 train-loss: 0.7641014456748962\n",
      "[LOG 20200511-10:27:15] epoch: 0, batch: 7041 train-loss: 0.6818670630455017\n",
      "[LOG 20200511-10:27:15] epoch: 0, batch: 7042 train-loss: 1.7010931968688965\n",
      "[LOG 20200511-10:27:16] epoch: 0, batch: 7043 train-loss: 1.187103033065796\n",
      "[LOG 20200511-10:27:16] epoch: 0, batch: 7044 train-loss: 0.6938861608505249\n",
      "[LOG 20200511-10:27:16] epoch: 0, batch: 7045 train-loss: 0.8428131937980652\n",
      "[LOG 20200511-10:27:16] epoch: 0, batch: 7046 train-loss: 0.919462263584137\n",
      "[LOG 20200511-10:27:16] epoch: 0, batch: 7047 train-loss: 2.6135716438293457\n",
      "[LOG 20200511-10:27:16] epoch: 0, batch: 7048 train-loss: 0.9242676496505737\n",
      "[LOG 20200511-10:27:16] epoch: 0, batch: 7049 train-loss: 1.3145408630371094\n",
      "[LOG 20200511-10:27:16] epoch: 0, batch: 7050 train-loss: 1.018203854560852\n",
      "[LOG 20200511-10:27:16] epoch: 0, batch: 7051 train-loss: 1.4118170738220215\n",
      "[LOG 20200511-10:27:17] epoch: 0, batch: 7052 train-loss: 1.1461765766143799\n",
      "[LOG 20200511-10:27:17] epoch: 0, batch: 7053 train-loss: 1.1359021663665771\n",
      "[LOG 20200511-10:27:17] epoch: 0, batch: 7054 train-loss: 0.7832531929016113\n",
      "[LOG 20200511-10:27:17] epoch: 0, batch: 7055 train-loss: 0.618756115436554\n",
      "[LOG 20200511-10:27:17] epoch: 0, batch: 7056 train-loss: 1.747801661491394\n",
      "[LOG 20200511-10:27:17] epoch: 0, batch: 7057 train-loss: 2.7456753253936768\n",
      "[LOG 20200511-10:27:17] epoch: 0, batch: 7058 train-loss: 1.0997978448867798\n",
      "[LOG 20200511-10:27:17] epoch: 0, batch: 7059 train-loss: 2.0969767570495605\n",
      "[LOG 20200511-10:27:18] epoch: 0, batch: 7060 train-loss: 1.9771126508712769\n",
      "[LOG 20200511-10:27:18] epoch: 0, batch: 7061 train-loss: 1.1325020790100098\n",
      "[LOG 20200511-10:27:18] epoch: 0, batch: 7062 train-loss: 1.569838523864746\n",
      "[LOG 20200511-10:27:18] epoch: 0, batch: 7063 train-loss: 0.5189201831817627\n",
      "[LOG 20200511-10:27:18] epoch: 0, batch: 7064 train-loss: 1.092529535293579\n",
      "[LOG 20200511-10:27:18] epoch: 0, batch: 7065 train-loss: 1.8315485715866089\n",
      "[LOG 20200511-10:27:18] epoch: 0, batch: 7066 train-loss: 0.43537381291389465\n",
      "[LOG 20200511-10:27:18] epoch: 0, batch: 7067 train-loss: 1.3819867372512817\n",
      "[LOG 20200511-10:27:18] epoch: 0, batch: 7068 train-loss: 1.3971120119094849\n",
      "[LOG 20200511-10:27:19] epoch: 0, batch: 7069 train-loss: 2.367112636566162\n",
      "[LOG 20200511-10:27:19] epoch: 0, batch: 7070 train-loss: 0.553217887878418\n",
      "[LOG 20200511-10:27:19] epoch: 0, batch: 7071 train-loss: 0.6152547001838684\n",
      "[LOG 20200511-10:27:19] epoch: 0, batch: 7072 train-loss: 0.832840621471405\n",
      "[LOG 20200511-10:27:19] epoch: 0, batch: 7073 train-loss: 1.2793697118759155\n",
      "[LOG 20200511-10:27:19] epoch: 0, batch: 7074 train-loss: 1.6148921251296997\n",
      "[LOG 20200511-10:27:19] epoch: 0, batch: 7075 train-loss: 0.23518142104148865\n",
      "[LOG 20200511-10:27:19] epoch: 0, batch: 7076 train-loss: 0.8809818029403687\n",
      "[LOG 20200511-10:27:20] epoch: 0, batch: 7077 train-loss: 0.5700328946113586\n",
      "[LOG 20200511-10:27:20] epoch: 0, batch: 7078 train-loss: 1.2086153030395508\n",
      "[LOG 20200511-10:27:20] epoch: 0, batch: 7079 train-loss: 1.6338192224502563\n",
      "[LOG 20200511-10:27:20] epoch: 0, batch: 7080 train-loss: 1.7253782749176025\n",
      "[LOG 20200511-10:27:20] epoch: 0, batch: 7081 train-loss: 0.8544244170188904\n",
      "[LOG 20200511-10:27:20] epoch: 0, batch: 7082 train-loss: 1.445881962776184\n",
      "[LOG 20200511-10:27:20] epoch: 0, batch: 7083 train-loss: 1.702614426612854\n",
      "[LOG 20200511-10:27:20] epoch: 0, batch: 7084 train-loss: 2.0504746437072754\n",
      "[LOG 20200511-10:27:21] epoch: 0, batch: 7085 train-loss: 2.3660473823547363\n",
      "[LOG 20200511-10:27:21] epoch: 0, batch: 7086 train-loss: 0.8881458044052124\n",
      "[LOG 20200511-10:27:21] epoch: 0, batch: 7087 train-loss: 2.4477524757385254\n",
      "[LOG 20200511-10:27:21] epoch: 0, batch: 7088 train-loss: 0.7707291841506958\n",
      "[LOG 20200511-10:27:21] epoch: 0, batch: 7089 train-loss: 0.9034455418586731\n",
      "[LOG 20200511-10:27:21] epoch: 0, batch: 7090 train-loss: 1.264648199081421\n",
      "[LOG 20200511-10:27:21] epoch: 0, batch: 7091 train-loss: 0.6549450159072876\n",
      "[LOG 20200511-10:27:21] epoch: 0, batch: 7092 train-loss: 0.5900029540061951\n",
      "[LOG 20200511-10:27:21] epoch: 0, batch: 7093 train-loss: 2.45090651512146\n",
      "[LOG 20200511-10:27:22] epoch: 0, batch: 7094 train-loss: 1.3660153150558472\n",
      "[LOG 20200511-10:27:22] epoch: 0, batch: 7095 train-loss: 0.9866397380828857\n",
      "[LOG 20200511-10:27:22] epoch: 0, batch: 7096 train-loss: 1.3718235492706299\n",
      "[LOG 20200511-10:27:22] epoch: 0, batch: 7097 train-loss: 0.7034199833869934\n",
      "[LOG 20200511-10:27:22] epoch: 0, batch: 7098 train-loss: 1.3841232061386108\n",
      "[LOG 20200511-10:27:22] epoch: 0, batch: 7099 train-loss: 1.3613330125808716\n",
      "[LOG 20200511-10:27:22] epoch: 0, batch: 7100 train-loss: 1.1221790313720703\n",
      "[LOG 20200511-10:27:22] epoch: 0, batch: 7101 train-loss: 1.76463782787323\n",
      "[LOG 20200511-10:27:23] epoch: 0, batch: 7102 train-loss: 0.5780344009399414\n",
      "[LOG 20200511-10:27:23] epoch: 0, batch: 7103 train-loss: 1.0990861654281616\n",
      "[LOG 20200511-10:27:23] epoch: 0, batch: 7104 train-loss: 1.3472754955291748\n",
      "[LOG 20200511-10:27:23] epoch: 0, batch: 7105 train-loss: 0.9130483865737915\n",
      "[LOG 20200511-10:27:23] epoch: 0, batch: 7106 train-loss: 1.4813041687011719\n",
      "[LOG 20200511-10:27:23] epoch: 0, batch: 7107 train-loss: 2.577507972717285\n",
      "[LOG 20200511-10:27:23] epoch: 0, batch: 7108 train-loss: 1.359108805656433\n",
      "[LOG 20200511-10:27:23] epoch: 0, batch: 7109 train-loss: 1.7728371620178223\n",
      "[LOG 20200511-10:27:23] epoch: 0, batch: 7110 train-loss: 1.0790209770202637\n",
      "[LOG 20200511-10:27:24] epoch: 0, batch: 7111 train-loss: 0.49716916680336\n",
      "[LOG 20200511-10:27:24] epoch: 0, batch: 7112 train-loss: 0.5224651098251343\n",
      "[LOG 20200511-10:27:24] epoch: 0, batch: 7113 train-loss: 1.9558382034301758\n",
      "[LOG 20200511-10:27:24] epoch: 0, batch: 7114 train-loss: 2.19488263130188\n",
      "[LOG 20200511-10:27:24] epoch: 0, batch: 7115 train-loss: 0.8428668975830078\n",
      "[LOG 20200511-10:27:24] epoch: 0, batch: 7116 train-loss: 1.2889811992645264\n",
      "[LOG 20200511-10:27:24] epoch: 0, batch: 7117 train-loss: 1.8418556451797485\n",
      "[LOG 20200511-10:27:24] epoch: 0, batch: 7118 train-loss: 1.0240744352340698\n",
      "[LOG 20200511-10:27:25] epoch: 0, batch: 7119 train-loss: 1.8212474584579468\n",
      "[LOG 20200511-10:27:25] epoch: 0, batch: 7120 train-loss: 1.5095547437667847\n",
      "[LOG 20200511-10:27:25] epoch: 0, batch: 7121 train-loss: 0.6550347805023193\n",
      "[LOG 20200511-10:27:25] epoch: 0, batch: 7122 train-loss: 1.1277834177017212\n",
      "[LOG 20200511-10:27:25] epoch: 0, batch: 7123 train-loss: 0.37960946559906006\n",
      "[LOG 20200511-10:27:25] epoch: 0, batch: 7124 train-loss: 1.3501958847045898\n",
      "[LOG 20200511-10:27:25] epoch: 0, batch: 7125 train-loss: 1.9885278940200806\n",
      "[LOG 20200511-10:27:25] epoch: 0, batch: 7126 train-loss: 1.1845359802246094\n",
      "[LOG 20200511-10:27:26] epoch: 0, batch: 7127 train-loss: 0.856695294380188\n",
      "[LOG 20200511-10:27:26] epoch: 0, batch: 7128 train-loss: 1.4393856525421143\n",
      "[LOG 20200511-10:27:26] epoch: 0, batch: 7129 train-loss: 1.28090500831604\n",
      "[LOG 20200511-10:27:26] epoch: 0, batch: 7130 train-loss: 0.943227231502533\n",
      "[LOG 20200511-10:27:26] epoch: 0, batch: 7131 train-loss: 0.6276608109474182\n",
      "[LOG 20200511-10:27:26] epoch: 0, batch: 7132 train-loss: 2.2359137535095215\n",
      "[LOG 20200511-10:27:26] epoch: 0, batch: 7133 train-loss: 1.3617527484893799\n",
      "[LOG 20200511-10:27:27] epoch: 0, batch: 7134 train-loss: 0.8066501021385193\n",
      "[LOG 20200511-10:27:27] epoch: 0, batch: 7135 train-loss: 1.2436388731002808\n",
      "[LOG 20200511-10:27:27] epoch: 0, batch: 7136 train-loss: 2.8258142471313477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:27:27] epoch: 0, batch: 7137 train-loss: 1.105757713317871\n",
      "[LOG 20200511-10:27:27] epoch: 0, batch: 7138 train-loss: 1.7107142210006714\n",
      "[LOG 20200511-10:27:27] epoch: 0, batch: 7139 train-loss: 0.8820376396179199\n",
      "[LOG 20200511-10:27:27] epoch: 0, batch: 7140 train-loss: 1.1729836463928223\n",
      "[LOG 20200511-10:27:27] epoch: 0, batch: 7141 train-loss: 1.5623626708984375\n",
      "[LOG 20200511-10:27:28] epoch: 0, batch: 7142 train-loss: 1.754351258277893\n",
      "[LOG 20200511-10:27:28] epoch: 0, batch: 7143 train-loss: 1.4249330759048462\n",
      "[LOG 20200511-10:27:28] epoch: 0, batch: 7144 train-loss: 1.660017490386963\n",
      "[LOG 20200511-10:27:28] epoch: 0, batch: 7145 train-loss: 1.2041929960250854\n",
      "[LOG 20200511-10:27:28] epoch: 0, batch: 7146 train-loss: 1.4300756454467773\n",
      "[LOG 20200511-10:27:28] epoch: 0, batch: 7147 train-loss: 1.9779517650604248\n",
      "[LOG 20200511-10:27:28] epoch: 0, batch: 7148 train-loss: 1.101884365081787\n",
      "[LOG 20200511-10:27:28] epoch: 0, batch: 7149 train-loss: 0.3821389973163605\n",
      "[LOG 20200511-10:27:29] epoch: 0, batch: 7150 train-loss: 1.649144172668457\n",
      "[LOG 20200511-10:27:29] epoch: 0, batch: 7151 train-loss: 0.753408670425415\n",
      "[LOG 20200511-10:27:29] epoch: 0, batch: 7152 train-loss: 1.4719319343566895\n",
      "[LOG 20200511-10:27:29] epoch: 0, batch: 7153 train-loss: 1.5492171049118042\n",
      "[LOG 20200511-10:27:29] epoch: 0, batch: 7154 train-loss: 1.287261962890625\n",
      "[LOG 20200511-10:27:29] epoch: 0, batch: 7155 train-loss: 1.1530004739761353\n",
      "[LOG 20200511-10:27:29] epoch: 0, batch: 7156 train-loss: 0.8023003339767456\n",
      "[LOG 20200511-10:27:29] epoch: 0, batch: 7157 train-loss: 0.8938201665878296\n",
      "[LOG 20200511-10:27:30] epoch: 0, batch: 7158 train-loss: 2.610044479370117\n",
      "[LOG 20200511-10:27:30] epoch: 0, batch: 7159 train-loss: 2.0415828227996826\n",
      "[LOG 20200511-10:27:30] epoch: 0, batch: 7160 train-loss: 0.4678417444229126\n",
      "[LOG 20200511-10:27:30] epoch: 0, batch: 7161 train-loss: 1.415793776512146\n",
      "[LOG 20200511-10:27:30] epoch: 0, batch: 7162 train-loss: 0.6297469735145569\n",
      "[LOG 20200511-10:27:30] epoch: 0, batch: 7163 train-loss: 1.7166725397109985\n",
      "[LOG 20200511-10:27:30] epoch: 0, batch: 7164 train-loss: 1.631920576095581\n",
      "[LOG 20200511-10:27:30] epoch: 0, batch: 7165 train-loss: 0.8394808173179626\n",
      "[LOG 20200511-10:27:30] epoch: 0, batch: 7166 train-loss: 0.593684196472168\n",
      "[LOG 20200511-10:27:31] epoch: 0, batch: 7167 train-loss: 0.9733728766441345\n",
      "[LOG 20200511-10:27:31] epoch: 0, batch: 7168 train-loss: 1.1127415895462036\n",
      "[LOG 20200511-10:27:31] epoch: 0, batch: 7169 train-loss: 0.9998378157615662\n",
      "[LOG 20200511-10:27:31] epoch: 0, batch: 7170 train-loss: 0.5633445978164673\n",
      "[LOG 20200511-10:27:31] epoch: 0, batch: 7171 train-loss: 1.6445143222808838\n",
      "[LOG 20200511-10:27:31] epoch: 0, batch: 7172 train-loss: 1.511368989944458\n",
      "[LOG 20200511-10:27:31] epoch: 0, batch: 7173 train-loss: 1.269327163696289\n",
      "[LOG 20200511-10:27:31] epoch: 0, batch: 7174 train-loss: 2.087848663330078\n",
      "[LOG 20200511-10:27:31] epoch: 0, batch: 7175 train-loss: 1.8772119283676147\n",
      "[LOG 20200511-10:27:32] epoch: 0, batch: 7176 train-loss: 0.48912298679351807\n",
      "[LOG 20200511-10:27:32] epoch: 0, batch: 7177 train-loss: 1.3890514373779297\n",
      "[LOG 20200511-10:27:32] epoch: 0, batch: 7178 train-loss: 2.1922755241394043\n",
      "[LOG 20200511-10:27:32] epoch: 0, batch: 7179 train-loss: 2.0466079711914062\n",
      "[LOG 20200511-10:27:32] epoch: 0, batch: 7180 train-loss: 1.8548407554626465\n",
      "[LOG 20200511-10:27:32] epoch: 0, batch: 7181 train-loss: 1.186765193939209\n",
      "[LOG 20200511-10:27:32] epoch: 0, batch: 7182 train-loss: 1.2270150184631348\n",
      "[LOG 20200511-10:27:32] epoch: 0, batch: 7183 train-loss: 2.378838539123535\n",
      "[LOG 20200511-10:27:32] epoch: 0, batch: 7184 train-loss: 1.7822518348693848\n",
      "[LOG 20200511-10:27:33] epoch: 0, batch: 7185 train-loss: 1.0244981050491333\n",
      "[LOG 20200511-10:27:33] epoch: 0, batch: 7186 train-loss: 1.3769961595535278\n",
      "[LOG 20200511-10:27:33] epoch: 0, batch: 7187 train-loss: 0.8245480060577393\n",
      "[LOG 20200511-10:27:33] epoch: 0, batch: 7188 train-loss: 2.8138608932495117\n",
      "[LOG 20200511-10:27:33] epoch: 0, batch: 7189 train-loss: 1.743436336517334\n",
      "[LOG 20200511-10:27:33] epoch: 0, batch: 7190 train-loss: 1.530439853668213\n",
      "[LOG 20200511-10:27:33] epoch: 0, batch: 7191 train-loss: 1.018835425376892\n",
      "[LOG 20200511-10:27:33] epoch: 0, batch: 7192 train-loss: 1.0910725593566895\n",
      "[LOG 20200511-10:27:33] epoch: 0, batch: 7193 train-loss: 1.657102108001709\n",
      "[LOG 20200511-10:27:34] epoch: 0, batch: 7194 train-loss: 1.6194548606872559\n",
      "[LOG 20200511-10:27:34] epoch: 0, batch: 7195 train-loss: 1.49185311794281\n",
      "[LOG 20200511-10:27:34] epoch: 0, batch: 7196 train-loss: 1.4776335954666138\n",
      "[LOG 20200511-10:27:34] epoch: 0, batch: 7197 train-loss: 1.631906270980835\n",
      "[LOG 20200511-10:27:34] epoch: 0, batch: 7198 train-loss: 1.3223005533218384\n",
      "[LOG 20200511-10:27:34] epoch: 0, batch: 7199 train-loss: 0.970308244228363\n",
      "[LOG 20200511-10:27:34] epoch: 0, batch: 7200 train-loss: 1.3857815265655518\n",
      "[LOG 20200511-10:27:34] epoch: 0, batch: 7201 train-loss: 1.2108988761901855\n",
      "[LOG 20200511-10:27:34] epoch: 0, batch: 7202 train-loss: 0.9129005670547485\n",
      "[LOG 20200511-10:27:35] epoch: 0, batch: 7203 train-loss: 0.7832770347595215\n",
      "[LOG 20200511-10:27:35] epoch: 0, batch: 7204 train-loss: 0.9915289878845215\n",
      "[LOG 20200511-10:27:35] epoch: 0, batch: 7205 train-loss: 0.9657580852508545\n",
      "[LOG 20200511-10:27:35] epoch: 0, batch: 7206 train-loss: 1.8259837627410889\n",
      "[LOG 20200511-10:27:35] epoch: 0, batch: 7207 train-loss: 1.3059461116790771\n",
      "[LOG 20200511-10:27:35] epoch: 0, batch: 7208 train-loss: 1.3344388008117676\n",
      "[LOG 20200511-10:27:35] epoch: 0, batch: 7209 train-loss: 1.4125127792358398\n",
      "[LOG 20200511-10:27:35] epoch: 0, batch: 7210 train-loss: 1.1109498739242554\n",
      "[LOG 20200511-10:27:36] epoch: 0, batch: 7211 train-loss: 2.052248239517212\n",
      "[LOG 20200511-10:27:36] epoch: 0, batch: 7212 train-loss: 0.8479644060134888\n",
      "[LOG 20200511-10:27:36] epoch: 0, batch: 7213 train-loss: 2.523542881011963\n",
      "[LOG 20200511-10:27:36] epoch: 0, batch: 7214 train-loss: 1.3189349174499512\n",
      "[LOG 20200511-10:27:36] epoch: 0, batch: 7215 train-loss: 0.9968315958976746\n",
      "[LOG 20200511-10:27:36] epoch: 0, batch: 7216 train-loss: 0.5686871409416199\n",
      "[LOG 20200511-10:27:36] epoch: 0, batch: 7217 train-loss: 0.5662735104560852\n",
      "[LOG 20200511-10:27:36] epoch: 0, batch: 7218 train-loss: 1.0205414295196533\n",
      "[LOG 20200511-10:27:36] epoch: 0, batch: 7219 train-loss: 0.9481582641601562\n",
      "[LOG 20200511-10:27:37] epoch: 0, batch: 7220 train-loss: 1.0823360681533813\n",
      "[LOG 20200511-10:27:37] epoch: 0, batch: 7221 train-loss: 1.2346071004867554\n",
      "[LOG 20200511-10:27:37] epoch: 0, batch: 7222 train-loss: 0.44472551345825195\n",
      "[LOG 20200511-10:27:37] epoch: 0, batch: 7223 train-loss: 0.9909873008728027\n",
      "[LOG 20200511-10:27:37] epoch: 0, batch: 7224 train-loss: 0.6988784074783325\n",
      "[LOG 20200511-10:27:37] epoch: 0, batch: 7225 train-loss: 1.2166435718536377\n",
      "[LOG 20200511-10:27:37] epoch: 0, batch: 7226 train-loss: 1.5299025774002075\n",
      "[LOG 20200511-10:27:37] epoch: 0, batch: 7227 train-loss: 0.7553769946098328\n",
      "[LOG 20200511-10:27:38] epoch: 0, batch: 7228 train-loss: 1.689134955406189\n",
      "[LOG 20200511-10:27:38] epoch: 0, batch: 7229 train-loss: 1.0342684984207153\n",
      "[LOG 20200511-10:27:38] epoch: 0, batch: 7230 train-loss: 0.801279604434967\n",
      "[LOG 20200511-10:27:38] epoch: 0, batch: 7231 train-loss: 0.9771972894668579\n",
      "[LOG 20200511-10:27:38] epoch: 0, batch: 7232 train-loss: 1.737667202949524\n",
      "[LOG 20200511-10:27:38] epoch: 0, batch: 7233 train-loss: 1.1353306770324707\n",
      "[LOG 20200511-10:27:38] epoch: 0, batch: 7234 train-loss: 2.0280914306640625\n",
      "[LOG 20200511-10:27:38] epoch: 0, batch: 7235 train-loss: 0.7468867897987366\n",
      "[LOG 20200511-10:27:38] epoch: 0, batch: 7236 train-loss: 0.949882984161377\n",
      "[LOG 20200511-10:27:39] epoch: 0, batch: 7237 train-loss: 0.6125251054763794\n",
      "[LOG 20200511-10:27:39] epoch: 0, batch: 7238 train-loss: 0.8572792410850525\n",
      "[LOG 20200511-10:27:39] epoch: 0, batch: 7239 train-loss: 0.7455214262008667\n",
      "[LOG 20200511-10:27:39] epoch: 0, batch: 7240 train-loss: 2.0409796237945557\n",
      "[LOG 20200511-10:27:39] epoch: 0, batch: 7241 train-loss: 2.04412579536438\n",
      "[LOG 20200511-10:27:39] epoch: 0, batch: 7242 train-loss: 0.6419686675071716\n",
      "[LOG 20200511-10:27:39] epoch: 0, batch: 7243 train-loss: 1.7211637496948242\n",
      "[LOG 20200511-10:27:40] epoch: 0, batch: 7244 train-loss: 1.8745520114898682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:27:40] epoch: 0, batch: 7245 train-loss: 1.2469055652618408\n",
      "[LOG 20200511-10:27:40] epoch: 0, batch: 7246 train-loss: 2.3448386192321777\n",
      "[LOG 20200511-10:27:40] epoch: 0, batch: 7247 train-loss: 0.5089052319526672\n",
      "[LOG 20200511-10:27:40] epoch: 0, batch: 7248 train-loss: 0.7118095755577087\n",
      "[LOG 20200511-10:27:40] epoch: 0, batch: 7249 train-loss: 1.6616384983062744\n",
      "[LOG 20200511-10:27:40] epoch: 0, batch: 7250 train-loss: 1.7692465782165527\n",
      "[LOG 20200511-10:27:40] epoch: 0, batch: 7251 train-loss: 1.1287809610366821\n",
      "[LOG 20200511-10:27:40] epoch: 0, batch: 7252 train-loss: 1.671992540359497\n",
      "[LOG 20200511-10:27:41] epoch: 0, batch: 7253 train-loss: 1.386812686920166\n",
      "[LOG 20200511-10:27:41] epoch: 0, batch: 7254 train-loss: 0.7046157717704773\n",
      "[LOG 20200511-10:27:41] epoch: 0, batch: 7255 train-loss: 0.9653705358505249\n",
      "[LOG 20200511-10:27:41] epoch: 0, batch: 7256 train-loss: 0.7506729364395142\n",
      "[LOG 20200511-10:27:41] epoch: 0, batch: 7257 train-loss: 1.1605656147003174\n",
      "[LOG 20200511-10:27:41] epoch: 0, batch: 7258 train-loss: 1.1278380155563354\n",
      "[LOG 20200511-10:27:41] epoch: 0, batch: 7259 train-loss: 1.7633450031280518\n",
      "[LOG 20200511-10:27:41] epoch: 0, batch: 7260 train-loss: 1.8754792213439941\n",
      "[LOG 20200511-10:27:41] epoch: 0, batch: 7261 train-loss: 1.4312843084335327\n",
      "[LOG 20200511-10:27:42] epoch: 0, batch: 7262 train-loss: 1.4980292320251465\n",
      "[LOG 20200511-10:27:42] epoch: 0, batch: 7263 train-loss: 1.9007537364959717\n",
      "[LOG 20200511-10:27:42] epoch: 0, batch: 7264 train-loss: 0.7658794522285461\n",
      "[LOG 20200511-10:27:42] epoch: 0, batch: 7265 train-loss: 1.3739595413208008\n",
      "[LOG 20200511-10:27:42] epoch: 0, batch: 7266 train-loss: 1.027906894683838\n",
      "[LOG 20200511-10:27:42] epoch: 0, batch: 7267 train-loss: 1.719367265701294\n",
      "[LOG 20200511-10:27:42] epoch: 0, batch: 7268 train-loss: 1.4382535219192505\n",
      "[LOG 20200511-10:27:42] epoch: 0, batch: 7269 train-loss: 1.9923040866851807\n",
      "[LOG 20200511-10:27:42] epoch: 0, batch: 7270 train-loss: 1.7896827459335327\n",
      "[LOG 20200511-10:27:43] epoch: 0, batch: 7271 train-loss: 2.1756157875061035\n",
      "[LOG 20200511-10:27:43] epoch: 0, batch: 7272 train-loss: 0.6447626352310181\n",
      "[LOG 20200511-10:27:43] epoch: 0, batch: 7273 train-loss: 1.1193993091583252\n",
      "[LOG 20200511-10:27:43] epoch: 0, batch: 7274 train-loss: 2.4654362201690674\n",
      "[LOG 20200511-10:27:43] epoch: 0, batch: 7275 train-loss: 1.672621488571167\n",
      "[LOG 20200511-10:27:43] epoch: 0, batch: 7276 train-loss: 1.033994436264038\n",
      "[LOG 20200511-10:27:43] epoch: 0, batch: 7277 train-loss: 1.0573010444641113\n",
      "[LOG 20200511-10:27:43] epoch: 0, batch: 7278 train-loss: 0.7701144218444824\n",
      "[LOG 20200511-10:27:43] epoch: 0, batch: 7279 train-loss: 2.461644411087036\n",
      "[LOG 20200511-10:27:44] epoch: 0, batch: 7280 train-loss: 1.4385995864868164\n",
      "[LOG 20200511-10:27:44] epoch: 0, batch: 7281 train-loss: 0.516065239906311\n",
      "[LOG 20200511-10:27:44] epoch: 0, batch: 7282 train-loss: 1.565853476524353\n",
      "[LOG 20200511-10:27:44] epoch: 0, batch: 7283 train-loss: 1.0746839046478271\n",
      "[LOG 20200511-10:27:44] epoch: 0, batch: 7284 train-loss: 0.9279756546020508\n",
      "[LOG 20200511-10:27:44] epoch: 0, batch: 7285 train-loss: 1.1069151163101196\n",
      "[LOG 20200511-10:27:44] epoch: 0, batch: 7286 train-loss: 1.4470784664154053\n",
      "[LOG 20200511-10:27:44] epoch: 0, batch: 7287 train-loss: 1.9132448434829712\n",
      "[LOG 20200511-10:27:44] epoch: 0, batch: 7288 train-loss: 1.0245981216430664\n",
      "[LOG 20200511-10:27:45] epoch: 0, batch: 7289 train-loss: 0.6856826543807983\n",
      "[LOG 20200511-10:27:45] epoch: 0, batch: 7290 train-loss: 1.0990488529205322\n",
      "[LOG 20200511-10:27:45] epoch: 0, batch: 7291 train-loss: 1.0754786729812622\n",
      "[LOG 20200511-10:27:45] epoch: 0, batch: 7292 train-loss: 1.5181970596313477\n",
      "[LOG 20200511-10:27:45] epoch: 0, batch: 7293 train-loss: 0.7895704507827759\n",
      "[LOG 20200511-10:27:45] epoch: 0, batch: 7294 train-loss: 0.5522481799125671\n",
      "[LOG 20200511-10:27:45] epoch: 0, batch: 7295 train-loss: 0.9599297046661377\n",
      "[LOG 20200511-10:27:45] epoch: 0, batch: 7296 train-loss: 1.8687903881072998\n",
      "[LOG 20200511-10:27:46] epoch: 0, batch: 7297 train-loss: 1.3634896278381348\n",
      "[LOG 20200511-10:27:46] epoch: 0, batch: 7298 train-loss: 1.0451347827911377\n",
      "[LOG 20200511-10:27:46] epoch: 0, batch: 7299 train-loss: 2.0215654373168945\n",
      "[LOG 20200511-10:27:46] epoch: 0, batch: 7300 train-loss: 1.0395684242248535\n",
      "[LOG 20200511-10:27:46] epoch: 0, batch: 7301 train-loss: 1.1842689514160156\n",
      "[LOG 20200511-10:27:46] epoch: 0, batch: 7302 train-loss: 0.7377557754516602\n",
      "[LOG 20200511-10:27:46] epoch: 0, batch: 7303 train-loss: 1.8032821416854858\n",
      "[LOG 20200511-10:27:46] epoch: 0, batch: 7304 train-loss: 1.0623950958251953\n",
      "[LOG 20200511-10:27:47] epoch: 0, batch: 7305 train-loss: 1.3895052671432495\n",
      "[LOG 20200511-10:27:47] epoch: 0, batch: 7306 train-loss: 1.1095912456512451\n",
      "[LOG 20200511-10:27:47] epoch: 0, batch: 7307 train-loss: 2.020641565322876\n",
      "[LOG 20200511-10:27:47] epoch: 0, batch: 7308 train-loss: 0.4699535369873047\n",
      "[LOG 20200511-10:27:47] epoch: 0, batch: 7309 train-loss: 1.0042082071304321\n",
      "[LOG 20200511-10:27:47] epoch: 0, batch: 7310 train-loss: 0.6874487996101379\n",
      "[LOG 20200511-10:27:47] epoch: 0, batch: 7311 train-loss: 1.0887593030929565\n",
      "[LOG 20200511-10:27:47] epoch: 0, batch: 7312 train-loss: 0.8779116868972778\n",
      "[LOG 20200511-10:27:47] epoch: 0, batch: 7313 train-loss: 1.579392433166504\n",
      "[LOG 20200511-10:27:48] epoch: 0, batch: 7314 train-loss: 1.2262828350067139\n",
      "[LOG 20200511-10:27:48] epoch: 0, batch: 7315 train-loss: 0.7998147010803223\n",
      "[LOG 20200511-10:27:48] epoch: 0, batch: 7316 train-loss: 2.0672144889831543\n",
      "[LOG 20200511-10:27:48] epoch: 0, batch: 7317 train-loss: 1.1764476299285889\n",
      "[LOG 20200511-10:27:48] epoch: 0, batch: 7318 train-loss: 0.9075020551681519\n",
      "[LOG 20200511-10:27:48] epoch: 0, batch: 7319 train-loss: 0.3490235209465027\n",
      "[LOG 20200511-10:27:48] epoch: 0, batch: 7320 train-loss: 0.7653018236160278\n",
      "[LOG 20200511-10:27:48] epoch: 0, batch: 7321 train-loss: 0.2760196626186371\n",
      "[LOG 20200511-10:27:49] epoch: 0, batch: 7322 train-loss: 2.3379387855529785\n",
      "[LOG 20200511-10:27:49] epoch: 0, batch: 7323 train-loss: 1.6940135955810547\n",
      "[LOG 20200511-10:27:49] epoch: 0, batch: 7324 train-loss: 1.7014939785003662\n",
      "[LOG 20200511-10:27:49] epoch: 0, batch: 7325 train-loss: 1.0583646297454834\n",
      "[LOG 20200511-10:27:49] epoch: 0, batch: 7326 train-loss: 1.7140864133834839\n",
      "[LOG 20200511-10:27:49] epoch: 0, batch: 7327 train-loss: 1.8184818029403687\n",
      "[LOG 20200511-10:27:49] epoch: 0, batch: 7328 train-loss: 0.8810114860534668\n",
      "[LOG 20200511-10:27:49] epoch: 0, batch: 7329 train-loss: 0.24811884760856628\n",
      "[LOG 20200511-10:27:49] epoch: 0, batch: 7330 train-loss: 1.3892611265182495\n",
      "[LOG 20200511-10:27:50] epoch: 0, batch: 7331 train-loss: 1.8881072998046875\n",
      "[LOG 20200511-10:27:50] epoch: 0, batch: 7332 train-loss: 0.8358461856842041\n",
      "[LOG 20200511-10:27:50] epoch: 0, batch: 7333 train-loss: 1.6180003881454468\n",
      "[LOG 20200511-10:27:50] epoch: 0, batch: 7334 train-loss: 0.6182174682617188\n",
      "[LOG 20200511-10:27:50] epoch: 0, batch: 7335 train-loss: 1.1667152643203735\n",
      "[LOG 20200511-10:27:50] epoch: 0, batch: 7336 train-loss: 1.218238115310669\n",
      "[LOG 20200511-10:27:50] epoch: 0, batch: 7337 train-loss: 0.9195767641067505\n",
      "[LOG 20200511-10:27:50] epoch: 0, batch: 7338 train-loss: 1.0302026271820068\n",
      "[LOG 20200511-10:27:51] epoch: 0, batch: 7339 train-loss: 0.6735527515411377\n",
      "[LOG 20200511-10:27:51] epoch: 0, batch: 7340 train-loss: 1.0059908628463745\n",
      "[LOG 20200511-10:27:51] epoch: 0, batch: 7341 train-loss: 0.5065897703170776\n",
      "[LOG 20200511-10:27:51] epoch: 0, batch: 7342 train-loss: 1.1096138954162598\n",
      "[LOG 20200511-10:27:51] epoch: 0, batch: 7343 train-loss: 1.0206449031829834\n",
      "[LOG 20200511-10:27:51] epoch: 0, batch: 7344 train-loss: 0.7921313047409058\n",
      "[LOG 20200511-10:27:51] epoch: 0, batch: 7345 train-loss: 1.0853372812271118\n",
      "[LOG 20200511-10:27:51] epoch: 0, batch: 7346 train-loss: 0.458959698677063\n",
      "[LOG 20200511-10:27:52] epoch: 0, batch: 7347 train-loss: 1.242158055305481\n",
      "[LOG 20200511-10:27:52] epoch: 0, batch: 7348 train-loss: 1.277724027633667\n",
      "[LOG 20200511-10:27:52] epoch: 0, batch: 7349 train-loss: 1.4975711107254028\n",
      "[LOG 20200511-10:27:52] epoch: 0, batch: 7350 train-loss: 2.100025177001953\n",
      "[LOG 20200511-10:27:52] epoch: 0, batch: 7351 train-loss: 0.7873872518539429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:27:52] epoch: 0, batch: 7352 train-loss: 2.1535286903381348\n",
      "[LOG 20200511-10:27:52] epoch: 0, batch: 7353 train-loss: 1.1853837966918945\n",
      "[LOG 20200511-10:27:52] epoch: 0, batch: 7354 train-loss: 1.2482752799987793\n",
      "[LOG 20200511-10:27:52] epoch: 0, batch: 7355 train-loss: 2.071535348892212\n",
      "[LOG 20200511-10:27:53] epoch: 0, batch: 7356 train-loss: 0.5605331659317017\n",
      "[LOG 20200511-10:27:53] epoch: 0, batch: 7357 train-loss: 1.0949556827545166\n",
      "[LOG 20200511-10:27:53] epoch: 0, batch: 7358 train-loss: 1.7869499921798706\n",
      "[LOG 20200511-10:27:53] epoch: 0, batch: 7359 train-loss: 1.0268049240112305\n",
      "[LOG 20200511-10:27:53] epoch: 0, batch: 7360 train-loss: 0.7953566312789917\n",
      "[LOG 20200511-10:27:53] epoch: 0, batch: 7361 train-loss: 0.9964480400085449\n",
      "[LOG 20200511-10:27:53] epoch: 0, batch: 7362 train-loss: 0.8276673555374146\n",
      "[LOG 20200511-10:27:53] epoch: 0, batch: 7363 train-loss: 1.0304393768310547\n",
      "[LOG 20200511-10:27:53] epoch: 0, batch: 7364 train-loss: 1.300194263458252\n",
      "[LOG 20200511-10:27:54] epoch: 0, batch: 7365 train-loss: 1.0135657787322998\n",
      "[LOG 20200511-10:27:54] epoch: 0, batch: 7366 train-loss: 1.525284767150879\n",
      "[LOG 20200511-10:27:54] epoch: 0, batch: 7367 train-loss: 1.1574618816375732\n",
      "[LOG 20200511-10:27:54] epoch: 0, batch: 7368 train-loss: 1.3899807929992676\n",
      "[LOG 20200511-10:27:54] epoch: 0, batch: 7369 train-loss: 0.5841071009635925\n",
      "[LOG 20200511-10:27:54] epoch: 0, batch: 7370 train-loss: 0.554142951965332\n",
      "[LOG 20200511-10:27:54] epoch: 0, batch: 7371 train-loss: 0.726496696472168\n",
      "[LOG 20200511-10:27:54] epoch: 0, batch: 7372 train-loss: 0.46927666664123535\n",
      "[LOG 20200511-10:27:54] epoch: 0, batch: 7373 train-loss: 0.3954029977321625\n",
      "[LOG 20200511-10:27:55] epoch: 0, batch: 7374 train-loss: 1.5821514129638672\n",
      "[LOG 20200511-10:27:55] epoch: 0, batch: 7375 train-loss: 1.84686279296875\n",
      "[LOG 20200511-10:27:55] epoch: 0, batch: 7376 train-loss: 0.9400257468223572\n",
      "[LOG 20200511-10:27:55] epoch: 0, batch: 7377 train-loss: 1.2797226905822754\n",
      "[LOG 20200511-10:27:55] epoch: 0, batch: 7378 train-loss: 0.8873955011367798\n",
      "[LOG 20200511-10:27:55] epoch: 0, batch: 7379 train-loss: 0.9278117418289185\n",
      "[LOG 20200511-10:27:55] epoch: 0, batch: 7380 train-loss: 1.8187752962112427\n",
      "[LOG 20200511-10:27:55] epoch: 0, batch: 7381 train-loss: 0.8681138753890991\n",
      "[LOG 20200511-10:27:56] epoch: 0, batch: 7382 train-loss: 1.7922680377960205\n",
      "[LOG 20200511-10:27:56] epoch: 0, batch: 7383 train-loss: 1.0928875207901\n",
      "[LOG 20200511-10:27:56] epoch: 0, batch: 7384 train-loss: 2.0688514709472656\n",
      "[LOG 20200511-10:27:56] epoch: 0, batch: 7385 train-loss: 1.3925925493240356\n",
      "[LOG 20200511-10:27:56] epoch: 0, batch: 7386 train-loss: 1.3531315326690674\n",
      "[LOG 20200511-10:27:56] epoch: 0, batch: 7387 train-loss: 1.0758085250854492\n",
      "[LOG 20200511-10:27:56] epoch: 0, batch: 7388 train-loss: 1.57130765914917\n",
      "[LOG 20200511-10:27:56] epoch: 0, batch: 7389 train-loss: 1.1470861434936523\n",
      "[LOG 20200511-10:27:56] epoch: 0, batch: 7390 train-loss: 1.1348388195037842\n",
      "[LOG 20200511-10:27:57] epoch: 0, batch: 7391 train-loss: 1.5025554895401\n",
      "[LOG 20200511-10:27:57] epoch: 0, batch: 7392 train-loss: 1.0402905941009521\n",
      "[LOG 20200511-10:27:57] epoch: 0, batch: 7393 train-loss: 1.3372164964675903\n",
      "[LOG 20200511-10:27:57] epoch: 0, batch: 7394 train-loss: 1.299180507659912\n",
      "[LOG 20200511-10:27:57] epoch: 0, batch: 7395 train-loss: 0.6056221127510071\n",
      "[LOG 20200511-10:27:57] epoch: 0, batch: 7396 train-loss: 0.5050283670425415\n",
      "[LOG 20200511-10:27:57] epoch: 0, batch: 7397 train-loss: 0.8332498669624329\n",
      "[LOG 20200511-10:27:57] epoch: 0, batch: 7398 train-loss: 1.0285906791687012\n",
      "[LOG 20200511-10:27:58] epoch: 0, batch: 7399 train-loss: 0.45456236600875854\n",
      "[LOG 20200511-10:27:58] epoch: 0, batch: 7400 train-loss: 0.9709295034408569\n",
      "[LOG 20200511-10:27:58] epoch: 0, batch: 7401 train-loss: 0.9538639783859253\n",
      "[LOG 20200511-10:27:58] epoch: 0, batch: 7402 train-loss: 1.3557813167572021\n",
      "[LOG 20200511-10:27:58] epoch: 0, batch: 7403 train-loss: 2.0776491165161133\n",
      "[LOG 20200511-10:27:58] epoch: 0, batch: 7404 train-loss: 0.9215375185012817\n",
      "[LOG 20200511-10:27:58] epoch: 0, batch: 7405 train-loss: 1.9258092641830444\n",
      "[LOG 20200511-10:27:58] epoch: 0, batch: 7406 train-loss: 0.7707899212837219\n",
      "[LOG 20200511-10:27:58] epoch: 0, batch: 7407 train-loss: 0.6025846600532532\n",
      "[LOG 20200511-10:27:59] epoch: 0, batch: 7408 train-loss: 1.3678979873657227\n",
      "[LOG 20200511-10:27:59] epoch: 0, batch: 7409 train-loss: 2.0307395458221436\n",
      "[LOG 20200511-10:27:59] epoch: 0, batch: 7410 train-loss: 1.3811941146850586\n",
      "[LOG 20200511-10:27:59] epoch: 0, batch: 7411 train-loss: 0.7658745050430298\n",
      "[LOG 20200511-10:27:59] epoch: 0, batch: 7412 train-loss: 2.566497564315796\n",
      "[LOG 20200511-10:27:59] epoch: 0, batch: 7413 train-loss: 0.8790354132652283\n",
      "[LOG 20200511-10:27:59] epoch: 0, batch: 7414 train-loss: 0.5618753433227539\n",
      "[LOG 20200511-10:27:59] epoch: 0, batch: 7415 train-loss: 1.7716892957687378\n",
      "[LOG 20200511-10:27:59] epoch: 0, batch: 7416 train-loss: 0.7093884348869324\n",
      "[LOG 20200511-10:28:00] epoch: 0, batch: 7417 train-loss: 1.620682954788208\n",
      "[LOG 20200511-10:28:00] epoch: 0, batch: 7418 train-loss: 1.2840502262115479\n",
      "[LOG 20200511-10:28:00] epoch: 0, batch: 7419 train-loss: 1.643115520477295\n",
      "[LOG 20200511-10:28:00] epoch: 0, batch: 7420 train-loss: 1.4316896200180054\n",
      "[LOG 20200511-10:28:00] epoch: 0, batch: 7421 train-loss: 1.6075260639190674\n",
      "[LOG 20200511-10:28:00] epoch: 0, batch: 7422 train-loss: 2.4222795963287354\n",
      "[LOG 20200511-10:28:00] epoch: 0, batch: 7423 train-loss: 1.3686249256134033\n",
      "[LOG 20200511-10:28:00] epoch: 0, batch: 7424 train-loss: 1.5635738372802734\n",
      "[LOG 20200511-10:28:00] epoch: 0, batch: 7425 train-loss: 1.5066883563995361\n",
      "[LOG 20200511-10:28:01] epoch: 0, batch: 7426 train-loss: 1.3849232196807861\n",
      "[LOG 20200511-10:28:01] epoch: 0, batch: 7427 train-loss: 1.2794877290725708\n",
      "[LOG 20200511-10:28:01] epoch: 0, batch: 7428 train-loss: 1.4916499853134155\n",
      "[LOG 20200511-10:28:01] epoch: 0, batch: 7429 train-loss: 0.9940628409385681\n",
      "[LOG 20200511-10:28:01] epoch: 0, batch: 7430 train-loss: 0.7118085622787476\n",
      "[LOG 20200511-10:28:01] epoch: 0, batch: 7431 train-loss: 0.43028444051742554\n",
      "[LOG 20200511-10:28:01] epoch: 0, batch: 7432 train-loss: 2.190098285675049\n",
      "[LOG 20200511-10:28:02] epoch: 0, batch: 7433 train-loss: 1.7997591495513916\n",
      "[LOG 20200511-10:28:02] epoch: 0, batch: 7434 train-loss: 3.443681478500366\n",
      "[LOG 20200511-10:28:02] epoch: 0, batch: 7435 train-loss: 2.1995201110839844\n",
      "[LOG 20200511-10:28:02] epoch: 0, batch: 7436 train-loss: 0.7472031712532043\n",
      "[LOG 20200511-10:28:02] epoch: 0, batch: 7437 train-loss: 0.6635779738426208\n",
      "[LOG 20200511-10:28:02] epoch: 0, batch: 7438 train-loss: 2.012401580810547\n",
      "[LOG 20200511-10:28:02] epoch: 0, batch: 7439 train-loss: 0.5657013654708862\n",
      "[LOG 20200511-10:28:02] epoch: 0, batch: 7440 train-loss: 1.7118120193481445\n",
      "[LOG 20200511-10:28:03] epoch: 0, batch: 7441 train-loss: 1.580509901046753\n",
      "[LOG 20200511-10:28:03] epoch: 0, batch: 7442 train-loss: 0.8827529549598694\n",
      "[LOG 20200511-10:28:03] epoch: 0, batch: 7443 train-loss: 1.2274657487869263\n",
      "[LOG 20200511-10:28:03] epoch: 0, batch: 7444 train-loss: 2.427081823348999\n",
      "[LOG 20200511-10:28:03] epoch: 0, batch: 7445 train-loss: 0.990432620048523\n",
      "[LOG 20200511-10:28:03] epoch: 0, batch: 7446 train-loss: 1.181186556816101\n",
      "[LOG 20200511-10:28:03] epoch: 0, batch: 7447 train-loss: 1.8743977546691895\n",
      "[LOG 20200511-10:28:03] epoch: 0, batch: 7448 train-loss: 1.2800462245941162\n",
      "[LOG 20200511-10:28:04] epoch: 0, batch: 7449 train-loss: 0.40525752305984497\n",
      "[LOG 20200511-10:28:04] epoch: 0, batch: 7450 train-loss: 1.8266935348510742\n",
      "[LOG 20200511-10:28:04] epoch: 0, batch: 7451 train-loss: 0.6962029933929443\n",
      "[LOG 20200511-10:28:04] epoch: 0, batch: 7452 train-loss: 1.9234671592712402\n",
      "[LOG 20200511-10:28:04] epoch: 0, batch: 7453 train-loss: 1.5355161428451538\n",
      "[LOG 20200511-10:28:04] epoch: 0, batch: 7454 train-loss: 1.2188456058502197\n",
      "[LOG 20200511-10:28:04] epoch: 0, batch: 7455 train-loss: 1.3950527906417847\n",
      "[LOG 20200511-10:28:04] epoch: 0, batch: 7456 train-loss: 1.5093796253204346\n",
      "[LOG 20200511-10:28:04] epoch: 0, batch: 7457 train-loss: 0.6903399229049683\n",
      "[LOG 20200511-10:28:05] epoch: 0, batch: 7458 train-loss: 1.1717212200164795\n",
      "[LOG 20200511-10:28:05] epoch: 0, batch: 7459 train-loss: 0.8038650155067444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:28:05] epoch: 0, batch: 7460 train-loss: 1.236002802848816\n",
      "[LOG 20200511-10:28:05] epoch: 0, batch: 7461 train-loss: 1.7640407085418701\n",
      "[LOG 20200511-10:28:05] epoch: 0, batch: 7462 train-loss: 1.6946302652359009\n",
      "[LOG 20200511-10:28:05] epoch: 0, batch: 7463 train-loss: 1.4428024291992188\n",
      "[LOG 20200511-10:28:05] epoch: 0, batch: 7464 train-loss: 1.255828857421875\n",
      "[LOG 20200511-10:28:06] epoch: 0, batch: 7465 train-loss: 1.2060320377349854\n",
      "[LOG 20200511-10:28:06] epoch: 0, batch: 7466 train-loss: 1.3019040822982788\n",
      "[LOG 20200511-10:28:06] epoch: 0, batch: 7467 train-loss: 1.422672986984253\n",
      "[LOG 20200511-10:28:06] epoch: 0, batch: 7468 train-loss: 0.6604218482971191\n",
      "[LOG 20200511-10:28:06] epoch: 0, batch: 7469 train-loss: 1.2449634075164795\n",
      "[LOG 20200511-10:28:06] epoch: 0, batch: 7470 train-loss: 0.8594314455986023\n",
      "[LOG 20200511-10:28:06] epoch: 0, batch: 7471 train-loss: 0.6639794111251831\n",
      "[LOG 20200511-10:28:06] epoch: 0, batch: 7472 train-loss: 1.3294529914855957\n",
      "[LOG 20200511-10:28:07] epoch: 0, batch: 7473 train-loss: 1.4487401247024536\n",
      "[LOG 20200511-10:28:07] epoch: 0, batch: 7474 train-loss: 2.170664072036743\n",
      "[LOG 20200511-10:28:07] epoch: 0, batch: 7475 train-loss: 2.120779275894165\n",
      "[LOG 20200511-10:28:07] epoch: 0, batch: 7476 train-loss: 1.1705410480499268\n",
      "[LOG 20200511-10:28:07] epoch: 0, batch: 7477 train-loss: 1.9157425165176392\n",
      "[LOG 20200511-10:28:07] epoch: 0, batch: 7478 train-loss: 0.5201082825660706\n",
      "[LOG 20200511-10:28:07] epoch: 0, batch: 7479 train-loss: 1.0426256656646729\n",
      "[LOG 20200511-10:28:07] epoch: 0, batch: 7480 train-loss: 0.33016684651374817\n",
      "[LOG 20200511-10:28:07] epoch: 0, batch: 7481 train-loss: 1.5233745574951172\n",
      "[LOG 20200511-10:28:08] epoch: 0, batch: 7482 train-loss: 2.231018304824829\n",
      "[LOG 20200511-10:28:08] epoch: 0, batch: 7483 train-loss: 1.0077348947525024\n",
      "[LOG 20200511-10:28:08] epoch: 0, batch: 7484 train-loss: 1.0188289880752563\n",
      "[LOG 20200511-10:28:08] epoch: 0, batch: 7485 train-loss: 1.1021690368652344\n",
      "[LOG 20200511-10:28:08] epoch: 0, batch: 7486 train-loss: 0.9105209112167358\n",
      "[LOG 20200511-10:28:08] epoch: 0, batch: 7487 train-loss: 1.2085371017456055\n",
      "[LOG 20200511-10:28:08] epoch: 0, batch: 7488 train-loss: 0.7347290515899658\n",
      "[LOG 20200511-10:28:08] epoch: 0, batch: 7489 train-loss: 0.9279633164405823\n",
      "[LOG 20200511-10:28:09] epoch: 0, batch: 7490 train-loss: 1.0471343994140625\n",
      "[LOG 20200511-10:28:09] epoch: 0, batch: 7491 train-loss: 1.7416231632232666\n",
      "[LOG 20200511-10:28:09] epoch: 0, batch: 7492 train-loss: 0.9898222088813782\n",
      "[LOG 20200511-10:28:09] epoch: 0, batch: 7493 train-loss: 2.657562494277954\n",
      "[LOG 20200511-10:28:09] epoch: 0, batch: 7494 train-loss: 0.4825912117958069\n",
      "[LOG 20200511-10:28:09] epoch: 0, batch: 7495 train-loss: 1.8565385341644287\n",
      "[LOG 20200511-10:28:09] epoch: 0, batch: 7496 train-loss: 0.6001003980636597\n",
      "[LOG 20200511-10:28:10] epoch: 0, batch: 7497 train-loss: 1.9024322032928467\n",
      "[LOG 20200511-10:28:10] epoch: 0, batch: 7498 train-loss: 1.387431263923645\n",
      "[LOG 20200511-10:28:10] epoch: 0, batch: 7499 train-loss: 1.3548072576522827\n",
      "[LOG 20200511-10:28:10] epoch: 0, batch: 7500 train-loss: 0.8009704351425171\n",
      "[LOG 20200511-10:28:10] epoch: 0, batch: 7501 train-loss: 0.36151933670043945\n",
      "[LOG 20200511-10:28:10] epoch: 0, batch: 7502 train-loss: 1.965198040008545\n",
      "[LOG 20200511-10:28:10] epoch: 0, batch: 7503 train-loss: 1.7193613052368164\n",
      "[LOG 20200511-10:28:10] epoch: 0, batch: 7504 train-loss: 0.8540657162666321\n",
      "[LOG 20200511-10:28:11] epoch: 0, batch: 7505 train-loss: 2.336498737335205\n",
      "[LOG 20200511-10:28:11] epoch: 0, batch: 7506 train-loss: 1.2309424877166748\n",
      "[LOG 20200511-10:28:11] epoch: 0, batch: 7507 train-loss: 1.3379226922988892\n",
      "[LOG 20200511-10:28:11] epoch: 0, batch: 7508 train-loss: 1.571899652481079\n",
      "[LOG 20200511-10:28:11] epoch: 0, batch: 7509 train-loss: 2.5414557456970215\n",
      "[LOG 20200511-10:28:11] epoch: 0, batch: 7510 train-loss: 1.2214534282684326\n",
      "[LOG 20200511-10:28:11] epoch: 0, batch: 7511 train-loss: 1.313122272491455\n",
      "[LOG 20200511-10:28:11] epoch: 0, batch: 7512 train-loss: 0.7140157222747803\n",
      "[LOG 20200511-10:28:12] epoch: 0, batch: 7513 train-loss: 0.8920864462852478\n",
      "[LOG 20200511-10:28:12] epoch: 0, batch: 7514 train-loss: 1.0518543720245361\n",
      "[LOG 20200511-10:28:12] epoch: 0, batch: 7515 train-loss: 0.8494576811790466\n",
      "[LOG 20200511-10:28:12] epoch: 0, batch: 7516 train-loss: 0.8214470744132996\n",
      "[LOG 20200511-10:28:12] epoch: 0, batch: 7517 train-loss: 0.6989774703979492\n",
      "[LOG 20200511-10:28:12] epoch: 0, batch: 7518 train-loss: 1.2854573726654053\n",
      "[LOG 20200511-10:28:12] epoch: 0, batch: 7519 train-loss: 1.2229089736938477\n",
      "[LOG 20200511-10:28:12] epoch: 0, batch: 7520 train-loss: 1.2670948505401611\n",
      "[LOG 20200511-10:28:13] epoch: 0, batch: 7521 train-loss: 1.419967532157898\n",
      "[LOG 20200511-10:28:13] epoch: 0, batch: 7522 train-loss: 1.0830597877502441\n",
      "[LOG 20200511-10:28:13] epoch: 0, batch: 7523 train-loss: 0.7662731409072876\n",
      "[LOG 20200511-10:28:13] epoch: 0, batch: 7524 train-loss: 1.5046789646148682\n",
      "[LOG 20200511-10:28:13] epoch: 0, batch: 7525 train-loss: 1.440413475036621\n",
      "[LOG 20200511-10:28:13] epoch: 0, batch: 7526 train-loss: 1.0388972759246826\n",
      "[LOG 20200511-10:28:13] epoch: 0, batch: 7527 train-loss: 0.7053205966949463\n",
      "[LOG 20200511-10:28:14] epoch: 0, batch: 7528 train-loss: 0.8220792412757874\n",
      "[LOG 20200511-10:28:14] epoch: 0, batch: 7529 train-loss: 1.1722469329833984\n",
      "[LOG 20200511-10:28:14] epoch: 0, batch: 7530 train-loss: 1.868869423866272\n",
      "[LOG 20200511-10:28:14] epoch: 0, batch: 7531 train-loss: 1.373925805091858\n",
      "[LOG 20200511-10:28:14] epoch: 0, batch: 7532 train-loss: 1.8529094457626343\n",
      "[LOG 20200511-10:28:14] epoch: 0, batch: 7533 train-loss: 0.9795227646827698\n",
      "[LOG 20200511-10:28:14] epoch: 0, batch: 7534 train-loss: 0.7869671583175659\n",
      "[LOG 20200511-10:28:14] epoch: 0, batch: 7535 train-loss: 0.7651325464248657\n",
      "[LOG 20200511-10:28:15] epoch: 0, batch: 7536 train-loss: 0.3383559584617615\n",
      "[LOG 20200511-10:28:15] epoch: 0, batch: 7537 train-loss: 1.7556008100509644\n",
      "[LOG 20200511-10:28:15] epoch: 0, batch: 7538 train-loss: 0.9473413228988647\n",
      "[LOG 20200511-10:28:15] epoch: 0, batch: 7539 train-loss: 1.7640784978866577\n",
      "[LOG 20200511-10:28:15] epoch: 0, batch: 7540 train-loss: 1.0992882251739502\n",
      "[LOG 20200511-10:28:15] epoch: 0, batch: 7541 train-loss: 1.212905764579773\n",
      "[LOG 20200511-10:28:15] epoch: 0, batch: 7542 train-loss: 1.6709421873092651\n",
      "[LOG 20200511-10:28:15] epoch: 0, batch: 7543 train-loss: 0.6685305237770081\n",
      "[LOG 20200511-10:28:16] epoch: 0, batch: 7544 train-loss: 1.8753132820129395\n",
      "[LOG 20200511-10:28:16] epoch: 0, batch: 7545 train-loss: 0.4727136492729187\n",
      "[LOG 20200511-10:28:16] epoch: 0, batch: 7546 train-loss: 0.5995697975158691\n",
      "[LOG 20200511-10:28:16] epoch: 0, batch: 7547 train-loss: 0.7685810327529907\n",
      "[LOG 20200511-10:28:16] epoch: 0, batch: 7548 train-loss: 1.6433289051055908\n",
      "[LOG 20200511-10:28:16] epoch: 0, batch: 7549 train-loss: 1.4591890573501587\n",
      "[LOG 20200511-10:28:16] epoch: 0, batch: 7550 train-loss: 0.9516703486442566\n",
      "[LOG 20200511-10:28:16] epoch: 0, batch: 7551 train-loss: 0.41225647926330566\n",
      "[LOG 20200511-10:28:16] epoch: 0, batch: 7552 train-loss: 0.8662503957748413\n",
      "[LOG 20200511-10:28:17] epoch: 0, batch: 7553 train-loss: 0.8173847198486328\n",
      "[LOG 20200511-10:28:17] epoch: 0, batch: 7554 train-loss: 1.1296674013137817\n",
      "[LOG 20200511-10:28:17] epoch: 0, batch: 7555 train-loss: 0.8161193132400513\n",
      "[LOG 20200511-10:28:17] epoch: 0, batch: 7556 train-loss: 1.4880213737487793\n",
      "[LOG 20200511-10:28:17] epoch: 0, batch: 7557 train-loss: 1.165025234222412\n",
      "[LOG 20200511-10:28:17] epoch: 0, batch: 7558 train-loss: 1.0898700952529907\n",
      "[LOG 20200511-10:28:17] epoch: 0, batch: 7559 train-loss: 2.306593656539917\n",
      "[LOG 20200511-10:28:17] epoch: 0, batch: 7560 train-loss: 1.6350641250610352\n",
      "[LOG 20200511-10:28:18] epoch: 0, batch: 7561 train-loss: 2.1620917320251465\n",
      "[LOG 20200511-10:28:18] epoch: 0, batch: 7562 train-loss: 0.8729259967803955\n",
      "[LOG 20200511-10:28:18] epoch: 0, batch: 7563 train-loss: 0.40054070949554443\n",
      "[LOG 20200511-10:28:18] epoch: 0, batch: 7564 train-loss: 1.2983806133270264\n",
      "[LOG 20200511-10:28:18] epoch: 0, batch: 7565 train-loss: 1.4178301095962524\n",
      "[LOG 20200511-10:28:18] epoch: 0, batch: 7566 train-loss: 1.0123579502105713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:28:18] epoch: 0, batch: 7567 train-loss: 1.0182816982269287\n",
      "[LOG 20200511-10:28:18] epoch: 0, batch: 7568 train-loss: 1.6613423824310303\n",
      "[LOG 20200511-10:28:19] epoch: 0, batch: 7569 train-loss: 0.8734729290008545\n",
      "[LOG 20200511-10:28:19] epoch: 0, batch: 7570 train-loss: 1.1434617042541504\n",
      "[LOG 20200511-10:28:19] epoch: 0, batch: 7571 train-loss: 0.7260190844535828\n",
      "[LOG 20200511-10:28:19] epoch: 0, batch: 7572 train-loss: 1.294536828994751\n",
      "[LOG 20200511-10:28:19] epoch: 0, batch: 7573 train-loss: 2.2447640895843506\n",
      "[LOG 20200511-10:28:19] epoch: 0, batch: 7574 train-loss: 0.9183126091957092\n",
      "[LOG 20200511-10:28:19] epoch: 0, batch: 7575 train-loss: 1.4728403091430664\n",
      "[LOG 20200511-10:28:19] epoch: 0, batch: 7576 train-loss: 1.2594722509384155\n",
      "[LOG 20200511-10:28:20] epoch: 0, batch: 7577 train-loss: 0.35285860300064087\n",
      "[LOG 20200511-10:28:20] epoch: 0, batch: 7578 train-loss: 0.7651260495185852\n",
      "[LOG 20200511-10:28:20] epoch: 0, batch: 7579 train-loss: 1.2226446866989136\n",
      "[LOG 20200511-10:28:20] epoch: 0, batch: 7580 train-loss: 0.8915232419967651\n",
      "[LOG 20200511-10:28:20] epoch: 0, batch: 7581 train-loss: 0.801633894443512\n",
      "[LOG 20200511-10:28:20] epoch: 0, batch: 7582 train-loss: 1.287148118019104\n",
      "[LOG 20200511-10:28:20] epoch: 0, batch: 7583 train-loss: 1.3748048543930054\n",
      "[LOG 20200511-10:28:20] epoch: 0, batch: 7584 train-loss: 1.6887874603271484\n",
      "[LOG 20200511-10:28:21] epoch: 0, batch: 7585 train-loss: 1.0823965072631836\n",
      "[LOG 20200511-10:28:21] epoch: 0, batch: 7586 train-loss: 1.033099889755249\n",
      "[LOG 20200511-10:28:21] epoch: 0, batch: 7587 train-loss: 1.1782630681991577\n",
      "[LOG 20200511-10:28:21] epoch: 0, batch: 7588 train-loss: 0.7408628463745117\n",
      "[LOG 20200511-10:28:21] epoch: 0, batch: 7589 train-loss: 1.3732212781906128\n",
      "[LOG 20200511-10:28:21] epoch: 0, batch: 7590 train-loss: 1.228898286819458\n",
      "[LOG 20200511-10:28:21] epoch: 0, batch: 7591 train-loss: 1.5396051406860352\n",
      "[LOG 20200511-10:28:21] epoch: 0, batch: 7592 train-loss: 1.6013727188110352\n",
      "[LOG 20200511-10:28:22] epoch: 0, batch: 7593 train-loss: 1.8116364479064941\n",
      "[LOG 20200511-10:28:22] epoch: 0, batch: 7594 train-loss: 2.5141773223876953\n",
      "[LOG 20200511-10:28:22] epoch: 0, batch: 7595 train-loss: 0.8107803463935852\n",
      "[LOG 20200511-10:28:22] epoch: 0, batch: 7596 train-loss: 1.494436502456665\n",
      "[LOG 20200511-10:28:22] epoch: 0, batch: 7597 train-loss: 1.7168474197387695\n",
      "[LOG 20200511-10:28:22] epoch: 0, batch: 7598 train-loss: 2.412079334259033\n",
      "[LOG 20200511-10:28:22] epoch: 0, batch: 7599 train-loss: 0.5377705097198486\n",
      "[LOG 20200511-10:28:22] epoch: 0, batch: 7600 train-loss: 1.0710186958312988\n",
      "[LOG 20200511-10:28:23] epoch: 0, batch: 7601 train-loss: 0.6812942028045654\n",
      "[LOG 20200511-10:28:23] epoch: 0, batch: 7602 train-loss: 1.029568076133728\n",
      "[LOG 20200511-10:28:23] epoch: 0, batch: 7603 train-loss: 1.144217848777771\n",
      "[LOG 20200511-10:28:23] epoch: 0, batch: 7604 train-loss: 1.338202714920044\n",
      "[LOG 20200511-10:28:23] epoch: 0, batch: 7605 train-loss: 1.187406063079834\n",
      "[LOG 20200511-10:28:23] epoch: 0, batch: 7606 train-loss: 1.5917470455169678\n",
      "[LOG 20200511-10:28:23] epoch: 0, batch: 7607 train-loss: 1.3543322086334229\n",
      "[LOG 20200511-10:28:24] epoch: 0, batch: 7608 train-loss: 1.0499945878982544\n",
      "[LOG 20200511-10:28:24] epoch: 0, batch: 7609 train-loss: 0.8255459666252136\n",
      "[LOG 20200511-10:28:24] epoch: 0, batch: 7610 train-loss: 1.3820340633392334\n",
      "[LOG 20200511-10:28:24] epoch: 0, batch: 7611 train-loss: 0.9886530637741089\n",
      "[LOG 20200511-10:28:24] epoch: 0, batch: 7612 train-loss: 1.5456302165985107\n",
      "[LOG 20200511-10:28:24] epoch: 0, batch: 7613 train-loss: 2.064314365386963\n",
      "[LOG 20200511-10:28:24] epoch: 0, batch: 7614 train-loss: 1.298198938369751\n",
      "[LOG 20200511-10:28:24] epoch: 0, batch: 7615 train-loss: 0.9013475179672241\n",
      "[LOG 20200511-10:28:25] epoch: 0, batch: 7616 train-loss: 1.0941898822784424\n",
      "[LOG 20200511-10:28:25] epoch: 0, batch: 7617 train-loss: 1.3326059579849243\n",
      "[LOG 20200511-10:28:25] epoch: 0, batch: 7618 train-loss: 0.43974754214286804\n",
      "[LOG 20200511-10:28:25] epoch: 0, batch: 7619 train-loss: 0.8526107668876648\n",
      "[LOG 20200511-10:28:25] epoch: 0, batch: 7620 train-loss: 1.761206030845642\n",
      "[LOG 20200511-10:28:25] epoch: 0, batch: 7621 train-loss: 0.9754677414894104\n",
      "[LOG 20200511-10:28:25] epoch: 0, batch: 7622 train-loss: 0.6828381419181824\n",
      "[LOG 20200511-10:28:25] epoch: 0, batch: 7623 train-loss: 0.6843157410621643\n",
      "[LOG 20200511-10:28:26] epoch: 0, batch: 7624 train-loss: 1.3037700653076172\n",
      "[LOG 20200511-10:28:26] epoch: 0, batch: 7625 train-loss: 1.5773276090621948\n",
      "[LOG 20200511-10:28:26] epoch: 0, batch: 7626 train-loss: 0.7569471597671509\n",
      "[LOG 20200511-10:28:26] epoch: 0, batch: 7627 train-loss: 1.2849063873291016\n",
      "[LOG 20200511-10:28:26] epoch: 0, batch: 7628 train-loss: 0.5670077800750732\n",
      "[LOG 20200511-10:28:26] epoch: 0, batch: 7629 train-loss: 0.6151127219200134\n",
      "[LOG 20200511-10:28:26] epoch: 0, batch: 7630 train-loss: 1.1626335382461548\n",
      "[LOG 20200511-10:28:27] epoch: 0, batch: 7631 train-loss: 0.5856419801712036\n",
      "[LOG 20200511-10:28:27] epoch: 0, batch: 7632 train-loss: 0.916655957698822\n",
      "[LOG 20200511-10:28:27] epoch: 0, batch: 7633 train-loss: 1.7733006477355957\n",
      "[LOG 20200511-10:28:27] epoch: 0, batch: 7634 train-loss: 1.5570704936981201\n",
      "[LOG 20200511-10:28:27] epoch: 0, batch: 7635 train-loss: 1.7144726514816284\n",
      "[LOG 20200511-10:28:27] epoch: 0, batch: 7636 train-loss: 0.3457101285457611\n",
      "[LOG 20200511-10:28:27] epoch: 0, batch: 7637 train-loss: 0.8373147249221802\n",
      "[LOG 20200511-10:28:27] epoch: 0, batch: 7638 train-loss: 1.4989891052246094\n",
      "[LOG 20200511-10:28:28] epoch: 0, batch: 7639 train-loss: 0.854865312576294\n",
      "[LOG 20200511-10:28:28] epoch: 0, batch: 7640 train-loss: 1.7676310539245605\n",
      "[LOG 20200511-10:28:28] epoch: 0, batch: 7641 train-loss: 0.5048326253890991\n",
      "[LOG 20200511-10:28:28] epoch: 0, batch: 7642 train-loss: 1.4702396392822266\n",
      "[LOG 20200511-10:28:28] epoch: 0, batch: 7643 train-loss: 0.8769778609275818\n",
      "[LOG 20200511-10:28:28] epoch: 0, batch: 7644 train-loss: 1.916101098060608\n",
      "[LOG 20200511-10:28:28] epoch: 0, batch: 7645 train-loss: 2.0240843296051025\n",
      "[LOG 20200511-10:28:28] epoch: 0, batch: 7646 train-loss: 1.3928258419036865\n",
      "[LOG 20200511-10:28:29] epoch: 0, batch: 7647 train-loss: 1.5385463237762451\n",
      "[LOG 20200511-10:28:29] epoch: 0, batch: 7648 train-loss: 1.6785433292388916\n",
      "[LOG 20200511-10:28:29] epoch: 0, batch: 7649 train-loss: 1.666731834411621\n",
      "[LOG 20200511-10:28:29] epoch: 0, batch: 7650 train-loss: 0.6216438412666321\n",
      "[LOG 20200511-10:28:29] epoch: 0, batch: 7651 train-loss: 1.005478858947754\n",
      "[LOG 20200511-10:28:29] epoch: 0, batch: 7652 train-loss: 1.2566311359405518\n",
      "[LOG 20200511-10:28:29] epoch: 0, batch: 7653 train-loss: 1.435369849205017\n",
      "[LOG 20200511-10:28:29] epoch: 0, batch: 7654 train-loss: 1.7244625091552734\n",
      "[LOG 20200511-10:28:30] epoch: 0, batch: 7655 train-loss: 1.4242832660675049\n",
      "[LOG 20200511-10:28:30] epoch: 0, batch: 7656 train-loss: 0.9814878106117249\n",
      "[LOG 20200511-10:28:30] epoch: 0, batch: 7657 train-loss: 0.7044776678085327\n",
      "[LOG 20200511-10:28:30] epoch: 0, batch: 7658 train-loss: 2.4027762413024902\n",
      "[LOG 20200511-10:28:30] epoch: 0, batch: 7659 train-loss: 0.824697732925415\n",
      "[LOG 20200511-10:28:30] epoch: 0, batch: 7660 train-loss: 0.7340064644813538\n",
      "[LOG 20200511-10:28:30] epoch: 0, batch: 7661 train-loss: 0.9855464696884155\n",
      "[LOG 20200511-10:28:30] epoch: 0, batch: 7662 train-loss: 1.0196144580841064\n",
      "[LOG 20200511-10:28:30] epoch: 0, batch: 7663 train-loss: 0.7518781423568726\n",
      "[LOG 20200511-10:28:31] epoch: 0, batch: 7664 train-loss: 1.266699194908142\n",
      "[LOG 20200511-10:28:31] epoch: 0, batch: 7665 train-loss: 1.7414666414260864\n",
      "[LOG 20200511-10:28:31] epoch: 0, batch: 7666 train-loss: 0.8639607429504395\n",
      "[LOG 20200511-10:28:31] epoch: 0, batch: 7667 train-loss: 1.016951322555542\n",
      "[LOG 20200511-10:28:31] epoch: 0, batch: 7668 train-loss: 2.076138496398926\n",
      "[LOG 20200511-10:28:31] epoch: 0, batch: 7669 train-loss: 1.3157163858413696\n",
      "[LOG 20200511-10:28:31] epoch: 0, batch: 7670 train-loss: 1.5852230787277222\n",
      "[LOG 20200511-10:28:32] epoch: 0, batch: 7671 train-loss: 1.2882742881774902\n",
      "[LOG 20200511-10:28:32] epoch: 0, batch: 7672 train-loss: 1.0546464920043945\n",
      "[LOG 20200511-10:28:32] epoch: 0, batch: 7673 train-loss: 0.8260765075683594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:28:32] epoch: 0, batch: 7674 train-loss: 1.0894221067428589\n",
      "[LOG 20200511-10:28:32] epoch: 0, batch: 7675 train-loss: 1.223265528678894\n",
      "[LOG 20200511-10:28:32] epoch: 0, batch: 7676 train-loss: 1.6248984336853027\n",
      "[LOG 20200511-10:28:32] epoch: 0, batch: 7677 train-loss: 0.8660979866981506\n",
      "[LOG 20200511-10:28:32] epoch: 0, batch: 7678 train-loss: 2.2290382385253906\n",
      "[LOG 20200511-10:28:33] epoch: 0, batch: 7679 train-loss: 0.4801769256591797\n",
      "[LOG 20200511-10:28:33] epoch: 0, batch: 7680 train-loss: 1.253659725189209\n",
      "[LOG 20200511-10:28:33] epoch: 0, batch: 7681 train-loss: 1.4458717107772827\n",
      "[LOG 20200511-10:28:33] epoch: 0, batch: 7682 train-loss: 1.1601793766021729\n",
      "[LOG 20200511-10:28:33] epoch: 0, batch: 7683 train-loss: 1.7536996603012085\n",
      "[LOG 20200511-10:28:33] epoch: 0, batch: 7684 train-loss: 0.6514598727226257\n",
      "[LOG 20200511-10:28:33] epoch: 0, batch: 7685 train-loss: 1.164047122001648\n",
      "[LOG 20200511-10:28:33] epoch: 0, batch: 7686 train-loss: 1.6124212741851807\n",
      "[LOG 20200511-10:28:33] epoch: 0, batch: 7687 train-loss: 1.2961548566818237\n",
      "[LOG 20200511-10:28:34] epoch: 0, batch: 7688 train-loss: 0.7739880084991455\n",
      "[LOG 20200511-10:28:34] epoch: 0, batch: 7689 train-loss: 2.2519335746765137\n",
      "[LOG 20200511-10:28:34] epoch: 0, batch: 7690 train-loss: 0.9095966219902039\n",
      "[LOG 20200511-10:28:34] epoch: 0, batch: 7691 train-loss: 1.028820514678955\n",
      "[LOG 20200511-10:28:34] epoch: 0, batch: 7692 train-loss: 1.9044170379638672\n",
      "[LOG 20200511-10:28:34] epoch: 0, batch: 7693 train-loss: 0.6522776484489441\n",
      "[LOG 20200511-10:28:34] epoch: 0, batch: 7694 train-loss: 1.870395302772522\n",
      "[LOG 20200511-10:28:34] epoch: 0, batch: 7695 train-loss: 1.8113901615142822\n",
      "[LOG 20200511-10:28:35] epoch: 0, batch: 7696 train-loss: 1.6484439373016357\n",
      "[LOG 20200511-10:28:35] epoch: 0, batch: 7697 train-loss: 0.8976761102676392\n",
      "[LOG 20200511-10:28:35] epoch: 0, batch: 7698 train-loss: 0.3940863609313965\n",
      "[LOG 20200511-10:28:35] epoch: 0, batch: 7699 train-loss: 1.3758573532104492\n",
      "[LOG 20200511-10:28:35] epoch: 0, batch: 7700 train-loss: 1.383885145187378\n",
      "[LOG 20200511-10:28:35] epoch: 0, batch: 7701 train-loss: 1.8902108669281006\n",
      "[LOG 20200511-10:28:35] epoch: 0, batch: 7702 train-loss: 1.5538100004196167\n",
      "[LOG 20200511-10:28:35] epoch: 0, batch: 7703 train-loss: 0.30406033992767334\n",
      "[LOG 20200511-10:28:36] epoch: 0, batch: 7704 train-loss: 1.2198529243469238\n",
      "[LOG 20200511-10:28:36] epoch: 0, batch: 7705 train-loss: 1.662233829498291\n",
      "[LOG 20200511-10:28:36] epoch: 0, batch: 7706 train-loss: 0.9942203760147095\n",
      "[LOG 20200511-10:28:36] epoch: 0, batch: 7707 train-loss: 1.986653447151184\n",
      "[LOG 20200511-10:28:36] epoch: 0, batch: 7708 train-loss: 1.7313551902770996\n",
      "[LOG 20200511-10:28:36] epoch: 0, batch: 7709 train-loss: 1.6183192729949951\n",
      "[LOG 20200511-10:28:36] epoch: 0, batch: 7710 train-loss: 2.9471616744995117\n",
      "[LOG 20200511-10:28:36] epoch: 0, batch: 7711 train-loss: 1.0155255794525146\n",
      "[LOG 20200511-10:28:37] epoch: 0, batch: 7712 train-loss: 0.9236152172088623\n",
      "[LOG 20200511-10:28:37] epoch: 0, batch: 7713 train-loss: 0.47239869832992554\n",
      "[LOG 20200511-10:28:37] epoch: 0, batch: 7714 train-loss: 1.1638257503509521\n",
      "[LOG 20200511-10:28:37] epoch: 0, batch: 7715 train-loss: 1.2802538871765137\n",
      "[LOG 20200511-10:28:37] epoch: 0, batch: 7716 train-loss: 1.8331773281097412\n",
      "[LOG 20200511-10:28:37] epoch: 0, batch: 7717 train-loss: 1.0524590015411377\n",
      "[LOG 20200511-10:28:37] epoch: 0, batch: 7718 train-loss: 0.9423666596412659\n",
      "[LOG 20200511-10:28:37] epoch: 0, batch: 7719 train-loss: 2.467437982559204\n",
      "[LOG 20200511-10:28:38] epoch: 0, batch: 7720 train-loss: 1.6261703968048096\n",
      "[LOG 20200511-10:28:38] epoch: 0, batch: 7721 train-loss: 1.2899913787841797\n",
      "[LOG 20200511-10:28:38] epoch: 0, batch: 7722 train-loss: 1.1057720184326172\n",
      "[LOG 20200511-10:28:38] epoch: 0, batch: 7723 train-loss: 0.8032873272895813\n",
      "[LOG 20200511-10:28:38] epoch: 0, batch: 7724 train-loss: 1.8704447746276855\n",
      "[LOG 20200511-10:28:38] epoch: 0, batch: 7725 train-loss: 1.3617750406265259\n",
      "[LOG 20200511-10:28:38] epoch: 0, batch: 7726 train-loss: 1.8541336059570312\n",
      "[LOG 20200511-10:28:38] epoch: 0, batch: 7727 train-loss: 0.9453690052032471\n",
      "[LOG 20200511-10:28:38] epoch: 0, batch: 7728 train-loss: 1.1861870288848877\n",
      "[LOG 20200511-10:28:39] epoch: 0, batch: 7729 train-loss: 0.9960821866989136\n",
      "[LOG 20200511-10:28:39] epoch: 0, batch: 7730 train-loss: 0.8352167010307312\n",
      "[LOG 20200511-10:28:39] epoch: 0, batch: 7731 train-loss: 1.187432050704956\n",
      "[LOG 20200511-10:28:39] epoch: 0, batch: 7732 train-loss: 0.752626359462738\n",
      "[LOG 20200511-10:28:39] epoch: 0, batch: 7733 train-loss: 1.7292369604110718\n",
      "[LOG 20200511-10:28:39] epoch: 0, batch: 7734 train-loss: 2.170489549636841\n",
      "[LOG 20200511-10:28:39] epoch: 0, batch: 7735 train-loss: 1.0580434799194336\n",
      "[LOG 20200511-10:28:39] epoch: 0, batch: 7736 train-loss: 1.0674792528152466\n",
      "[LOG 20200511-10:28:40] epoch: 0, batch: 7737 train-loss: 0.9147555828094482\n",
      "[LOG 20200511-10:28:40] epoch: 0, batch: 7738 train-loss: 1.9512678384780884\n",
      "[LOG 20200511-10:28:40] epoch: 0, batch: 7739 train-loss: 1.6604276895523071\n",
      "[LOG 20200511-10:28:40] epoch: 0, batch: 7740 train-loss: 0.3022196888923645\n",
      "[LOG 20200511-10:28:40] epoch: 0, batch: 7741 train-loss: 1.5716160535812378\n",
      "[LOG 20200511-10:28:40] epoch: 0, batch: 7742 train-loss: 1.518550992012024\n",
      "[LOG 20200511-10:28:40] epoch: 0, batch: 7743 train-loss: 0.7202184200286865\n",
      "[LOG 20200511-10:28:40] epoch: 0, batch: 7744 train-loss: 0.7464576363563538\n",
      "[LOG 20200511-10:28:40] epoch: 0, batch: 7745 train-loss: 0.6545102596282959\n",
      "[LOG 20200511-10:28:41] epoch: 0, batch: 7746 train-loss: 0.9214693307876587\n",
      "[LOG 20200511-10:28:41] epoch: 0, batch: 7747 train-loss: 2.3744924068450928\n",
      "[LOG 20200511-10:28:41] epoch: 0, batch: 7748 train-loss: 1.41714346408844\n",
      "[LOG 20200511-10:28:41] epoch: 0, batch: 7749 train-loss: 0.640244722366333\n",
      "[LOG 20200511-10:28:41] epoch: 0, batch: 7750 train-loss: 1.115243911743164\n",
      "[LOG 20200511-10:28:41] epoch: 0, batch: 7751 train-loss: 0.913384199142456\n",
      "[LOG 20200511-10:28:41] epoch: 0, batch: 7752 train-loss: 2.6763405799865723\n",
      "[LOG 20200511-10:28:41] epoch: 0, batch: 7753 train-loss: 0.8407982587814331\n",
      "[LOG 20200511-10:28:41] epoch: 0, batch: 7754 train-loss: 0.9966744184494019\n",
      "[LOG 20200511-10:28:41] epoch: 0, batch: 7755 train-loss: 0.39016127586364746\n",
      "[LOG 20200511-10:28:42] epoch: 0, batch: 7756 train-loss: 2.126828670501709\n",
      "[LOG 20200511-10:28:42] epoch: 0, batch: 7757 train-loss: 1.9346137046813965\n",
      "[LOG 20200511-10:28:42] epoch: 0, batch: 7758 train-loss: 0.6811074614524841\n",
      "[LOG 20200511-10:28:42] epoch: 0, batch: 7759 train-loss: 0.5760917067527771\n",
      "[LOG 20200511-10:28:42] epoch: 0, batch: 7760 train-loss: 1.2059662342071533\n",
      "[LOG 20200511-10:28:42] epoch: 0, batch: 7761 train-loss: 1.2128560543060303\n",
      "[LOG 20200511-10:28:42] epoch: 0, batch: 7762 train-loss: 0.8917216062545776\n",
      "[LOG 20200511-10:28:42] epoch: 0, batch: 7763 train-loss: 1.882149338722229\n",
      "[LOG 20200511-10:28:42] epoch: 0, batch: 7764 train-loss: 1.536522626876831\n",
      "[LOG 20200511-10:28:43] epoch: 0, batch: 7765 train-loss: 2.7383151054382324\n",
      "[LOG 20200511-10:28:43] epoch: 0, batch: 7766 train-loss: 1.1814777851104736\n",
      "[LOG 20200511-10:28:43] epoch: 0, batch: 7767 train-loss: 1.828277826309204\n",
      "[LOG 20200511-10:28:43] epoch: 0, batch: 7768 train-loss: 0.7563101649284363\n",
      "[LOG 20200511-10:28:43] epoch: 0, batch: 7769 train-loss: 0.8738248348236084\n",
      "[LOG 20200511-10:28:43] epoch: 0, batch: 7770 train-loss: 1.5458347797393799\n",
      "[LOG 20200511-10:28:43] epoch: 0, batch: 7771 train-loss: 0.9079724550247192\n",
      "[LOG 20200511-10:28:43] epoch: 0, batch: 7772 train-loss: 1.4282848834991455\n",
      "[LOG 20200511-10:28:43] epoch: 0, batch: 7773 train-loss: 2.1630725860595703\n",
      "[LOG 20200511-10:28:43] epoch: 0, batch: 7774 train-loss: 0.9481011629104614\n",
      "[LOG 20200511-10:28:44] epoch: 0, batch: 7775 train-loss: 0.9522963762283325\n",
      "[LOG 20200511-10:28:44] epoch: 0, batch: 7776 train-loss: 2.1768438816070557\n",
      "[LOG 20200511-10:28:44] epoch: 0, batch: 7777 train-loss: 0.6348308324813843\n",
      "[LOG 20200511-10:28:44] epoch: 0, batch: 7778 train-loss: 1.2362738847732544\n",
      "[LOG 20200511-10:28:44] epoch: 0, batch: 7779 train-loss: 1.235085129737854\n",
      "[LOG 20200511-10:28:44] epoch: 0, batch: 7780 train-loss: 0.25073012709617615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:28:44] epoch: 0, batch: 7781 train-loss: 1.012202262878418\n",
      "[LOG 20200511-10:28:44] epoch: 0, batch: 7782 train-loss: 2.217540740966797\n",
      "[LOG 20200511-10:28:44] epoch: 0, batch: 7783 train-loss: 1.2564431428909302\n",
      "[LOG 20200511-10:28:45] epoch: 0, batch: 7784 train-loss: 1.2458988428115845\n",
      "[LOG 20200511-10:28:45] epoch: 0, batch: 7785 train-loss: 2.0548934936523438\n",
      "[LOG 20200511-10:28:45] epoch: 0, batch: 7786 train-loss: 0.639350175857544\n",
      "[LOG 20200511-10:28:45] epoch: 0, batch: 7787 train-loss: 0.6004900932312012\n",
      "[LOG 20200511-10:28:45] epoch: 0, batch: 7788 train-loss: 0.7973095178604126\n",
      "[LOG 20200511-10:28:45] epoch: 0, batch: 7789 train-loss: 1.3660778999328613\n",
      "[LOG 20200511-10:28:45] epoch: 0, batch: 7790 train-loss: 1.7851636409759521\n",
      "[LOG 20200511-10:28:45] epoch: 0, batch: 7791 train-loss: 0.8170919418334961\n",
      "[LOG 20200511-10:28:45] epoch: 0, batch: 7792 train-loss: 0.8536195755004883\n",
      "[LOG 20200511-10:28:46] epoch: 0, batch: 7793 train-loss: 0.7675479650497437\n",
      "[LOG 20200511-10:28:46] epoch: 0, batch: 7794 train-loss: 1.834230899810791\n",
      "[LOG 20200511-10:28:46] epoch: 0, batch: 7795 train-loss: 0.5073626637458801\n",
      "[LOG 20200511-10:28:46] epoch: 0, batch: 7796 train-loss: 1.166905164718628\n",
      "[LOG 20200511-10:28:46] epoch: 0, batch: 7797 train-loss: 1.1226441860198975\n",
      "[LOG 20200511-10:28:46] epoch: 0, batch: 7798 train-loss: 2.533099889755249\n",
      "[LOG 20200511-10:28:46] epoch: 0, batch: 7799 train-loss: 0.8169002532958984\n",
      "[LOG 20200511-10:28:46] epoch: 0, batch: 7800 train-loss: 1.14963698387146\n",
      "[LOG 20200511-10:28:47] epoch: 0, batch: 7801 train-loss: 0.6743703484535217\n",
      "[LOG 20200511-10:28:47] epoch: 0, batch: 7802 train-loss: 0.7284848093986511\n",
      "[LOG 20200511-10:28:47] epoch: 0, batch: 7803 train-loss: 0.4164644181728363\n",
      "[LOG 20200511-10:28:47] epoch: 0, batch: 7804 train-loss: 2.290433406829834\n",
      "[LOG 20200511-10:28:47] epoch: 0, batch: 7805 train-loss: 0.8445734977722168\n",
      "[LOG 20200511-10:28:47] epoch: 0, batch: 7806 train-loss: 2.50888991355896\n",
      "[LOG 20200511-10:28:47] epoch: 0, batch: 7807 train-loss: 1.0358757972717285\n",
      "[LOG 20200511-10:28:47] epoch: 0, batch: 7808 train-loss: 0.9652193784713745\n",
      "[LOG 20200511-10:28:48] epoch: 0, batch: 7809 train-loss: 0.8140432834625244\n",
      "[LOG 20200511-10:28:48] epoch: 0, batch: 7810 train-loss: 1.6702649593353271\n",
      "[LOG 20200511-10:28:48] epoch: 0, batch: 7811 train-loss: 0.5763455629348755\n",
      "[LOG 20200511-10:28:48] epoch: 0, batch: 7812 train-loss: 1.245168685913086\n",
      "[LOG 20200511-10:28:48] epoch: 0, batch: 7813 train-loss: 1.514386773109436\n",
      "[LOG 20200511-10:28:48] epoch: 0, batch: 7814 train-loss: 0.9808253049850464\n",
      "[LOG 20200511-10:28:48] epoch: 0, batch: 7815 train-loss: 1.655798077583313\n",
      "[LOG 20200511-10:28:48] epoch: 0, batch: 7816 train-loss: 1.054247260093689\n",
      "[LOG 20200511-10:28:49] epoch: 0, batch: 7817 train-loss: 0.7316750288009644\n",
      "[LOG 20200511-10:28:49] epoch: 0, batch: 7818 train-loss: 1.4470479488372803\n",
      "[LOG 20200511-10:28:49] epoch: 0, batch: 7819 train-loss: 1.4120556116104126\n",
      "[LOG 20200511-10:28:49] epoch: 0, batch: 7820 train-loss: 0.6575561761856079\n",
      "[LOG 20200511-10:28:49] epoch: 0, batch: 7821 train-loss: 0.5546502470970154\n",
      "[LOG 20200511-10:28:49] epoch: 0, batch: 7822 train-loss: 1.4606767892837524\n",
      "[LOG 20200511-10:28:49] epoch: 0, batch: 7823 train-loss: 1.1841589212417603\n",
      "[LOG 20200511-10:28:49] epoch: 0, batch: 7824 train-loss: 0.7049329876899719\n",
      "[LOG 20200511-10:28:49] epoch: 0, batch: 7825 train-loss: 0.4641411304473877\n",
      "[LOG 20200511-10:28:50] epoch: 0, batch: 7826 train-loss: 1.2937978506088257\n",
      "[LOG 20200511-10:28:50] epoch: 0, batch: 7827 train-loss: 0.6733826398849487\n",
      "[LOG 20200511-10:28:50] epoch: 0, batch: 7828 train-loss: 0.7937975525856018\n",
      "[LOG 20200511-10:28:50] epoch: 0, batch: 7829 train-loss: 0.5637909770011902\n",
      "[LOG 20200511-10:28:50] epoch: 0, batch: 7830 train-loss: 1.5560657978057861\n",
      "[LOG 20200511-10:28:50] epoch: 0, batch: 7831 train-loss: 0.8685801029205322\n",
      "[LOG 20200511-10:28:50] epoch: 0, batch: 7832 train-loss: 1.063451886177063\n",
      "[LOG 20200511-10:28:50] epoch: 0, batch: 7833 train-loss: 2.0835981369018555\n",
      "[LOG 20200511-10:28:51] epoch: 0, batch: 7834 train-loss: 1.3365033864974976\n",
      "[LOG 20200511-10:28:51] epoch: 0, batch: 7835 train-loss: 0.7685250043869019\n",
      "[LOG 20200511-10:28:51] epoch: 0, batch: 7836 train-loss: 1.7832351922988892\n",
      "[LOG 20200511-10:28:51] epoch: 0, batch: 7837 train-loss: 1.8213999271392822\n",
      "[LOG 20200511-10:28:51] epoch: 0, batch: 7838 train-loss: 1.202225685119629\n",
      "[LOG 20200511-10:28:51] epoch: 0, batch: 7839 train-loss: 1.273419976234436\n",
      "[LOG 20200511-10:28:51] epoch: 0, batch: 7840 train-loss: 0.7955365777015686\n",
      "[LOG 20200511-10:28:51] epoch: 0, batch: 7841 train-loss: 1.5031487941741943\n",
      "[LOG 20200511-10:28:51] epoch: 0, batch: 7842 train-loss: 0.8892340660095215\n",
      "[LOG 20200511-10:28:52] epoch: 0, batch: 7843 train-loss: 1.8954064846038818\n",
      "[LOG 20200511-10:28:52] epoch: 0, batch: 7844 train-loss: 1.5682203769683838\n",
      "[LOG 20200511-10:28:52] epoch: 0, batch: 7845 train-loss: 1.6834259033203125\n",
      "[LOG 20200511-10:28:52] epoch: 0, batch: 7846 train-loss: 0.8382883071899414\n",
      "[LOG 20200511-10:28:52] epoch: 0, batch: 7847 train-loss: 2.015104055404663\n",
      "[LOG 20200511-10:28:52] epoch: 0, batch: 7848 train-loss: 0.8768196702003479\n",
      "[LOG 20200511-10:28:52] epoch: 0, batch: 7849 train-loss: 0.7965818643569946\n",
      "[LOG 20200511-10:28:52] epoch: 0, batch: 7850 train-loss: 0.9294260740280151\n",
      "[LOG 20200511-10:28:52] epoch: 0, batch: 7851 train-loss: 0.8414517045021057\n",
      "[LOG 20200511-10:28:52] epoch: 0, batch: 7852 train-loss: 1.0236908197402954\n",
      "[LOG 20200511-10:28:53] epoch: 0, batch: 7853 train-loss: 1.144866943359375\n",
      "[LOG 20200511-10:28:53] epoch: 0, batch: 7854 train-loss: 0.9707005620002747\n",
      "[LOG 20200511-10:28:53] epoch: 0, batch: 7855 train-loss: 0.8080893754959106\n",
      "[LOG 20200511-10:28:53] epoch: 0, batch: 7856 train-loss: 0.3059411346912384\n",
      "[LOG 20200511-10:28:53] epoch: 0, batch: 7857 train-loss: 0.9991317987442017\n",
      "[LOG 20200511-10:28:53] epoch: 0, batch: 7858 train-loss: 0.5165762305259705\n",
      "[LOG 20200511-10:28:53] epoch: 0, batch: 7859 train-loss: 2.204075813293457\n",
      "[LOG 20200511-10:28:53] epoch: 0, batch: 7860 train-loss: 1.2174272537231445\n",
      "[LOG 20200511-10:28:53] epoch: 0, batch: 7861 train-loss: 3.2978715896606445\n",
      "[LOG 20200511-10:28:54] epoch: 0, batch: 7862 train-loss: 0.8957316875457764\n",
      "[LOG 20200511-10:28:54] epoch: 0, batch: 7863 train-loss: 1.1609958410263062\n",
      "[LOG 20200511-10:28:54] epoch: 0, batch: 7864 train-loss: 2.245845317840576\n",
      "[LOG 20200511-10:28:54] epoch: 0, batch: 7865 train-loss: 1.3723875284194946\n",
      "[LOG 20200511-10:28:54] epoch: 0, batch: 7866 train-loss: 1.0698683261871338\n",
      "[LOG 20200511-10:28:54] epoch: 0, batch: 7867 train-loss: 1.6756778955459595\n",
      "[LOG 20200511-10:28:54] epoch: 0, batch: 7868 train-loss: 0.7531027793884277\n",
      "[LOG 20200511-10:28:54] epoch: 0, batch: 7869 train-loss: 0.3941144049167633\n",
      "[LOG 20200511-10:28:55] epoch: 0, batch: 7870 train-loss: 1.2017672061920166\n",
      "[LOG 20200511-10:28:55] epoch: 0, batch: 7871 train-loss: 0.8561140298843384\n",
      "[LOG 20200511-10:28:55] epoch: 0, batch: 7872 train-loss: 0.4250348210334778\n",
      "[LOG 20200511-10:28:55] epoch: 0, batch: 7873 train-loss: 1.2842789888381958\n",
      "[LOG 20200511-10:28:55] epoch: 0, batch: 7874 train-loss: 1.5622203350067139\n",
      "[LOG 20200511-10:28:55] epoch: 0, batch: 7875 train-loss: 1.3385262489318848\n",
      "[LOG 20200511-10:28:55] epoch: 0, batch: 7876 train-loss: 1.9469068050384521\n",
      "[LOG 20200511-10:28:55] epoch: 0, batch: 7877 train-loss: 0.5520219802856445\n",
      "[LOG 20200511-10:28:55] epoch: 0, batch: 7878 train-loss: 2.461683750152588\n",
      "[LOG 20200511-10:28:56] epoch: 0, batch: 7879 train-loss: 1.1524701118469238\n",
      "[LOG 20200511-10:28:56] epoch: 0, batch: 7880 train-loss: 0.6678789854049683\n",
      "[LOG 20200511-10:28:56] epoch: 0, batch: 7881 train-loss: 1.2519131898880005\n",
      "[LOG 20200511-10:28:56] epoch: 0, batch: 7882 train-loss: 0.7503343820571899\n",
      "[LOG 20200511-10:28:56] epoch: 0, batch: 7883 train-loss: 0.24226117134094238\n",
      "[LOG 20200511-10:28:56] epoch: 0, batch: 7884 train-loss: 0.6205016374588013\n",
      "[LOG 20200511-10:28:56] epoch: 0, batch: 7885 train-loss: 1.2024970054626465\n",
      "[LOG 20200511-10:28:56] epoch: 0, batch: 7886 train-loss: 1.4813120365142822\n",
      "[LOG 20200511-10:28:56] epoch: 0, batch: 7887 train-loss: 2.5740323066711426\n",
      "[LOG 20200511-10:28:57] epoch: 0, batch: 7888 train-loss: 0.8073148131370544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:28:57] epoch: 0, batch: 7889 train-loss: 0.9510881304740906\n",
      "[LOG 20200511-10:28:57] epoch: 0, batch: 7890 train-loss: 0.8764156699180603\n",
      "[LOG 20200511-10:28:57] epoch: 0, batch: 7891 train-loss: 0.5079348087310791\n",
      "[LOG 20200511-10:28:57] epoch: 0, batch: 7892 train-loss: 0.88637775182724\n",
      "[LOG 20200511-10:28:57] epoch: 0, batch: 7893 train-loss: 1.7120224237442017\n",
      "[LOG 20200511-10:28:57] epoch: 0, batch: 7894 train-loss: 1.361352562904358\n",
      "[LOG 20200511-10:28:57] epoch: 0, batch: 7895 train-loss: 1.3942968845367432\n",
      "[LOG 20200511-10:28:57] epoch: 0, batch: 7896 train-loss: 1.206179141998291\n",
      "[LOG 20200511-10:28:58] epoch: 0, batch: 7897 train-loss: 1.3841638565063477\n",
      "[LOG 20200511-10:28:58] epoch: 0, batch: 7898 train-loss: 0.6521651744842529\n",
      "[LOG 20200511-10:28:58] epoch: 0, batch: 7899 train-loss: 1.4469883441925049\n",
      "[LOG 20200511-10:28:58] epoch: 0, batch: 7900 train-loss: 1.5458428859710693\n",
      "[LOG 20200511-10:28:58] epoch: 0, batch: 7901 train-loss: 1.6278538703918457\n",
      "[LOG 20200511-10:28:58] epoch: 0, batch: 7902 train-loss: 0.9706661701202393\n",
      "[LOG 20200511-10:28:58] epoch: 0, batch: 7903 train-loss: 0.9990190863609314\n",
      "[LOG 20200511-10:28:58] epoch: 0, batch: 7904 train-loss: 1.572650671005249\n",
      "[LOG 20200511-10:28:58] epoch: 0, batch: 7905 train-loss: 0.33208972215652466\n",
      "[LOG 20200511-10:28:58] epoch: 0, batch: 7906 train-loss: 1.4716697931289673\n",
      "[LOG 20200511-10:28:59] epoch: 0, batch: 7907 train-loss: 1.9977519512176514\n",
      "[LOG 20200511-10:28:59] epoch: 0, batch: 7908 train-loss: 2.114009380340576\n",
      "[LOG 20200511-10:28:59] epoch: 0, batch: 7909 train-loss: 0.5216831564903259\n",
      "[LOG 20200511-10:28:59] epoch: 0, batch: 7910 train-loss: 1.1860610246658325\n",
      "[LOG 20200511-10:28:59] epoch: 0, batch: 7911 train-loss: 0.9193392992019653\n",
      "[LOG 20200511-10:28:59] epoch: 0, batch: 7912 train-loss: 1.2620413303375244\n",
      "[LOG 20200511-10:28:59] epoch: 0, batch: 7913 train-loss: 0.8100138902664185\n",
      "[LOG 20200511-10:28:59] epoch: 0, batch: 7914 train-loss: 1.7013864517211914\n",
      "[LOG 20200511-10:28:59] epoch: 0, batch: 7915 train-loss: 0.8185538649559021\n",
      "[LOG 20200511-10:29:00] epoch: 0, batch: 7916 train-loss: 3.509097099304199\n",
      "[LOG 20200511-10:29:00] epoch: 0, batch: 7917 train-loss: 1.2084736824035645\n",
      "[LOG 20200511-10:29:00] epoch: 0, batch: 7918 train-loss: 1.1098082065582275\n",
      "[LOG 20200511-10:29:00] epoch: 0, batch: 7919 train-loss: 1.248591423034668\n",
      "[LOG 20200511-10:29:00] epoch: 0, batch: 7920 train-loss: 1.0462604761123657\n",
      "[LOG 20200511-10:29:00] epoch: 0, batch: 7921 train-loss: 1.0144044160842896\n",
      "[LOG 20200511-10:29:00] epoch: 0, batch: 7922 train-loss: 1.5459611415863037\n",
      "[LOG 20200511-10:29:00] epoch: 0, batch: 7923 train-loss: 1.8655281066894531\n",
      "[LOG 20200511-10:29:00] epoch: 0, batch: 7924 train-loss: 1.7112929821014404\n",
      "[LOG 20200511-10:29:00] epoch: 0, batch: 7925 train-loss: 1.5707365274429321\n",
      "[LOG 20200511-10:29:01] epoch: 0, batch: 7926 train-loss: 0.5253899097442627\n",
      "[LOG 20200511-10:29:01] epoch: 0, batch: 7927 train-loss: 1.0819095373153687\n",
      "[LOG 20200511-10:29:01] epoch: 0, batch: 7928 train-loss: 1.114709496498108\n",
      "[LOG 20200511-10:29:01] epoch: 0, batch: 7929 train-loss: 1.6796820163726807\n",
      "[LOG 20200511-10:29:01] epoch: 0, batch: 7930 train-loss: 0.9233534336090088\n",
      "[LOG 20200511-10:29:01] epoch: 0, batch: 7931 train-loss: 0.32222163677215576\n",
      "[LOG 20200511-10:29:01] epoch: 0, batch: 7932 train-loss: 0.6584555506706238\n",
      "[LOG 20200511-10:29:01] epoch: 0, batch: 7933 train-loss: 1.6147428750991821\n",
      "[LOG 20200511-10:29:01] epoch: 0, batch: 7934 train-loss: 1.343069076538086\n",
      "[LOG 20200511-10:29:02] epoch: 0, batch: 7935 train-loss: 1.6884886026382446\n",
      "[LOG 20200511-10:29:02] epoch: 0, batch: 7936 train-loss: 1.0979373455047607\n",
      "[LOG 20200511-10:29:02] epoch: 0, batch: 7937 train-loss: 1.1190330982208252\n",
      "[LOG 20200511-10:29:02] epoch: 0, batch: 7938 train-loss: 1.207142949104309\n",
      "[LOG 20200511-10:29:02] epoch: 0, batch: 7939 train-loss: 1.0653467178344727\n",
      "[LOG 20200511-10:29:02] epoch: 0, batch: 7940 train-loss: 1.292379379272461\n",
      "[LOG 20200511-10:29:02] epoch: 0, batch: 7941 train-loss: 1.4444384574890137\n",
      "[LOG 20200511-10:29:02] epoch: 0, batch: 7942 train-loss: 0.6903584003448486\n",
      "[LOG 20200511-10:29:02] epoch: 0, batch: 7943 train-loss: 1.6255812644958496\n",
      "[LOG 20200511-10:29:02] epoch: 0, batch: 7944 train-loss: 1.0180386304855347\n",
      "[LOG 20200511-10:29:03] epoch: 0, batch: 7945 train-loss: 1.6546435356140137\n",
      "[LOG 20200511-10:29:03] epoch: 0, batch: 7946 train-loss: 1.8597725629806519\n",
      "[LOG 20200511-10:29:03] epoch: 0, batch: 7947 train-loss: 0.7521408200263977\n",
      "[LOG 20200511-10:29:03] epoch: 0, batch: 7948 train-loss: 1.5112314224243164\n",
      "[LOG 20200511-10:29:03] epoch: 0, batch: 7949 train-loss: 1.7420976161956787\n",
      "[LOG 20200511-10:29:03] epoch: 0, batch: 7950 train-loss: 0.9208875298500061\n",
      "[LOG 20200511-10:29:03] epoch: 0, batch: 7951 train-loss: 0.5835917592048645\n",
      "[LOG 20200511-10:29:03] epoch: 0, batch: 7952 train-loss: 1.3795584440231323\n",
      "[LOG 20200511-10:29:03] epoch: 0, batch: 7953 train-loss: 2.0979998111724854\n",
      "[LOG 20200511-10:29:04] epoch: 0, batch: 7954 train-loss: 2.8940892219543457\n",
      "[LOG 20200511-10:29:04] epoch: 0, batch: 7955 train-loss: 0.485943078994751\n",
      "[LOG 20200511-10:29:04] epoch: 0, batch: 7956 train-loss: 1.2664449214935303\n",
      "[LOG 20200511-10:29:04] epoch: 0, batch: 7957 train-loss: 0.761131763458252\n",
      "[LOG 20200511-10:29:04] epoch: 0, batch: 7958 train-loss: 1.5271081924438477\n",
      "[LOG 20200511-10:29:04] epoch: 0, batch: 7959 train-loss: 0.7312427759170532\n",
      "[LOG 20200511-10:29:04] epoch: 0, batch: 7960 train-loss: 1.440341830253601\n",
      "[LOG 20200511-10:29:04] epoch: 0, batch: 7961 train-loss: 0.8178712129592896\n",
      "[LOG 20200511-10:29:04] epoch: 0, batch: 7962 train-loss: 0.385311096906662\n",
      "[LOG 20200511-10:29:04] epoch: 0, batch: 7963 train-loss: 1.239776849746704\n",
      "[LOG 20200511-10:29:05] epoch: 0, batch: 7964 train-loss: 1.0858346223831177\n",
      "[LOG 20200511-10:29:05] epoch: 0, batch: 7965 train-loss: 1.0052028894424438\n",
      "[LOG 20200511-10:29:05] epoch: 0, batch: 7966 train-loss: 2.2258472442626953\n",
      "[LOG 20200511-10:29:05] epoch: 0, batch: 7967 train-loss: 0.8361685872077942\n",
      "[LOG 20200511-10:29:05] epoch: 0, batch: 7968 train-loss: 1.3639686107635498\n",
      "[LOG 20200511-10:29:05] epoch: 0, batch: 7969 train-loss: 1.9899617433547974\n",
      "[LOG 20200511-10:29:05] epoch: 0, batch: 7970 train-loss: 1.9471461772918701\n",
      "[LOG 20200511-10:29:05] epoch: 0, batch: 7971 train-loss: 0.6078717112541199\n",
      "[LOG 20200511-10:29:05] epoch: 0, batch: 7972 train-loss: 1.488441824913025\n",
      "[LOG 20200511-10:29:06] epoch: 0, batch: 7973 train-loss: 1.2059664726257324\n",
      "[LOG 20200511-10:29:06] epoch: 0, batch: 7974 train-loss: 1.6156435012817383\n",
      "[LOG 20200511-10:29:06] epoch: 0, batch: 7975 train-loss: 1.1809378862380981\n",
      "[LOG 20200511-10:29:06] epoch: 0, batch: 7976 train-loss: 1.6192041635513306\n",
      "[LOG 20200511-10:29:06] epoch: 0, batch: 7977 train-loss: 1.2040992975234985\n",
      "[LOG 20200511-10:29:06] epoch: 0, batch: 7978 train-loss: 1.356748342514038\n",
      "[LOG 20200511-10:29:06] epoch: 0, batch: 7979 train-loss: 0.9906856417655945\n",
      "[LOG 20200511-10:29:06] epoch: 0, batch: 7980 train-loss: 0.6004060506820679\n",
      "[LOG 20200511-10:29:06] epoch: 0, batch: 7981 train-loss: 0.674307644367218\n",
      "[LOG 20200511-10:29:06] epoch: 0, batch: 7982 train-loss: 1.3539420366287231\n",
      "[LOG 20200511-10:29:07] epoch: 0, batch: 7983 train-loss: 0.9825845956802368\n",
      "[LOG 20200511-10:29:07] epoch: 0, batch: 7984 train-loss: 1.5359610319137573\n",
      "[LOG 20200511-10:29:07] epoch: 0, batch: 7985 train-loss: 1.037651538848877\n",
      "[LOG 20200511-10:29:07] epoch: 0, batch: 7986 train-loss: 1.3509563207626343\n",
      "[LOG 20200511-10:29:07] epoch: 0, batch: 7987 train-loss: 3.0698907375335693\n",
      "[LOG 20200511-10:29:07] epoch: 0, batch: 7988 train-loss: 1.0053715705871582\n",
      "[LOG 20200511-10:29:07] epoch: 0, batch: 7989 train-loss: 0.6648579835891724\n",
      "[LOG 20200511-10:29:07] epoch: 0, batch: 7990 train-loss: 1.4454848766326904\n",
      "[LOG 20200511-10:29:07] epoch: 0, batch: 7991 train-loss: 1.409337043762207\n",
      "[LOG 20200511-10:29:07] epoch: 0, batch: 7992 train-loss: 0.8173773288726807\n",
      "[LOG 20200511-10:29:08] epoch: 0, batch: 7993 train-loss: 0.5297281742095947\n",
      "[LOG 20200511-10:29:08] epoch: 0, batch: 7994 train-loss: 1.3474884033203125\n",
      "[LOG 20200511-10:29:08] epoch: 0, batch: 7995 train-loss: 1.3613195419311523\n",
      "[LOG 20200511-10:29:08] epoch: 0, batch: 7996 train-loss: 0.7447029948234558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:29:08] epoch: 0, batch: 7997 train-loss: 1.8268046379089355\n",
      "[LOG 20200511-10:29:08] epoch: 0, batch: 7998 train-loss: 1.082383394241333\n",
      "[LOG 20200511-10:29:08] epoch: 0, batch: 7999 train-loss: 1.0779154300689697\n",
      "[LOG 20200511-10:29:08] epoch: 0, batch: 8000 train-loss: 1.1603548526763916\n",
      "[LOG 20200511-10:29:08] epoch: 0, batch: 8001 train-loss: 0.6941575407981873\n",
      "[LOG 20200511-10:29:09] epoch: 0, batch: 8002 train-loss: 1.5187556743621826\n",
      "[LOG 20200511-10:29:09] epoch: 0, batch: 8003 train-loss: 0.6999068260192871\n",
      "[LOG 20200511-10:29:09] epoch: 0, batch: 8004 train-loss: 0.528565526008606\n",
      "[LOG 20200511-10:29:09] epoch: 0, batch: 8005 train-loss: 1.4047836065292358\n",
      "[LOG 20200511-10:29:09] epoch: 0, batch: 8006 train-loss: 1.255517601966858\n",
      "[LOG 20200511-10:29:09] epoch: 0, batch: 8007 train-loss: 0.8007914423942566\n",
      "[LOG 20200511-10:29:09] epoch: 0, batch: 8008 train-loss: 0.8039618730545044\n",
      "[LOG 20200511-10:29:09] epoch: 0, batch: 8009 train-loss: 1.0260822772979736\n",
      "[LOG 20200511-10:29:09] epoch: 0, batch: 8010 train-loss: 1.7999234199523926\n",
      "[LOG 20200511-10:29:09] epoch: 0, batch: 8011 train-loss: 1.687286138534546\n",
      "[LOG 20200511-10:29:10] epoch: 0, batch: 8012 train-loss: 1.0630558729171753\n",
      "[LOG 20200511-10:29:10] epoch: 0, batch: 8013 train-loss: 1.6660598516464233\n",
      "[LOG 20200511-10:29:10] epoch: 0, batch: 8014 train-loss: 1.7263052463531494\n",
      "[LOG 20200511-10:29:10] epoch: 0, batch: 8015 train-loss: 1.5849934816360474\n",
      "[LOG 20200511-10:29:10] epoch: 0, batch: 8016 train-loss: 0.7203840017318726\n",
      "[LOG 20200511-10:29:10] epoch: 0, batch: 8017 train-loss: 1.3445286750793457\n",
      "[LOG 20200511-10:29:10] epoch: 0, batch: 8018 train-loss: 0.4882972538471222\n",
      "[LOG 20200511-10:29:10] epoch: 0, batch: 8019 train-loss: 0.37912580370903015\n",
      "[LOG 20200511-10:29:10] epoch: 0, batch: 8020 train-loss: 1.447779655456543\n",
      "[LOG 20200511-10:29:11] epoch: 0, batch: 8021 train-loss: 1.4699645042419434\n",
      "[LOG 20200511-10:29:11] epoch: 0, batch: 8022 train-loss: 1.3582501411437988\n",
      "[LOG 20200511-10:29:11] epoch: 0, batch: 8023 train-loss: 0.30184417963027954\n",
      "[LOG 20200511-10:29:11] epoch: 0, batch: 8024 train-loss: 0.8914742469787598\n",
      "[LOG 20200511-10:29:11] epoch: 0, batch: 8025 train-loss: 1.1433024406433105\n",
      "[LOG 20200511-10:29:11] epoch: 0, batch: 8026 train-loss: 1.8292847871780396\n",
      "[LOG 20200511-10:29:11] epoch: 0, batch: 8027 train-loss: 1.070106029510498\n",
      "[LOG 20200511-10:29:11] epoch: 0, batch: 8028 train-loss: 0.589975893497467\n",
      "[LOG 20200511-10:29:11] epoch: 0, batch: 8029 train-loss: 1.2139713764190674\n",
      "[LOG 20200511-10:29:11] epoch: 0, batch: 8030 train-loss: 1.1122254133224487\n",
      "[LOG 20200511-10:29:12] epoch: 0, batch: 8031 train-loss: 1.6200098991394043\n",
      "[LOG 20200511-10:29:12] epoch: 0, batch: 8032 train-loss: 0.7536048889160156\n",
      "[LOG 20200511-10:29:12] epoch: 0, batch: 8033 train-loss: 2.4323291778564453\n",
      "[LOG 20200511-10:29:12] epoch: 0, batch: 8034 train-loss: 1.7240678071975708\n",
      "[LOG 20200511-10:29:12] epoch: 0, batch: 8035 train-loss: 1.8935269117355347\n",
      "[LOG 20200511-10:29:12] epoch: 0, batch: 8036 train-loss: 1.033921480178833\n",
      "[LOG 20200511-10:29:12] epoch: 0, batch: 8037 train-loss: 1.5827734470367432\n",
      "[LOG 20200511-10:29:12] epoch: 0, batch: 8038 train-loss: 0.9221786856651306\n",
      "[LOG 20200511-10:29:12] epoch: 0, batch: 8039 train-loss: 1.6782562732696533\n",
      "[LOG 20200511-10:29:13] epoch: 0, batch: 8040 train-loss: 1.2813308238983154\n",
      "[LOG 20200511-10:29:13] epoch: 0, batch: 8041 train-loss: 1.29995858669281\n",
      "[LOG 20200511-10:29:13] epoch: 0, batch: 8042 train-loss: 0.8701145052909851\n",
      "[LOG 20200511-10:29:13] epoch: 0, batch: 8043 train-loss: 1.5824017524719238\n",
      "[LOG 20200511-10:29:13] epoch: 0, batch: 8044 train-loss: 1.4993400573730469\n",
      "[LOG 20200511-10:29:13] epoch: 0, batch: 8045 train-loss: 2.2076337337493896\n",
      "[LOG 20200511-10:29:13] epoch: 0, batch: 8046 train-loss: 1.564854621887207\n",
      "[LOG 20200511-10:29:13] epoch: 0, batch: 8047 train-loss: 1.0958671569824219\n",
      "[LOG 20200511-10:29:13] epoch: 0, batch: 8048 train-loss: 1.864796757698059\n",
      "[LOG 20200511-10:29:13] epoch: 0, batch: 8049 train-loss: 1.4891284704208374\n",
      "[LOG 20200511-10:29:14] epoch: 0, batch: 8050 train-loss: 0.793559193611145\n",
      "[LOG 20200511-10:29:14] epoch: 0, batch: 8051 train-loss: 1.8222763538360596\n",
      "[LOG 20200511-10:29:14] epoch: 0, batch: 8052 train-loss: 1.246217966079712\n",
      "[LOG 20200511-10:29:14] epoch: 0, batch: 8053 train-loss: 2.2591686248779297\n",
      "[LOG 20200511-10:29:14] epoch: 0, batch: 8054 train-loss: 1.4717237949371338\n",
      "[LOG 20200511-10:29:14] epoch: 0, batch: 8055 train-loss: 0.5920042991638184\n",
      "[LOG 20200511-10:29:14] epoch: 0, batch: 8056 train-loss: 0.7251405119895935\n",
      "[LOG 20200511-10:29:14] epoch: 0, batch: 8057 train-loss: 1.1288647651672363\n",
      "[LOG 20200511-10:29:14] epoch: 0, batch: 8058 train-loss: 1.6227211952209473\n",
      "[LOG 20200511-10:29:15] epoch: 0, batch: 8059 train-loss: 1.625633955001831\n",
      "[LOG 20200511-10:29:15] epoch: 0, batch: 8060 train-loss: 1.5042515993118286\n",
      "[LOG 20200511-10:29:15] epoch: 0, batch: 8061 train-loss: 0.8417106866836548\n",
      "[LOG 20200511-10:29:15] epoch: 0, batch: 8062 train-loss: 0.9845740795135498\n",
      "[LOG 20200511-10:29:15] epoch: 0, batch: 8063 train-loss: 1.553429365158081\n",
      "[LOG 20200511-10:29:15] epoch: 0, batch: 8064 train-loss: 1.6843371391296387\n",
      "[LOG 20200511-10:29:15] epoch: 0, batch: 8065 train-loss: 1.2159967422485352\n",
      "[LOG 20200511-10:29:15] epoch: 0, batch: 8066 train-loss: 1.1526061296463013\n",
      "[LOG 20200511-10:29:15] epoch: 0, batch: 8067 train-loss: 0.8004425168037415\n",
      "[LOG 20200511-10:29:16] epoch: 0, batch: 8068 train-loss: 1.059473991394043\n",
      "[LOG 20200511-10:29:16] epoch: 0, batch: 8069 train-loss: 1.1690272092819214\n",
      "[LOG 20200511-10:29:16] epoch: 0, batch: 8070 train-loss: 0.9427728652954102\n",
      "[LOG 20200511-10:29:16] epoch: 0, batch: 8071 train-loss: 2.072047233581543\n",
      "[LOG 20200511-10:29:16] epoch: 0, batch: 8072 train-loss: 1.329890251159668\n",
      "[LOG 20200511-10:29:16] epoch: 0, batch: 8073 train-loss: 2.55900239944458\n",
      "[LOG 20200511-10:29:16] epoch: 0, batch: 8074 train-loss: 1.771422028541565\n",
      "[LOG 20200511-10:29:16] epoch: 0, batch: 8075 train-loss: 0.400326132774353\n",
      "[LOG 20200511-10:29:17] epoch: 0, batch: 8076 train-loss: 1.1553888320922852\n",
      "[LOG 20200511-10:29:17] epoch: 0, batch: 8077 train-loss: 1.0430117845535278\n",
      "[LOG 20200511-10:29:17] epoch: 0, batch: 8078 train-loss: 0.6130391359329224\n",
      "[LOG 20200511-10:29:17] epoch: 0, batch: 8079 train-loss: 1.277875304222107\n",
      "[LOG 20200511-10:29:17] epoch: 0, batch: 8080 train-loss: 0.8931585550308228\n",
      "[LOG 20200511-10:29:17] epoch: 0, batch: 8081 train-loss: 1.9942238330841064\n",
      "[LOG 20200511-10:29:17] epoch: 0, batch: 8082 train-loss: 1.539749026298523\n",
      "[LOG 20200511-10:29:17] epoch: 0, batch: 8083 train-loss: 0.9354993104934692\n",
      "[LOG 20200511-10:29:18] epoch: 0, batch: 8084 train-loss: 1.2279376983642578\n",
      "[LOG 20200511-10:29:18] epoch: 0, batch: 8085 train-loss: 1.3314323425292969\n",
      "[LOG 20200511-10:29:18] epoch: 0, batch: 8086 train-loss: 1.4982445240020752\n",
      "[LOG 20200511-10:29:18] epoch: 0, batch: 8087 train-loss: 0.7999991178512573\n",
      "[LOG 20200511-10:29:18] epoch: 0, batch: 8088 train-loss: 2.4793925285339355\n",
      "[LOG 20200511-10:29:18] epoch: 0, batch: 8089 train-loss: 0.9631123542785645\n",
      "[LOG 20200511-10:29:18] epoch: 0, batch: 8090 train-loss: 1.355037808418274\n",
      "[LOG 20200511-10:29:18] epoch: 0, batch: 8091 train-loss: 0.6019783616065979\n",
      "[LOG 20200511-10:29:18] epoch: 0, batch: 8092 train-loss: 1.06731379032135\n",
      "[LOG 20200511-10:29:19] epoch: 0, batch: 8093 train-loss: 2.0640711784362793\n",
      "[LOG 20200511-10:29:19] epoch: 0, batch: 8094 train-loss: 0.20400658249855042\n",
      "[LOG 20200511-10:29:19] epoch: 0, batch: 8095 train-loss: 0.9955801963806152\n",
      "[LOG 20200511-10:29:19] epoch: 0, batch: 8096 train-loss: 1.4592574834823608\n",
      "[LOG 20200511-10:29:19] epoch: 0, batch: 8097 train-loss: 1.1478619575500488\n",
      "[LOG 20200511-10:29:19] epoch: 0, batch: 8098 train-loss: 1.445165753364563\n",
      "[LOG 20200511-10:29:19] epoch: 0, batch: 8099 train-loss: 0.7688227891921997\n",
      "[LOG 20200511-10:29:19] epoch: 0, batch: 8100 train-loss: 1.0537647008895874\n",
      "[LOG 20200511-10:29:20] epoch: 0, batch: 8101 train-loss: 1.3111950159072876\n",
      "[LOG 20200511-10:29:20] epoch: 0, batch: 8102 train-loss: 1.6381657123565674\n",
      "[LOG 20200511-10:29:20] epoch: 0, batch: 8103 train-loss: 1.0564019680023193\n",
      "[LOG 20200511-10:29:20] epoch: 0, batch: 8104 train-loss: 0.8124184608459473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:29:20] epoch: 0, batch: 8105 train-loss: 2.072244644165039\n",
      "[LOG 20200511-10:29:20] epoch: 0, batch: 8106 train-loss: 1.0223023891448975\n",
      "[LOG 20200511-10:29:20] epoch: 0, batch: 8107 train-loss: 1.775425910949707\n",
      "[LOG 20200511-10:29:20] epoch: 0, batch: 8108 train-loss: 1.6220877170562744\n",
      "[LOG 20200511-10:29:20] epoch: 0, batch: 8109 train-loss: 0.9190653562545776\n",
      "[LOG 20200511-10:29:21] epoch: 0, batch: 8110 train-loss: 0.6456162333488464\n",
      "[LOG 20200511-10:29:21] epoch: 0, batch: 8111 train-loss: 0.8337956070899963\n",
      "[LOG 20200511-10:29:21] epoch: 0, batch: 8112 train-loss: 0.44913631677627563\n",
      "[LOG 20200511-10:29:21] epoch: 0, batch: 8113 train-loss: 0.8099594116210938\n",
      "[LOG 20200511-10:29:21] epoch: 0, batch: 8114 train-loss: 1.6778990030288696\n",
      "[LOG 20200511-10:29:21] epoch: 0, batch: 8115 train-loss: 1.2153971195220947\n",
      "[LOG 20200511-10:29:21] epoch: 0, batch: 8116 train-loss: 1.3457369804382324\n",
      "[LOG 20200511-10:29:21] epoch: 0, batch: 8117 train-loss: 1.0313186645507812\n",
      "[LOG 20200511-10:29:21] epoch: 0, batch: 8118 train-loss: 0.5281563401222229\n",
      "[LOG 20200511-10:29:22] epoch: 0, batch: 8119 train-loss: 1.5222724676132202\n",
      "[LOG 20200511-10:29:22] epoch: 0, batch: 8120 train-loss: 1.3054027557373047\n",
      "[LOG 20200511-10:29:22] epoch: 0, batch: 8121 train-loss: 0.9453058838844299\n",
      "[LOG 20200511-10:29:22] epoch: 0, batch: 8122 train-loss: 1.9767781496047974\n",
      "[LOG 20200511-10:29:22] epoch: 0, batch: 8123 train-loss: 0.8301332592964172\n",
      "[LOG 20200511-10:29:22] epoch: 0, batch: 8124 train-loss: 1.0598353147506714\n",
      "[LOG 20200511-10:29:22] epoch: 0, batch: 8125 train-loss: 0.4515256881713867\n",
      "[LOG 20200511-10:29:22] epoch: 0, batch: 8126 train-loss: 1.3659850358963013\n",
      "[LOG 20200511-10:29:23] epoch: 0, batch: 8127 train-loss: 1.3751987218856812\n",
      "[LOG 20200511-10:29:23] epoch: 0, batch: 8128 train-loss: 0.837526261806488\n",
      "[LOG 20200511-10:29:23] epoch: 0, batch: 8129 train-loss: 1.020028829574585\n",
      "[LOG 20200511-10:29:23] epoch: 0, batch: 8130 train-loss: 1.3163864612579346\n",
      "[LOG 20200511-10:29:23] epoch: 0, batch: 8131 train-loss: 1.2448633909225464\n",
      "[LOG 20200511-10:29:23] epoch: 0, batch: 8132 train-loss: 1.0482984781265259\n",
      "[LOG 20200511-10:29:23] epoch: 0, batch: 8133 train-loss: 0.5954291224479675\n",
      "[LOG 20200511-10:29:23] epoch: 0, batch: 8134 train-loss: 0.30765846371650696\n",
      "[LOG 20200511-10:29:24] epoch: 0, batch: 8135 train-loss: 2.006775140762329\n",
      "[LOG 20200511-10:29:24] epoch: 0, batch: 8136 train-loss: 1.3071149587631226\n",
      "[LOG 20200511-10:29:24] epoch: 0, batch: 8137 train-loss: 0.8631013631820679\n",
      "[LOG 20200511-10:29:24] epoch: 0, batch: 8138 train-loss: 1.718649983406067\n",
      "[LOG 20200511-10:29:24] epoch: 0, batch: 8139 train-loss: 0.6014025807380676\n",
      "[LOG 20200511-10:29:24] epoch: 0, batch: 8140 train-loss: 2.429403781890869\n",
      "[LOG 20200511-10:29:24] epoch: 0, batch: 8141 train-loss: 1.006364345550537\n",
      "[LOG 20200511-10:29:24] epoch: 0, batch: 8142 train-loss: 1.990244746208191\n",
      "[LOG 20200511-10:29:25] epoch: 0, batch: 8143 train-loss: 1.114701271057129\n",
      "[LOG 20200511-10:29:25] epoch: 0, batch: 8144 train-loss: 1.4484065771102905\n",
      "[LOG 20200511-10:29:25] epoch: 0, batch: 8145 train-loss: 1.303937554359436\n",
      "[LOG 20200511-10:29:25] epoch: 0, batch: 8146 train-loss: 1.159098744392395\n",
      "[LOG 20200511-10:29:25] epoch: 0, batch: 8147 train-loss: 1.6102855205535889\n",
      "[LOG 20200511-10:29:25] epoch: 0, batch: 8148 train-loss: 1.3393473625183105\n",
      "[LOG 20200511-10:29:25] epoch: 0, batch: 8149 train-loss: 0.4590725600719452\n",
      "[LOG 20200511-10:29:25] epoch: 0, batch: 8150 train-loss: 1.6777105331420898\n",
      "[LOG 20200511-10:29:25] epoch: 0, batch: 8151 train-loss: 1.3792424201965332\n",
      "[LOG 20200511-10:29:26] epoch: 0, batch: 8152 train-loss: 1.2917433977127075\n",
      "[LOG 20200511-10:29:26] epoch: 0, batch: 8153 train-loss: 1.4700202941894531\n",
      "[LOG 20200511-10:29:26] epoch: 0, batch: 8154 train-loss: 0.5694962739944458\n",
      "[LOG 20200511-10:29:26] epoch: 0, batch: 8155 train-loss: 0.7334883213043213\n",
      "[LOG 20200511-10:29:26] epoch: 0, batch: 8156 train-loss: 0.6895829439163208\n",
      "[LOG 20200511-10:29:26] epoch: 0, batch: 8157 train-loss: 1.5679970979690552\n",
      "[LOG 20200511-10:29:26] epoch: 0, batch: 8158 train-loss: 1.2222563028335571\n",
      "[LOG 20200511-10:29:26] epoch: 0, batch: 8159 train-loss: 1.0225437879562378\n",
      "[LOG 20200511-10:29:27] epoch: 0, batch: 8160 train-loss: 1.3199397325515747\n",
      "[LOG 20200511-10:29:27] epoch: 0, batch: 8161 train-loss: 0.8436264991760254\n",
      "[LOG 20200511-10:29:27] epoch: 0, batch: 8162 train-loss: 1.031758189201355\n",
      "[LOG 20200511-10:29:27] epoch: 0, batch: 8163 train-loss: 0.9184775948524475\n",
      "[LOG 20200511-10:29:27] epoch: 0, batch: 8164 train-loss: 1.070488691329956\n",
      "[LOG 20200511-10:29:27] epoch: 0, batch: 8165 train-loss: 0.9898576736450195\n",
      "[LOG 20200511-10:29:27] epoch: 0, batch: 8166 train-loss: 1.6561479568481445\n",
      "[LOG 20200511-10:29:27] epoch: 0, batch: 8167 train-loss: 1.8301209211349487\n",
      "[LOG 20200511-10:29:27] epoch: 0, batch: 8168 train-loss: 1.0009135007858276\n",
      "[LOG 20200511-10:29:28] epoch: 0, batch: 8169 train-loss: 1.1690019369125366\n",
      "[LOG 20200511-10:29:28] epoch: 0, batch: 8170 train-loss: 1.4029251337051392\n",
      "[LOG 20200511-10:29:28] epoch: 0, batch: 8171 train-loss: 1.6076490879058838\n",
      "[LOG 20200511-10:29:28] epoch: 0, batch: 8172 train-loss: 2.2423946857452393\n",
      "[LOG 20200511-10:29:28] epoch: 0, batch: 8173 train-loss: 1.384459137916565\n",
      "[LOG 20200511-10:29:28] epoch: 0, batch: 8174 train-loss: 1.4249464273452759\n",
      "[LOG 20200511-10:29:28] epoch: 0, batch: 8175 train-loss: 0.5885499119758606\n",
      "[LOG 20200511-10:29:28] epoch: 0, batch: 8176 train-loss: 2.839822292327881\n",
      "[LOG 20200511-10:29:29] epoch: 0, batch: 8177 train-loss: 1.0773072242736816\n",
      "[LOG 20200511-10:29:29] epoch: 0, batch: 8178 train-loss: 0.48241978883743286\n",
      "[LOG 20200511-10:29:29] epoch: 0, batch: 8179 train-loss: 0.4639190435409546\n",
      "[LOG 20200511-10:29:29] epoch: 0, batch: 8180 train-loss: 0.7080923914909363\n",
      "[LOG 20200511-10:29:29] epoch: 0, batch: 8181 train-loss: 1.7137539386749268\n",
      "[LOG 20200511-10:29:29] epoch: 0, batch: 8182 train-loss: 2.087603807449341\n",
      "[LOG 20200511-10:29:29] epoch: 0, batch: 8183 train-loss: 1.4084863662719727\n",
      "[LOG 20200511-10:29:29] epoch: 0, batch: 8184 train-loss: 1.8110911846160889\n",
      "[LOG 20200511-10:29:29] epoch: 0, batch: 8185 train-loss: 0.9263885617256165\n",
      "[LOG 20200511-10:29:30] epoch: 0, batch: 8186 train-loss: 1.681464672088623\n",
      "[LOG 20200511-10:29:30] epoch: 0, batch: 8187 train-loss: 0.633293628692627\n",
      "[LOG 20200511-10:29:30] epoch: 0, batch: 8188 train-loss: 0.37001296877861023\n",
      "[LOG 20200511-10:29:30] epoch: 0, batch: 8189 train-loss: 2.049365997314453\n",
      "[LOG 20200511-10:29:30] epoch: 0, batch: 8190 train-loss: 0.5746182203292847\n",
      "[LOG 20200511-10:29:30] epoch: 0, batch: 8191 train-loss: 2.20076847076416\n",
      "[LOG 20200511-10:29:30] epoch: 0, batch: 8192 train-loss: 1.0588067770004272\n",
      "[LOG 20200511-10:29:30] epoch: 0, batch: 8193 train-loss: 1.9035258293151855\n",
      "[LOG 20200511-10:29:30] epoch: 0, batch: 8194 train-loss: 1.4681119918823242\n",
      "[LOG 20200511-10:29:31] epoch: 0, batch: 8195 train-loss: 0.8345574736595154\n",
      "[LOG 20200511-10:29:31] epoch: 0, batch: 8196 train-loss: 0.5505000352859497\n",
      "[LOG 20200511-10:29:31] epoch: 0, batch: 8197 train-loss: 1.9604361057281494\n",
      "[LOG 20200511-10:29:31] epoch: 0, batch: 8198 train-loss: 0.5068215131759644\n",
      "[LOG 20200511-10:29:31] epoch: 0, batch: 8199 train-loss: 0.9584912061691284\n",
      "[LOG 20200511-10:29:31] epoch: 0, batch: 8200 train-loss: 1.0544922351837158\n",
      "[LOG 20200511-10:29:31] epoch: 0, batch: 8201 train-loss: 2.2438743114471436\n",
      "[LOG 20200511-10:29:31] epoch: 0, batch: 8202 train-loss: 0.9839068651199341\n",
      "[LOG 20200511-10:29:32] epoch: 0, batch: 8203 train-loss: 0.7574477195739746\n",
      "[LOG 20200511-10:29:32] epoch: 0, batch: 8204 train-loss: 1.0273654460906982\n",
      "[LOG 20200511-10:29:32] epoch: 0, batch: 8205 train-loss: 0.7095953822135925\n",
      "[LOG 20200511-10:29:32] epoch: 0, batch: 8206 train-loss: 1.5161075592041016\n",
      "[LOG 20200511-10:29:32] epoch: 0, batch: 8207 train-loss: 0.6281464099884033\n",
      "[LOG 20200511-10:29:32] epoch: 0, batch: 8208 train-loss: 0.8321191668510437\n",
      "[LOG 20200511-10:29:32] epoch: 0, batch: 8209 train-loss: 1.0261175632476807\n",
      "[LOG 20200511-10:29:32] epoch: 0, batch: 8210 train-loss: 0.6449718475341797\n",
      "[LOG 20200511-10:29:32] epoch: 0, batch: 8211 train-loss: 0.7970304489135742\n",
      "[LOG 20200511-10:29:33] epoch: 0, batch: 8212 train-loss: 0.6929324269294739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:29:33] epoch: 0, batch: 8213 train-loss: 0.8272583484649658\n",
      "[LOG 20200511-10:29:33] epoch: 0, batch: 8214 train-loss: 1.397073745727539\n",
      "[LOG 20200511-10:29:33] epoch: 0, batch: 8215 train-loss: 2.0826902389526367\n",
      "[LOG 20200511-10:29:33] epoch: 0, batch: 8216 train-loss: 1.0397558212280273\n",
      "[LOG 20200511-10:29:33] epoch: 0, batch: 8217 train-loss: 0.8808412551879883\n",
      "[LOG 20200511-10:29:33] epoch: 0, batch: 8218 train-loss: 1.331835150718689\n",
      "[LOG 20200511-10:29:33] epoch: 0, batch: 8219 train-loss: 1.7835644483566284\n",
      "[LOG 20200511-10:29:33] epoch: 0, batch: 8220 train-loss: 1.531928300857544\n",
      "[LOG 20200511-10:29:34] epoch: 0, batch: 8221 train-loss: 0.5546858906745911\n",
      "[LOG 20200511-10:29:34] epoch: 0, batch: 8222 train-loss: 0.990634024143219\n",
      "[LOG 20200511-10:29:34] epoch: 0, batch: 8223 train-loss: 0.9363412857055664\n",
      "[LOG 20200511-10:29:34] epoch: 0, batch: 8224 train-loss: 0.8902953863143921\n",
      "[LOG 20200511-10:29:34] epoch: 0, batch: 8225 train-loss: 1.5094472169876099\n",
      "[LOG 20200511-10:29:34] epoch: 0, batch: 8226 train-loss: 0.9796268939971924\n",
      "[LOG 20200511-10:29:34] epoch: 0, batch: 8227 train-loss: 1.5186400413513184\n",
      "[LOG 20200511-10:29:34] epoch: 0, batch: 8228 train-loss: 0.37621742486953735\n",
      "[LOG 20200511-10:29:35] epoch: 0, batch: 8229 train-loss: 1.42947518825531\n",
      "[LOG 20200511-10:29:35] epoch: 0, batch: 8230 train-loss: 1.5859508514404297\n",
      "[LOG 20200511-10:29:35] epoch: 0, batch: 8231 train-loss: 1.428348183631897\n",
      "[LOG 20200511-10:29:35] epoch: 0, batch: 8232 train-loss: 0.9869334101676941\n",
      "[LOG 20200511-10:29:35] epoch: 0, batch: 8233 train-loss: 0.38199859857559204\n",
      "[LOG 20200511-10:29:35] epoch: 0, batch: 8234 train-loss: 1.5615630149841309\n",
      "[LOG 20200511-10:29:35] epoch: 0, batch: 8235 train-loss: 0.8599928021430969\n",
      "[LOG 20200511-10:29:35] epoch: 0, batch: 8236 train-loss: 0.5088114738464355\n",
      "[LOG 20200511-10:29:35] epoch: 0, batch: 8237 train-loss: 0.45691582560539246\n",
      "[LOG 20200511-10:29:36] epoch: 0, batch: 8238 train-loss: 1.2929607629776\n",
      "[LOG 20200511-10:29:36] epoch: 0, batch: 8239 train-loss: 1.455509901046753\n",
      "[LOG 20200511-10:29:36] epoch: 0, batch: 8240 train-loss: 0.7350995540618896\n",
      "[LOG 20200511-10:29:36] epoch: 0, batch: 8241 train-loss: 1.2005298137664795\n",
      "[LOG 20200511-10:29:36] epoch: 0, batch: 8242 train-loss: 0.631781280040741\n",
      "[LOG 20200511-10:29:36] epoch: 0, batch: 8243 train-loss: 1.051236629486084\n",
      "[LOG 20200511-10:29:36] epoch: 0, batch: 8244 train-loss: 2.288774251937866\n",
      "[LOG 20200511-10:29:36] epoch: 0, batch: 8245 train-loss: 0.5895162224769592\n",
      "[LOG 20200511-10:29:37] epoch: 0, batch: 8246 train-loss: 0.8925929069519043\n",
      "[LOG 20200511-10:29:37] epoch: 0, batch: 8247 train-loss: 1.5594862699508667\n",
      "[LOG 20200511-10:29:37] epoch: 0, batch: 8248 train-loss: 1.0304449796676636\n",
      "[LOG 20200511-10:29:37] epoch: 0, batch: 8249 train-loss: 1.4561095237731934\n",
      "[LOG 20200511-10:29:37] epoch: 0, batch: 8250 train-loss: 0.925013542175293\n",
      "[LOG 20200511-10:29:37] epoch: 0, batch: 8251 train-loss: 1.131249189376831\n",
      "[LOG 20200511-10:29:37] epoch: 0, batch: 8252 train-loss: 0.8728053569793701\n",
      "[LOG 20200511-10:29:37] epoch: 0, batch: 8253 train-loss: 0.3846324384212494\n",
      "[LOG 20200511-10:29:37] epoch: 0, batch: 8254 train-loss: 1.2425222396850586\n",
      "[LOG 20200511-10:29:38] epoch: 0, batch: 8255 train-loss: 1.4267910718917847\n",
      "[LOG 20200511-10:29:38] epoch: 0, batch: 8256 train-loss: 1.2459096908569336\n",
      "[LOG 20200511-10:29:38] epoch: 0, batch: 8257 train-loss: 1.3369390964508057\n",
      "[LOG 20200511-10:29:38] epoch: 0, batch: 8258 train-loss: 0.8615845441818237\n",
      "[LOG 20200511-10:29:38] epoch: 0, batch: 8259 train-loss: 1.0601086616516113\n",
      "[LOG 20200511-10:29:38] epoch: 0, batch: 8260 train-loss: 1.402268409729004\n",
      "[LOG 20200511-10:29:38] epoch: 0, batch: 8261 train-loss: 0.46167534589767456\n",
      "[LOG 20200511-10:29:38] epoch: 0, batch: 8262 train-loss: 1.4158458709716797\n",
      "[LOG 20200511-10:29:38] epoch: 0, batch: 8263 train-loss: 1.240016222000122\n",
      "[LOG 20200511-10:29:39] epoch: 0, batch: 8264 train-loss: 1.3721213340759277\n",
      "[LOG 20200511-10:29:39] epoch: 0, batch: 8265 train-loss: 0.6155945658683777\n",
      "[LOG 20200511-10:29:39] epoch: 0, batch: 8266 train-loss: 0.5503033399581909\n",
      "[LOG 20200511-10:29:39] epoch: 0, batch: 8267 train-loss: 2.2687270641326904\n",
      "[LOG 20200511-10:29:39] epoch: 0, batch: 8268 train-loss: 0.6478703022003174\n",
      "[LOG 20200511-10:29:39] epoch: 0, batch: 8269 train-loss: 1.2698354721069336\n",
      "[LOG 20200511-10:29:39] epoch: 0, batch: 8270 train-loss: 0.4535392224788666\n",
      "[LOG 20200511-10:29:39] epoch: 0, batch: 8271 train-loss: 1.836801290512085\n",
      "[LOG 20200511-10:29:39] epoch: 0, batch: 8272 train-loss: 1.1290342807769775\n",
      "[LOG 20200511-10:29:40] epoch: 0, batch: 8273 train-loss: 0.9277681708335876\n",
      "[LOG 20200511-10:29:40] epoch: 0, batch: 8274 train-loss: 0.7322404980659485\n",
      "[LOG 20200511-10:29:40] epoch: 0, batch: 8275 train-loss: 1.0904450416564941\n",
      "[LOG 20200511-10:29:40] epoch: 0, batch: 8276 train-loss: 1.2071774005889893\n",
      "[LOG 20200511-10:29:40] epoch: 0, batch: 8277 train-loss: 2.3966002464294434\n",
      "[LOG 20200511-10:29:40] epoch: 0, batch: 8278 train-loss: 0.8393545150756836\n",
      "[LOG 20200511-10:29:40] epoch: 0, batch: 8279 train-loss: 1.509515643119812\n",
      "[LOG 20200511-10:29:40] epoch: 0, batch: 8280 train-loss: 0.7425658702850342\n",
      "[LOG 20200511-10:29:41] epoch: 0, batch: 8281 train-loss: 1.1372135877609253\n",
      "[LOG 20200511-10:29:41] epoch: 0, batch: 8282 train-loss: 0.4467814862728119\n",
      "[LOG 20200511-10:29:41] epoch: 0, batch: 8283 train-loss: 1.0090057849884033\n",
      "[LOG 20200511-10:29:41] epoch: 0, batch: 8284 train-loss: 0.7228345274925232\n",
      "[LOG 20200511-10:29:41] epoch: 0, batch: 8285 train-loss: 1.9638038873672485\n",
      "[LOG 20200511-10:29:41] epoch: 0, batch: 8286 train-loss: 1.6848528385162354\n",
      "[LOG 20200511-10:29:41] epoch: 0, batch: 8287 train-loss: 0.9373787641525269\n",
      "[LOG 20200511-10:29:41] epoch: 0, batch: 8288 train-loss: 1.1126742362976074\n",
      "[LOG 20200511-10:29:41] epoch: 0, batch: 8289 train-loss: 2.3288865089416504\n",
      "[LOG 20200511-10:29:41] epoch: 0, batch: 8290 train-loss: 0.46396467089653015\n",
      "[LOG 20200511-10:29:42] epoch: 0, batch: 8291 train-loss: 1.1743168830871582\n",
      "[LOG 20200511-10:29:42] epoch: 0, batch: 8292 train-loss: 0.8183038234710693\n",
      "[LOG 20200511-10:29:42] epoch: 0, batch: 8293 train-loss: 0.8035975694656372\n",
      "[LOG 20200511-10:29:42] epoch: 0, batch: 8294 train-loss: 1.832247018814087\n",
      "[LOG 20200511-10:29:42] epoch: 0, batch: 8295 train-loss: 2.0880393981933594\n",
      "[LOG 20200511-10:29:42] epoch: 0, batch: 8296 train-loss: 0.7420681715011597\n",
      "[LOG 20200511-10:29:42] epoch: 0, batch: 8297 train-loss: 0.5508829951286316\n",
      "[LOG 20200511-10:29:42] epoch: 0, batch: 8298 train-loss: 1.4935710430145264\n",
      "[LOG 20200511-10:29:42] epoch: 0, batch: 8299 train-loss: 1.8078242540359497\n",
      "[LOG 20200511-10:29:43] epoch: 0, batch: 8300 train-loss: 1.4359524250030518\n",
      "[LOG 20200511-10:29:43] epoch: 0, batch: 8301 train-loss: 1.7089594602584839\n",
      "[LOG 20200511-10:29:43] epoch: 0, batch: 8302 train-loss: 1.5555139780044556\n",
      "[LOG 20200511-10:29:43] epoch: 0, batch: 8303 train-loss: 1.3330315351486206\n",
      "[LOG 20200511-10:29:43] epoch: 0, batch: 8304 train-loss: 0.6375053524971008\n",
      "[LOG 20200511-10:29:43] epoch: 0, batch: 8305 train-loss: 0.8988267779350281\n",
      "[LOG 20200511-10:29:43] epoch: 0, batch: 8306 train-loss: 1.46364426612854\n",
      "[LOG 20200511-10:29:43] epoch: 0, batch: 8307 train-loss: 1.4974159002304077\n",
      "[LOG 20200511-10:29:43] epoch: 0, batch: 8308 train-loss: 1.7542330026626587\n",
      "[LOG 20200511-10:29:44] epoch: 0, batch: 8309 train-loss: 0.2744402587413788\n",
      "[LOG 20200511-10:29:44] epoch: 0, batch: 8310 train-loss: 1.5740232467651367\n",
      "[LOG 20200511-10:29:44] epoch: 0, batch: 8311 train-loss: 0.7910876870155334\n",
      "[LOG 20200511-10:29:44] epoch: 0, batch: 8312 train-loss: 0.988986074924469\n",
      "[LOG 20200511-10:29:44] epoch: 0, batch: 8313 train-loss: 1.4914120435714722\n",
      "[LOG 20200511-10:29:44] epoch: 0, batch: 8314 train-loss: 1.4960856437683105\n",
      "[LOG 20200511-10:29:44] epoch: 0, batch: 8315 train-loss: 1.12996506690979\n",
      "[LOG 20200511-10:29:44] epoch: 0, batch: 8316 train-loss: 1.7719464302062988\n",
      "[LOG 20200511-10:29:44] epoch: 0, batch: 8317 train-loss: 1.1347368955612183\n",
      "[LOG 20200511-10:29:45] epoch: 0, batch: 8318 train-loss: 1.6528410911560059\n",
      "[LOG 20200511-10:29:45] epoch: 0, batch: 8319 train-loss: 1.1183090209960938\n",
      "[LOG 20200511-10:29:45] epoch: 0, batch: 8320 train-loss: 0.19077642261981964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:29:45] epoch: 0, batch: 8321 train-loss: 2.184544801712036\n",
      "[LOG 20200511-10:29:45] epoch: 0, batch: 8322 train-loss: 1.352881669998169\n",
      "[LOG 20200511-10:29:45] epoch: 0, batch: 8323 train-loss: 1.1502562761306763\n",
      "[LOG 20200511-10:29:45] epoch: 0, batch: 8324 train-loss: 0.7058506011962891\n",
      "[LOG 20200511-10:29:45] epoch: 0, batch: 8325 train-loss: 1.775007963180542\n",
      "[LOG 20200511-10:29:45] epoch: 0, batch: 8326 train-loss: 1.335024118423462\n",
      "[LOG 20200511-10:29:46] epoch: 0, batch: 8327 train-loss: 1.1795425415039062\n",
      "[LOG 20200511-10:29:46] epoch: 0, batch: 8328 train-loss: 0.6535477638244629\n",
      "[LOG 20200511-10:29:46] epoch: 0, batch: 8329 train-loss: 1.089000940322876\n",
      "[LOG 20200511-10:29:46] epoch: 0, batch: 8330 train-loss: 1.330376148223877\n",
      "[LOG 20200511-10:29:46] epoch: 0, batch: 8331 train-loss: 1.473585605621338\n",
      "[LOG 20200511-10:29:46] epoch: 0, batch: 8332 train-loss: 1.0130670070648193\n",
      "[LOG 20200511-10:29:46] epoch: 0, batch: 8333 train-loss: 0.9249718189239502\n",
      "[LOG 20200511-10:29:46] epoch: 0, batch: 8334 train-loss: 1.4477548599243164\n",
      "[LOG 20200511-10:29:47] epoch: 0, batch: 8335 train-loss: 1.2237095832824707\n",
      "[LOG 20200511-10:29:47] epoch: 0, batch: 8336 train-loss: 0.652445912361145\n",
      "[LOG 20200511-10:29:47] epoch: 0, batch: 8337 train-loss: 0.6566950082778931\n",
      "[LOG 20200511-10:29:47] epoch: 0, batch: 8338 train-loss: 2.1359481811523438\n",
      "[LOG 20200511-10:29:47] epoch: 0, batch: 8339 train-loss: 0.9537571668624878\n",
      "[LOG 20200511-10:29:47] epoch: 0, batch: 8340 train-loss: 1.8927333354949951\n",
      "[LOG 20200511-10:29:47] epoch: 0, batch: 8341 train-loss: 1.985069990158081\n",
      "[LOG 20200511-10:29:47] epoch: 0, batch: 8342 train-loss: 0.6465726494789124\n",
      "[LOG 20200511-10:29:47] epoch: 0, batch: 8343 train-loss: 1.0015413761138916\n",
      "[LOG 20200511-10:29:48] epoch: 0, batch: 8344 train-loss: 1.4735687971115112\n",
      "[LOG 20200511-10:29:48] epoch: 0, batch: 8345 train-loss: 2.0567069053649902\n",
      "[LOG 20200511-10:29:48] epoch: 0, batch: 8346 train-loss: 0.5335971713066101\n",
      "[LOG 20200511-10:29:48] epoch: 0, batch: 8347 train-loss: 0.39054107666015625\n",
      "[LOG 20200511-10:29:48] epoch: 0, batch: 8348 train-loss: 1.2127069234848022\n",
      "[LOG 20200511-10:29:48] epoch: 0, batch: 8349 train-loss: 1.1274125576019287\n",
      "[LOG 20200511-10:29:48] epoch: 0, batch: 8350 train-loss: 1.1420185565948486\n",
      "[LOG 20200511-10:29:48] epoch: 0, batch: 8351 train-loss: 1.2666569948196411\n",
      "[LOG 20200511-10:29:48] epoch: 0, batch: 8352 train-loss: 1.7611606121063232\n",
      "[LOG 20200511-10:29:49] epoch: 0, batch: 8353 train-loss: 1.4321891069412231\n",
      "[LOG 20200511-10:29:49] epoch: 0, batch: 8354 train-loss: 1.5250377655029297\n",
      "[LOG 20200511-10:29:49] epoch: 0, batch: 8355 train-loss: 1.0154739618301392\n",
      "[LOG 20200511-10:29:49] epoch: 0, batch: 8356 train-loss: 2.1029934883117676\n",
      "[LOG 20200511-10:29:49] epoch: 0, batch: 8357 train-loss: 1.2967324256896973\n",
      "[LOG 20200511-10:29:49] epoch: 0, batch: 8358 train-loss: 1.4493308067321777\n",
      "[LOG 20200511-10:29:49] epoch: 0, batch: 8359 train-loss: 0.2988487780094147\n",
      "[LOG 20200511-10:29:49] epoch: 0, batch: 8360 train-loss: 1.2817047834396362\n",
      "[LOG 20200511-10:29:50] epoch: 0, batch: 8361 train-loss: 1.792931318283081\n",
      "[LOG 20200511-10:29:50] epoch: 0, batch: 8362 train-loss: 1.9192099571228027\n",
      "[LOG 20200511-10:29:50] epoch: 0, batch: 8363 train-loss: 0.795886218547821\n",
      "[LOG 20200511-10:29:50] epoch: 0, batch: 8364 train-loss: 1.4990452527999878\n",
      "[LOG 20200511-10:29:50] epoch: 0, batch: 8365 train-loss: 0.7292829751968384\n",
      "[LOG 20200511-10:29:50] epoch: 0, batch: 8366 train-loss: 1.4403752088546753\n",
      "[LOG 20200511-10:29:50] epoch: 0, batch: 8367 train-loss: 1.0175461769104004\n",
      "[LOG 20200511-10:29:50] epoch: 0, batch: 8368 train-loss: 1.8918530941009521\n",
      "[LOG 20200511-10:29:50] epoch: 0, batch: 8369 train-loss: 0.445585697889328\n",
      "[LOG 20200511-10:29:51] epoch: 0, batch: 8370 train-loss: 0.28722891211509705\n",
      "[LOG 20200511-10:29:51] epoch: 0, batch: 8371 train-loss: 1.6431524753570557\n",
      "[LOG 20200511-10:29:51] epoch: 0, batch: 8372 train-loss: 1.2316999435424805\n",
      "[LOG 20200511-10:29:51] epoch: 0, batch: 8373 train-loss: 0.9996377825737\n",
      "[LOG 20200511-10:29:51] epoch: 0, batch: 8374 train-loss: 0.6945116519927979\n",
      "[LOG 20200511-10:29:51] epoch: 0, batch: 8375 train-loss: 1.2645716667175293\n",
      "[LOG 20200511-10:29:51] epoch: 0, batch: 8376 train-loss: 0.9038412570953369\n",
      "[LOG 20200511-10:29:51] epoch: 0, batch: 8377 train-loss: 1.3636751174926758\n",
      "[LOG 20200511-10:29:51] epoch: 0, batch: 8378 train-loss: 0.37276631593704224\n",
      "[LOG 20200511-10:29:52] epoch: 0, batch: 8379 train-loss: 1.0316599607467651\n",
      "[LOG 20200511-10:29:52] epoch: 0, batch: 8380 train-loss: 0.7020219564437866\n",
      "[LOG 20200511-10:29:52] epoch: 0, batch: 8381 train-loss: 1.6050796508789062\n",
      "[LOG 20200511-10:29:52] epoch: 0, batch: 8382 train-loss: 1.3226125240325928\n",
      "[LOG 20200511-10:29:52] epoch: 0, batch: 8383 train-loss: 0.7630499601364136\n",
      "[LOG 20200511-10:29:52] epoch: 0, batch: 8384 train-loss: 1.0817564725875854\n",
      "[LOG 20200511-10:29:52] epoch: 0, batch: 8385 train-loss: 1.5695794820785522\n",
      "[LOG 20200511-10:29:52] epoch: 0, batch: 8386 train-loss: 0.7671856880187988\n",
      "[LOG 20200511-10:29:53] epoch: 0, batch: 8387 train-loss: 1.4642612934112549\n",
      "[LOG 20200511-10:29:53] epoch: 0, batch: 8388 train-loss: 2.094151020050049\n",
      "[LOG 20200511-10:29:53] epoch: 0, batch: 8389 train-loss: 1.2694807052612305\n",
      "[LOG 20200511-10:29:53] epoch: 0, batch: 8390 train-loss: 1.2799837589263916\n",
      "[LOG 20200511-10:29:53] epoch: 0, batch: 8391 train-loss: 2.2802860736846924\n",
      "[LOG 20200511-10:29:53] epoch: 0, batch: 8392 train-loss: 0.6190085411071777\n",
      "[LOG 20200511-10:29:53] epoch: 0, batch: 8393 train-loss: 0.6979072690010071\n",
      "[LOG 20200511-10:29:53] epoch: 0, batch: 8394 train-loss: 0.9107847809791565\n",
      "[LOG 20200511-10:29:54] epoch: 0, batch: 8395 train-loss: 1.8150436878204346\n",
      "[LOG 20200511-10:29:54] epoch: 0, batch: 8396 train-loss: 1.4952207803726196\n",
      "[LOG 20200511-10:29:54] epoch: 0, batch: 8397 train-loss: 0.5177151560783386\n",
      "[LOG 20200511-10:29:54] epoch: 0, batch: 8398 train-loss: 0.9229505658149719\n",
      "[LOG 20200511-10:29:54] epoch: 0, batch: 8399 train-loss: 0.9980077743530273\n",
      "[LOG 20200511-10:29:54] epoch: 0, batch: 8400 train-loss: 1.0443135499954224\n",
      "[LOG 20200511-10:29:54] epoch: 0, batch: 8401 train-loss: 1.4059252738952637\n",
      "[LOG 20200511-10:29:54] epoch: 0, batch: 8402 train-loss: 0.8038883209228516\n",
      "[LOG 20200511-10:29:55] epoch: 0, batch: 8403 train-loss: 1.4879428148269653\n",
      "[LOG 20200511-10:29:55] epoch: 0, batch: 8404 train-loss: 1.1614062786102295\n",
      "[LOG 20200511-10:29:55] epoch: 0, batch: 8405 train-loss: 2.502631902694702\n",
      "[LOG 20200511-10:29:55] epoch: 0, batch: 8406 train-loss: 0.7474197745323181\n",
      "[LOG 20200511-10:29:55] epoch: 0, batch: 8407 train-loss: 0.3617016673088074\n",
      "[LOG 20200511-10:29:55] epoch: 0, batch: 8408 train-loss: 1.0621614456176758\n",
      "[LOG 20200511-10:29:55] epoch: 0, batch: 8409 train-loss: 0.683722972869873\n",
      "[LOG 20200511-10:29:55] epoch: 0, batch: 8410 train-loss: 2.6393110752105713\n",
      "[LOG 20200511-10:29:55] epoch: 0, batch: 8411 train-loss: 1.9090888500213623\n",
      "[LOG 20200511-10:29:56] epoch: 0, batch: 8412 train-loss: 0.7651122212409973\n",
      "[LOG 20200511-10:29:56] epoch: 0, batch: 8413 train-loss: 1.8341645002365112\n",
      "[LOG 20200511-10:29:56] epoch: 0, batch: 8414 train-loss: 1.4718186855316162\n",
      "[LOG 20200511-10:29:56] epoch: 0, batch: 8415 train-loss: 1.6307761669158936\n",
      "[LOG 20200511-10:29:56] epoch: 0, batch: 8416 train-loss: 0.8811827301979065\n",
      "[LOG 20200511-10:29:56] epoch: 0, batch: 8417 train-loss: 0.35668158531188965\n",
      "[LOG 20200511-10:29:56] epoch: 0, batch: 8418 train-loss: 0.9189687371253967\n",
      "[LOG 20200511-10:29:57] epoch: 0, batch: 8419 train-loss: 1.229569673538208\n",
      "[LOG 20200511-10:29:57] epoch: 0, batch: 8420 train-loss: 1.2385168075561523\n",
      "[LOG 20200511-10:29:57] epoch: 0, batch: 8421 train-loss: 2.866811752319336\n",
      "[LOG 20200511-10:29:57] epoch: 0, batch: 8422 train-loss: 1.8791003227233887\n",
      "[LOG 20200511-10:29:57] epoch: 0, batch: 8423 train-loss: 0.8126101493835449\n",
      "[LOG 20200511-10:29:57] epoch: 0, batch: 8424 train-loss: 1.1856861114501953\n",
      "[LOG 20200511-10:29:57] epoch: 0, batch: 8425 train-loss: 1.492659330368042\n",
      "[LOG 20200511-10:29:57] epoch: 0, batch: 8426 train-loss: 1.5125055313110352\n",
      "[LOG 20200511-10:29:58] epoch: 0, batch: 8427 train-loss: 2.4817001819610596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:29:58] epoch: 0, batch: 8428 train-loss: 0.6998906135559082\n",
      "[LOG 20200511-10:29:58] epoch: 0, batch: 8429 train-loss: 1.9454131126403809\n",
      "[LOG 20200511-10:29:58] epoch: 0, batch: 8430 train-loss: 1.4381816387176514\n",
      "[LOG 20200511-10:29:58] epoch: 0, batch: 8431 train-loss: 1.5521094799041748\n",
      "[LOG 20200511-10:29:58] epoch: 0, batch: 8432 train-loss: 0.7909881472587585\n",
      "[LOG 20200511-10:29:58] epoch: 0, batch: 8433 train-loss: 1.583400011062622\n",
      "[LOG 20200511-10:29:58] epoch: 0, batch: 8434 train-loss: 0.9463273882865906\n",
      "[LOG 20200511-10:29:59] epoch: 0, batch: 8435 train-loss: 2.0357630252838135\n",
      "[LOG 20200511-10:29:59] epoch: 0, batch: 8436 train-loss: 1.506422996520996\n",
      "[LOG 20200511-10:29:59] epoch: 0, batch: 8437 train-loss: 1.05874502658844\n",
      "[LOG 20200511-10:29:59] epoch: 0, batch: 8438 train-loss: 1.3074443340301514\n",
      "[LOG 20200511-10:29:59] epoch: 0, batch: 8439 train-loss: 1.7324748039245605\n",
      "[LOG 20200511-10:29:59] epoch: 0, batch: 8440 train-loss: 0.6902514696121216\n",
      "[LOG 20200511-10:29:59] epoch: 0, batch: 8441 train-loss: 1.1143064498901367\n",
      "[LOG 20200511-10:29:59] epoch: 0, batch: 8442 train-loss: 1.352767825126648\n",
      "[LOG 20200511-10:30:00] epoch: 0, batch: 8443 train-loss: 1.5094923973083496\n",
      "[LOG 20200511-10:30:00] epoch: 0, batch: 8444 train-loss: 1.4510748386383057\n",
      "[LOG 20200511-10:30:00] epoch: 0, batch: 8445 train-loss: 1.5664520263671875\n",
      "[LOG 20200511-10:30:00] epoch: 0, batch: 8446 train-loss: 1.532193899154663\n",
      "[LOG 20200511-10:30:00] epoch: 0, batch: 8447 train-loss: 1.4012542963027954\n",
      "[LOG 20200511-10:30:00] epoch: 0, batch: 8448 train-loss: 1.8490986824035645\n",
      "[LOG 20200511-10:30:00] epoch: 0, batch: 8449 train-loss: 1.1213942766189575\n",
      "[LOG 20200511-10:30:00] epoch: 0, batch: 8450 train-loss: 1.4647912979125977\n",
      "[LOG 20200511-10:30:01] epoch: 0, batch: 8451 train-loss: 1.3876200914382935\n",
      "[LOG 20200511-10:30:01] epoch: 0, batch: 8452 train-loss: 1.6217713356018066\n",
      "[LOG 20200511-10:30:01] epoch: 0, batch: 8453 train-loss: 1.2738988399505615\n",
      "[LOG 20200511-10:30:01] epoch: 0, batch: 8454 train-loss: 1.1591624021530151\n",
      "[LOG 20200511-10:30:01] epoch: 0, batch: 8455 train-loss: 0.6414386034011841\n",
      "[LOG 20200511-10:30:01] epoch: 0, batch: 8456 train-loss: 1.3554259538650513\n",
      "[LOG 20200511-10:30:01] epoch: 0, batch: 8457 train-loss: 1.411393165588379\n",
      "[LOG 20200511-10:30:01] epoch: 0, batch: 8458 train-loss: 0.36525529623031616\n",
      "[LOG 20200511-10:30:01] epoch: 0, batch: 8459 train-loss: 1.9481236934661865\n",
      "[LOG 20200511-10:30:02] epoch: 0, batch: 8460 train-loss: 0.36620885133743286\n",
      "[LOG 20200511-10:30:02] epoch: 0, batch: 8461 train-loss: 0.8769499659538269\n",
      "[LOG 20200511-10:30:02] epoch: 0, batch: 8462 train-loss: 0.2911083996295929\n",
      "[LOG 20200511-10:30:02] epoch: 0, batch: 8463 train-loss: 1.489502191543579\n",
      "[LOG 20200511-10:30:02] epoch: 0, batch: 8464 train-loss: 1.3527617454528809\n",
      "[LOG 20200511-10:30:02] epoch: 0, batch: 8465 train-loss: 1.0370227098464966\n",
      "[LOG 20200511-10:30:02] epoch: 0, batch: 8466 train-loss: 1.0653342008590698\n",
      "[LOG 20200511-10:30:02] epoch: 0, batch: 8467 train-loss: 0.9274585247039795\n",
      "[LOG 20200511-10:30:02] epoch: 0, batch: 8468 train-loss: 0.7424153685569763\n",
      "[LOG 20200511-10:30:03] epoch: 0, batch: 8469 train-loss: 1.3448951244354248\n",
      "[LOG 20200511-10:30:03] epoch: 0, batch: 8470 train-loss: 0.2130325734615326\n",
      "[LOG 20200511-10:30:03] epoch: 0, batch: 8471 train-loss: 1.2798582315444946\n",
      "[LOG 20200511-10:30:03] epoch: 0, batch: 8472 train-loss: 2.302824020385742\n",
      "[LOG 20200511-10:30:03] epoch: 0, batch: 8473 train-loss: 1.0366570949554443\n",
      "[LOG 20200511-10:30:03] epoch: 0, batch: 8474 train-loss: 0.5483560562133789\n",
      "[LOG 20200511-10:30:03] epoch: 0, batch: 8475 train-loss: 1.5976190567016602\n",
      "[LOG 20200511-10:30:03] epoch: 0, batch: 8476 train-loss: 0.6266589760780334\n",
      "[LOG 20200511-10:30:03] epoch: 0, batch: 8477 train-loss: 2.1100521087646484\n",
      "[LOG 20200511-10:30:04] epoch: 0, batch: 8478 train-loss: 1.6690715551376343\n",
      "[LOG 20200511-10:30:04] epoch: 0, batch: 8479 train-loss: 0.3014107942581177\n",
      "[LOG 20200511-10:30:04] epoch: 0, batch: 8480 train-loss: 0.5469241142272949\n",
      "[LOG 20200511-10:30:04] epoch: 0, batch: 8481 train-loss: 0.4320593476295471\n",
      "[LOG 20200511-10:30:04] epoch: 0, batch: 8482 train-loss: 1.1204793453216553\n",
      "[LOG 20200511-10:30:04] epoch: 0, batch: 8483 train-loss: 0.9396135807037354\n",
      "[LOG 20200511-10:30:04] epoch: 0, batch: 8484 train-loss: 1.0972458124160767\n",
      "[LOG 20200511-10:30:04] epoch: 0, batch: 8485 train-loss: 1.3998260498046875\n",
      "[LOG 20200511-10:30:04] epoch: 0, batch: 8486 train-loss: 2.204049825668335\n",
      "[LOG 20200511-10:30:04] epoch: 0, batch: 8487 train-loss: 1.0308432579040527\n",
      "[LOG 20200511-10:30:05] epoch: 0, batch: 8488 train-loss: 0.7580777406692505\n",
      "[LOG 20200511-10:30:05] epoch: 0, batch: 8489 train-loss: 1.384689450263977\n",
      "[LOG 20200511-10:30:05] epoch: 0, batch: 8490 train-loss: 1.5752865076065063\n",
      "[LOG 20200511-10:30:05] epoch: 0, batch: 8491 train-loss: 1.0418576002120972\n",
      "[LOG 20200511-10:30:05] epoch: 0, batch: 8492 train-loss: 1.8751859664916992\n",
      "[LOG 20200511-10:30:05] epoch: 0, batch: 8493 train-loss: 1.0842939615249634\n",
      "[LOG 20200511-10:30:05] epoch: 0, batch: 8494 train-loss: 1.8803216218948364\n",
      "[LOG 20200511-10:30:05] epoch: 0, batch: 8495 train-loss: 0.9276864528656006\n",
      "[LOG 20200511-10:30:05] epoch: 0, batch: 8496 train-loss: 0.9893947243690491\n",
      "[LOG 20200511-10:30:06] epoch: 0, batch: 8497 train-loss: 1.8018425703048706\n",
      "[LOG 20200511-10:30:06] epoch: 0, batch: 8498 train-loss: 1.8834881782531738\n",
      "[LOG 20200511-10:30:06] epoch: 0, batch: 8499 train-loss: 0.8818613290786743\n",
      "[LOG 20200511-10:30:06] epoch: 0, batch: 8500 train-loss: 1.898794412612915\n",
      "[LOG 20200511-10:30:06] epoch: 0, batch: 8501 train-loss: 0.5128415822982788\n",
      "[LOG 20200511-10:30:06] epoch: 0, batch: 8502 train-loss: 1.619657039642334\n",
      "[LOG 20200511-10:30:06] epoch: 0, batch: 8503 train-loss: 1.1602047681808472\n",
      "[LOG 20200511-10:30:06] epoch: 0, batch: 8504 train-loss: 1.3907089233398438\n",
      "[LOG 20200511-10:30:07] epoch: 0, batch: 8505 train-loss: 2.3308768272399902\n",
      "[LOG 20200511-10:30:07] epoch: 0, batch: 8506 train-loss: 1.1543734073638916\n",
      "[LOG 20200511-10:30:07] epoch: 0, batch: 8507 train-loss: 0.9583434462547302\n",
      "[LOG 20200511-10:30:07] epoch: 0, batch: 8508 train-loss: 1.6805996894836426\n",
      "[LOG 20200511-10:30:07] epoch: 0, batch: 8509 train-loss: 1.5614013671875\n",
      "[LOG 20200511-10:30:07] epoch: 0, batch: 8510 train-loss: 2.7526373863220215\n",
      "[LOG 20200511-10:30:07] epoch: 0, batch: 8511 train-loss: 1.6466079950332642\n",
      "[LOG 20200511-10:30:07] epoch: 0, batch: 8512 train-loss: 0.9870296716690063\n",
      "[LOG 20200511-10:30:07] epoch: 0, batch: 8513 train-loss: 2.318122386932373\n",
      "[LOG 20200511-10:30:08] epoch: 0, batch: 8514 train-loss: 1.1106996536254883\n",
      "[LOG 20200511-10:30:08] epoch: 0, batch: 8515 train-loss: 1.020960807800293\n",
      "[LOG 20200511-10:30:08] epoch: 0, batch: 8516 train-loss: 2.2632412910461426\n",
      "[LOG 20200511-10:30:08] epoch: 0, batch: 8517 train-loss: 0.5143769979476929\n",
      "[LOG 20200511-10:30:08] epoch: 0, batch: 8518 train-loss: 1.5442962646484375\n",
      "[LOG 20200511-10:30:08] epoch: 0, batch: 8519 train-loss: 1.2292602062225342\n",
      "[LOG 20200511-10:30:08] epoch: 0, batch: 8520 train-loss: 1.92142653465271\n",
      "[LOG 20200511-10:30:08] epoch: 0, batch: 8521 train-loss: 1.9998350143432617\n",
      "[LOG 20200511-10:30:08] epoch: 0, batch: 8522 train-loss: 1.2323832511901855\n",
      "[LOG 20200511-10:30:09] epoch: 0, batch: 8523 train-loss: 1.0428433418273926\n",
      "[LOG 20200511-10:30:09] epoch: 0, batch: 8524 train-loss: 1.3363761901855469\n",
      "[LOG 20200511-10:30:09] epoch: 0, batch: 8525 train-loss: 0.8873199224472046\n",
      "[LOG 20200511-10:30:09] epoch: 0, batch: 8526 train-loss: 1.0089619159698486\n",
      "[LOG 20200511-10:30:09] epoch: 0, batch: 8527 train-loss: 1.9192968606948853\n",
      "[LOG 20200511-10:30:09] epoch: 0, batch: 8528 train-loss: 0.18308043479919434\n",
      "[LOG 20200511-10:30:09] epoch: 0, batch: 8529 train-loss: 1.0365538597106934\n",
      "[LOG 20200511-10:30:09] epoch: 0, batch: 8530 train-loss: 1.8137339353561401\n",
      "[LOG 20200511-10:30:09] epoch: 0, batch: 8531 train-loss: 0.32268452644348145\n",
      "[LOG 20200511-10:30:09] epoch: 0, batch: 8532 train-loss: 2.1743736267089844\n",
      "[LOG 20200511-10:30:10] epoch: 0, batch: 8533 train-loss: 1.2035504579544067\n",
      "[LOG 20200511-10:30:10] epoch: 0, batch: 8534 train-loss: 1.1311397552490234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20200511-10:30:10] epoch: 0, batch: 8535 train-loss: 0.2938361167907715\n",
      "[LOG 20200511-10:30:10] epoch: 0, batch: 8536 train-loss: 1.772860050201416\n",
      "[LOG 20200511-10:30:10] epoch: 0, batch: 8537 train-loss: 1.1342551708221436\n",
      "[LOG 20200511-10:30:10] epoch: 0, batch: 8538 train-loss: 0.9195014834403992\n",
      "[LOG 20200511-10:30:10] epoch: 0, batch: 8539 train-loss: 1.1286835670471191\n",
      "[LOG 20200511-10:30:10] epoch: 0, batch: 8540 train-loss: 1.355498194694519\n",
      "[LOG 20200511-10:30:10] epoch: 0, batch: 8541 train-loss: 1.2202130556106567\n",
      "[LOG 20200511-10:30:11] epoch: 0, batch: 8542 train-loss: 1.7832235097885132\n",
      "[LOG 20200511-10:30:11] epoch: 0, batch: 8543 train-loss: 1.0560475587844849\n",
      "[LOG 20200511-10:30:11] epoch: 0, batch: 8544 train-loss: 1.2124303579330444\n",
      "[LOG 20200511-10:30:11] epoch: 0, batch: 8545 train-loss: 1.555751919746399\n",
      "[LOG 20200511-10:30:11] epoch: 0, batch: 8546 train-loss: 1.1636168956756592\n",
      "[LOG 20200511-10:30:11] epoch: 0, batch: 8547 train-loss: 0.8109356164932251\n",
      "[LOG 20200511-10:30:11] epoch: 0, batch: 8548 train-loss: 0.8038979768753052\n",
      "[LOG 20200511-10:30:11] epoch: 0, batch: 8549 train-loss: 1.133495569229126\n",
      "[LOG 20200511-10:30:11] epoch: 0, batch: 8550 train-loss: 1.0563714504241943\n",
      "[LOG 20200511-10:30:12] epoch: 0, batch: 8551 train-loss: 0.7691904306411743\n",
      "[LOG 20200511-10:30:12] epoch: 0, batch: 8552 train-loss: 1.9683358669281006\n",
      "[LOG 20200511-10:30:12] epoch: 0, batch: 8553 train-loss: 0.927246630191803\n",
      "[LOG 20200511-10:30:12] epoch: 0, batch: 8554 train-loss: 0.5090497732162476\n",
      "[LOG 20200511-10:30:12] epoch: 0, batch: 8555 train-loss: 0.890119731426239\n",
      "[LOG 20200511-10:30:12] epoch: 0, batch: 8556 train-loss: 1.6461986303329468\n",
      "[LOG 20200511-10:30:12] epoch: 0, batch: 8557 train-loss: 0.7870938181877136\n",
      "[LOG 20200511-10:30:12] epoch: 0, batch: 8558 train-loss: 0.6703550815582275\n",
      "[LOG 20200511-10:30:12] epoch: 0, batch: 8559 train-loss: 0.5338315963745117\n",
      "[LOG 20200511-10:30:13] epoch: 0, batch: 8560 train-loss: 1.1731523275375366\n",
      "[LOG 20200511-10:30:13] epoch: 0, batch: 8561 train-loss: 1.1301426887512207\n",
      "[LOG 20200511-10:30:13] epoch: 0, batch: 8562 train-loss: 0.8745297193527222\n",
      "[LOG 20200511-10:30:13] epoch: 0, batch: 8563 train-loss: 0.833510160446167\n",
      "[LOG 20200511-10:30:13] epoch: 0, batch: 8564 train-loss: 1.5944256782531738\n",
      "[LOG 20200511-10:30:13] epoch: 0, batch: 8565 train-loss: 0.6540497541427612\n",
      "[LOG 20200511-10:30:13] epoch: 0, batch: 8566 train-loss: 1.901327133178711\n",
      "[LOG 20200511-10:30:13] epoch: 0, batch: 8567 train-loss: 0.6006709933280945\n",
      "[LOG 20200511-10:30:13] epoch: 0, batch: 8568 train-loss: 1.7324378490447998\n",
      "[LOG 20200511-10:30:13] epoch: 0, batch: 8569 train-loss: 0.8883469104766846\n",
      "[LOG 20200511-10:30:14] epoch: 0, batch: 8570 train-loss: 1.3817410469055176\n",
      "[LOG 20200511-10:30:14] epoch: 0, batch: 8571 train-loss: 0.7871184349060059\n",
      "[LOG 20200511-10:30:14] epoch: 0, batch: 8572 train-loss: 0.7743219137191772\n",
      "[LOG 20200511-10:30:14] epoch: 0, batch: 8573 train-loss: 1.4347612857818604\n",
      "[LOG 20200511-10:30:14] epoch: 0, batch: 8574 train-loss: 1.0614699125289917\n",
      "[LOG 20200511-10:30:14] epoch: 0, batch: 8575 train-loss: 1.078385591506958\n",
      "[LOG 20200511-10:30:14] epoch: 0, batch: 8576 train-loss: 0.9415895342826843\n",
      "[LOG 20200511-10:30:14] epoch: 0, batch: 8577 train-loss: 0.7142413854598999\n",
      "[LOG 20200511-10:30:14] epoch: 0, batch: 8578 train-loss: 2.2190420627593994\n",
      "[LOG 20200511-10:30:15] epoch: 0, batch: 8579 train-loss: 1.1302716732025146\n",
      "[LOG 20200511-10:30:15] epoch: 0, batch: 8580 train-loss: 1.1164475679397583\n",
      "[LOG 20200511-10:30:15] epoch: 0, batch: 8581 train-loss: 2.3864336013793945\n",
      "[LOG 20200511-10:30:15] epoch: 0, batch: 8582 train-loss: 0.6849701404571533\n",
      "[LOG 20200511-10:30:15] epoch: 0, batch: 8583 train-loss: 0.5554795861244202\n",
      "[LOG 20200511-10:30:15] epoch: 0, batch: 8584 train-loss: 1.3404337167739868\n",
      "[LOG 20200511-10:30:15] epoch: 0, batch: 8585 train-loss: 2.3623290061950684\n",
      "[LOG 20200511-10:30:15] epoch: 0, batch: 8586 train-loss: 1.2401057481765747\n",
      "[LOG 20200511-10:30:15] epoch: 0, batch: 8587 train-loss: 1.2736097574234009\n",
      "[LOG 20200511-10:30:16] epoch: 0, batch: 8588 train-loss: 0.6312092542648315\n",
      "[LOG 20200511-10:30:16] epoch: 0, batch: 8589 train-loss: 0.8548943996429443\n",
      "[LOG 20200511-10:30:16] epoch: 0, batch: 8590 train-loss: 1.754535436630249\n",
      "[LOG 20200511-10:30:16] epoch: 0, batch: 8591 train-loss: 0.5587525367736816\n",
      "[LOG 20200511-10:30:16] epoch: 0, batch: 8592 train-loss: 0.9894516468048096\n",
      "[LOG 20200511-10:30:16] epoch: 0, batch: 8593 train-loss: 0.6038742661476135\n",
      "[LOG 20200511-10:30:16] epoch: 0, batch: 8594 train-loss: 1.2983887195587158\n",
      "[LOG 20200511-10:30:16] epoch: 0, batch: 8595 train-loss: 1.494147539138794\n",
      "[LOG 20200511-10:30:16] epoch: 0, batch: 8596 train-loss: 2.810385227203369\n",
      "[LOG 20200511-10:30:17] epoch: 0, batch: 8597 train-loss: 1.866057276725769\n",
      "[LOG 20200511-10:30:17] epoch: 0, batch: 8598 train-loss: 1.42372465133667\n",
      "[LOG 20200511-10:30:17] epoch: 0, batch: 8599 train-loss: 1.7085506916046143\n",
      "[LOG 20200511-10:30:17] epoch: 0, batch: 8600 train-loss: 1.3379361629486084\n",
      "[LOG 20200511-10:30:17] epoch: 0, batch: 8601 train-loss: 1.1539040803909302\n",
      "[LOG 20200511-10:30:17] epoch: 0, batch: 8602 train-loss: 1.412602424621582\n",
      "[LOG 20200511-10:30:17] epoch: 0, batch: 8603 train-loss: 0.7740582823753357\n",
      "[LOG 20200511-10:30:17] epoch: 0, batch: 8604 train-loss: 0.9363977909088135\n",
      "[LOG 20200511-10:30:17] epoch: 0, batch: 8605 train-loss: 1.862001895904541\n",
      "[LOG 20200511-10:30:18] epoch: 0, batch: 8606 train-loss: 0.618658721446991\n",
      "[LOG 20200511-10:30:18] epoch: 0, batch: 8607 train-loss: 1.5826029777526855\n",
      "[LOG 20200511-10:30:18] epoch: 0, batch: 8608 train-loss: 1.610195279121399\n",
      "[LOG 20200511-10:30:18] epoch: 0, batch: 8609 train-loss: 1.114592432975769\n",
      "[LOG 20200511-10:30:18] epoch: 0, batch: 8610 train-loss: 1.3202484846115112\n",
      "[LOG 20200511-10:30:18] epoch: 0, batch: 8611 train-loss: 0.6194114089012146\n",
      "[LOG 20200511-10:30:18] epoch: 0, batch: 8612 train-loss: 0.5740794539451599\n",
      "[LOG 20200511-10:30:18] epoch: 0, batch: 8613 train-loss: 0.5518889427185059\n",
      "[LOG 20200511-10:30:18] epoch: 0, batch: 8614 train-loss: 1.2849763631820679\n",
      "[LOG 20200511-10:30:19] epoch: 0, batch: 8615 train-loss: 0.9626781940460205\n",
      "[LOG 20200511-10:30:19] epoch: 0, batch: 8616 train-loss: 1.1209728717803955\n",
      "[LOG 20200511-10:30:19] epoch: 0, batch: 8617 train-loss: 1.1265360116958618\n",
      "[LOG 20200511-10:30:19] epoch: 0, batch: 8618 train-loss: 0.8174383640289307\n",
      "[LOG 20200511-10:30:19] epoch: 0, batch: 8619 train-loss: 1.2375829219818115\n",
      "[LOG 20200511-10:30:19] epoch: 0, batch: 8620 train-loss: 0.935482919216156\n",
      "[LOG 20200511-10:30:19] epoch: 0, batch: 8621 train-loss: 1.3822758197784424\n",
      "[LOG 20200511-10:30:19] epoch: 0, batch: 8622 train-loss: 0.949687123298645\n",
      "[LOG 20200511-10:30:19] epoch: 0, batch: 8623 train-loss: 0.7166620492935181\n",
      "[LOG 20200511-10:30:20] epoch: 0, batch: 8624 train-loss: 1.6586039066314697\n",
      "[LOG 20200511-10:30:20] epoch: 0, batch: 8625 train-loss: 1.6009480953216553\n",
      "[LOG 20200511-10:30:20] epoch: 0, batch: 8626 train-loss: 0.8783612251281738\n",
      "[LOG 20200511-10:30:20] epoch: 0, batch: 8627 train-loss: 1.1458014249801636\n",
      "[LOG 20200511-10:30:20] epoch: 0, batch: 8628 train-loss: 1.3349941968917847\n",
      "[LOG 20200511-10:30:20] epoch: 0, batch: 8629 train-loss: 1.9012455940246582\n",
      "[LOG 20200511-10:30:20] epoch: 0, batch: 8630 train-loss: 2.1268372535705566\n",
      "[LOG 20200511-10:30:20] epoch: 0, batch: 8631 train-loss: 1.1712970733642578\n",
      "[LOG 20200511-10:30:21] epoch: 0, batch: 8632 train-loss: 1.4094434976577759\n",
      "[LOG 20200511-10:30:21] epoch: 0, batch: 8633 train-loss: 1.509455680847168\n",
      "[LOG 20200511-10:30:21] epoch: 0, batch: 8634 train-loss: 0.9840986132621765\n",
      "[LOG 20200511-10:30:21] epoch: 0, batch: 8635 train-loss: 1.030457854270935\n"
     ]
    }
   ],
   "source": [
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set the model in training mode\n",
    "resnet_model.train()\n",
    "\n",
    "# train the CIFAR10 model\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # init collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "    \n",
    "    # iterate over all-mini batches\n",
    "    for i, (images, labels) in enumerate(cifar10_train_dataloader):\n",
    "        \n",
    "        # push mini-batch data to computation device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # run forward pass through the network\n",
    "        output = resnet_model(images)\n",
    "        \n",
    "        # reset graph gradients\n",
    "        resnet_model.zero_grad()\n",
    "        \n",
    "        # determine classification loss\n",
    "        loss = nll_loss(output, labels)\n",
    "        \n",
    "        # run backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network paramaters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # collect mini-batch reconstruction loss\n",
    "        train_mini_batch_losses.append(loss.data.item())\n",
    "        \n",
    "        # print mini-batch loss\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "        print('[LOG {}] epoch: {}, batch: {} train-loss: {}'.format(str(now), str(epoch), str(i), str(loss.item())))\n",
    "\n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "    \n",
    "    # print epoch loss\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] epoch: {} train-loss: {}'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "    \n",
    "    # save model to local directory\n",
    "    model_name = 'cifar10_resnet_model_epoch_{}.pth'.format(str(epoch))\n",
    "    torch.save(resnet_model.state_dict(), os.path.join(\"./models\", model_name))\n",
    "    \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successfull training let's visualize and inspect the loss per training iteration (mini-batch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' classification error\n",
    "ax.plot(np.array(range(1, len(train_mini_batch_losses)+1)), train_mini_batch_losses, label='mini-batch loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[training mini-batch $mb_i$]\", fontsize=10)\n",
    "ax.set_ylabel(\"[Classification Error $\\mathcal{L}^{NLL}$]\", fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Iterations $mb_i$ vs. Classification Error $L^{NLL}$', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Evaluation of the Trained \"ResNet\" Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conduct our evaluation based on a model that was already pre-trained for a total of 200 training epochs. Remember, that we stored a snapshot of the model after each training epoch to our local model directory. We will now load a pre-trained snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore pre-trained model snapshot\n",
    "best_resnet_model_name = \"cifar10_resnet_model_epoch_199.pth\"\n",
    "\n",
    "# init pre-trained model class\n",
    "best_resnet_model = ResNet(layers=[2, 2, 2])\n",
    "\n",
    "# load pre-trained models\n",
    "best_resnet_model.load_state_dict(torch.load(os.path.join(\"models\", best_resnet_model_name), map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again inspect if the pre-trained model was loaded successfully: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set model in evaluation mode\n",
    "best_resnet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the `CIFAR10Net` above we will now evaluate the pre-trained `ResNet` model based on the mini-batches of the evaluation dataset to derive the mean negative log-likelihood loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init collection of mini-batch losses\n",
    "eval_mini_batch_losses = []\n",
    "\n",
    "# iterate over all-mini batches\n",
    "for i, (images, labels) in enumerate(cifar10_eval_dataloader):\n",
    "\n",
    "    # run forward pass through the network\n",
    "    output = best_resnet_model(images)\n",
    "\n",
    "    # determine classification loss\n",
    "    loss = nll_loss(output, labels)\n",
    "\n",
    "    # collect mini-batch reconstruction loss\n",
    "    eval_mini_batch_losses.append(loss.data.item())\n",
    "\n",
    "# determine mean min-batch loss of epoch\n",
    "eval_loss = np.mean(eval_mini_batch_losses)\n",
    "\n",
    "# print epoch loss\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] eval-loss: {}'.format(str(now), str(eval_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, great. The evaluation loss obtained by `ResNet` model looks significantly compared to the one achieved by the `CIFAR10Net` model. Let's now inspect a few sample predictions to get an impression of the model quality. Therefore, we will again pick a random image of our evaluation dataset and retrieve its PyTorch tensor as well as the corresponding label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a sample image id\n",
    "image_id = 11\n",
    "\n",
    "# obtain the image from the dataloader\n",
    "cifar10_eval_image = iter(cifar10_eval_dataloader).next()[0][image_id]\n",
    "\n",
    "# obtain the true label of the image\n",
    "label = cifar10_eval_data.targets[image_id]\n",
    "\n",
    "# obtain the model's class prediction\n",
    "prediction = torch.argmax(best_resnet_model(cifar10_eval_image.unsqueeze(0)), dim=1).item()\n",
    "\n",
    "# define tensor to image transformation\n",
    "trans = torchvision.transforms.ToPILImage()\n",
    "\n",
    "# set image plot title \n",
    "plt.title('Example: {}, Label: {}, Prediction: {}'.format(str(image_id), str(cifar10_classes[label]), str(cifar10_classes[prediction])))\n",
    "\n",
    "# un-normalize cifar 10 image sample\n",
    "cifar10_eval_image_plot = cifar10_eval_image / 2.0 + 0.5\n",
    "\n",
    "# plot cifar 10 image sample\n",
    "plt.imshow(trans(cifar10_eval_image_plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now obtain the predictions for all the `CIFAR10` images of the evaluation data using the pre-trained `ResNet`model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(best_resnet_model(iter(cifar10_eval_dataloader).next()[0]), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain the overall classification accuracy of the trained `ResNet` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(cifar10_eval_data.targets, predictions.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's inspect the confusion matrix of the model predictions to determine major sources of misclassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine classification matrix of the predicted and target classes\n",
    "mat = confusion_matrix(cifar10_eval_data.targets, predictions.detach())\n",
    "\n",
    "# plot corresponding confusion matrix\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='YlOrRd_r', xticklabels=cifar10_classes, yticklabels=cifar10_classes)\n",
    "plt.title('CIFAR-10 ResNet classification matrix')\n",
    "plt.xlabel('[true label]')\n",
    "plt.ylabel('[predicted label]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, a step by step introduction into **design, implementation, training and evaluation** of convolutional neural networks CNNs to classify tiny images of objects is presented. The code and exercises presented in this lab may serves as a starting point for developing more complex, deeper and more tailored CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Pls. note that to convert the notebook, you need to install Python's **nbconvert** library and its extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the nbconvert library\n",
    "!pip install nbconvert\n",
    "!pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert the Jupyter notebook into a plain Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script aiml_lab_06.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "254.39999389648438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
