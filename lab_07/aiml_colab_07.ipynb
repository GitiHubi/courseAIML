{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEdpHZuqcE2t"
   },
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"hsg_logo.png\">\n",
    "\n",
    "###  Lab 07 - \"Supervised Machine Learning - Logistic Regression\"\n",
    "\n",
    "Introduction to AI and ML, University of St. Gallen, Autumn Term 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMko_r4RdP79"
   },
   "source": [
    "Throughout the last labs, we had a look into three distinct supervised learning classifiers, namely the **Gaussian Naive-Bayes (GNB)**, the **k Nearest-Neighbors (kNN)** as well as the **Support Vector Machine (SVM)** classifier. You learned how to train models in a supervised training setup and to evaluate and interpret classification results. \n",
    "\n",
    "In this lab, we will shift gears and will have a closer look into a **non-linear regression** technique referred to as **Logistic Regression**. The **Logistic Regression** technique generalizes **Linear Regression** (that you learned about in the first module of the CFDS course) by applying a significant change to its computation. \n",
    "\n",
    "Similarly to linear regression it computes a linear combination of the input variables but passes the result through a non-linear logistic (or sigmoid) function afterwards. The addition of this non-linear function allows learning **non-linear relationships** between random input and target variables. As a result, this technique is part of the **non-linear** type of regression techniques, which can be distinguished from the **linear** type as shown in the following illustration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzTHNQzqSyKs"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 600px; height: auto\" src=\"classifiers.png\">\n",
    "\n",
    "(Courtesy: Intro to AI & ML lecture, Prof. Dr. Borth, University of St. Gallen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_DVTuEEUHPo"
   },
   "source": [
    "In this lab, we will learn a customer churn predictor using **Logistic Regression**. In general **Logistic Regression** is a supervised machine learning technique used in different application areas, such as (1) time series forecasting, trends identification, data smoothing and hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFl9v82TVC1D"
   },
   "source": [
    "This lab builds in parts on the tutorial created by `nitinkaushik01` on `Github`. The original tutorial can be accessed via the following URL: https://github.com/nitinkaushik01/Deep_and_Machine_Learning_Projects/blob/master/Churn_Prediction_of_Customers/Customer_Churn_Prediction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAcBQIzhV9Gi"
   },
   "source": [
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3OYmA4-WTYY"
   },
   "source": [
    "### Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ-PEgD9WVTI"
   },
   "source": [
    "After today's lab, you should be able to:\n",
    "\n",
    "> 1. Understand how a **Logistic Regression** classifier can be trained and evaluated.\n",
    "> 2. Transform raw data from an excel sheet to a machine learning-friendly **dataframe**.\n",
    "> 3. Train and evaluate discriminative **machine learning models** using Python's `scikit-learn` library.\n",
    "> 4. Understand how to **evaluate** and **interpret** the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbAILv-MXaej"
   },
   "source": [
    "Before we start let's watch a motivational video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "G-zSBJ1WXZXe",
    "outputId": "b475ab79-fde1-411b-c971-ae8077dc3a78"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# Microsoft: \"AI for Health Program\"\n",
    "# YouTubeVideo('ii-FfE-7C-k', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZs3pvyZcKuu"
   },
   "source": [
    "### Setup of the Analysis Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgH_b2JJ2oZZ"
   },
   "source": [
    "We need to import a couple of Python libraries that allow for data analysis and data visualization. In this lab will use the `Pandas`, `Numpy`, `Scikit-Learn`, `Matplotlib` and the `Seaborn` library. Let's import the libraries by the execution of the statements below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPQbH8pwcMhB"
   },
   "outputs": [],
   "source": [
    "# import the numpy and pandas data science library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import data pre-processing libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import Logistic Regression classifier library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import sklearn classification evaluation library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# import matplotlib data visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PKzHuHk2nSq"
   },
   "source": [
    "Set random seed of all our experiments - this insures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6QJqhBt2MRd"
   },
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kQGNctG3QH4"
   },
   "source": [
    "Enable inline Jupyter notebook plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjpyaOVS3QjV"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYztgsm13Q41"
   },
   "source": [
    "Use the 'Seaborn' plotting style in all subsequent visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ygFMwRW3RJd"
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EfwBAgHcPtG"
   },
   "source": [
    "### 1. Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLr_nozjY02v"
   },
   "source": [
    "#### 1.1 Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ity77fwa6_J"
   },
   "source": [
    "The **\"Telco Customer Churn Dataset\"** (obtainable from the Kaggle webpage: https://www.kaggle.com/blastchar/telco-customer-churn) encompasses information on clients of a telecommunication company. Each row represents a customer, and each column contains a customer’s attributes. The raw data contains **7043 rows** (customers) and **21 columns (or features)**. Thereby, the `Churn` column denotes the target information which we aim to predict. If for a given customer, the `Churn` value equals **\"Yes\"**, it means the customer left the company. A `Churn` value that corresponds to **\"No\"** indicates that the customer continued its relationship with the company. In detail the dataset is comprised of the following information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzYoDHcOd4-M"
   },
   "source": [
    "1. Services that each customer has signed up for:\n",
    ">- `PhoneService` - this corresponds to. \n",
    ">- `MultipleLines`\n",
    ">- `InternetService`\n",
    ">- `OnlineSecurity`\n",
    ">- `OnlineBackup`\n",
    ">- `DeviceProtection`\n",
    ">- `TechSupport`\n",
    ">- `StreamingTV`\n",
    ">- `StreamingMovies`\n",
    "\n",
    "2. Customer account information:\n",
    ">- `customerID`\n",
    ">- `tenure` (how long they've been a customer)\n",
    ">- `Contract`\n",
    ">- `PaymentMethod`\n",
    ">- `PaperlessBilling`\n",
    ">- `MonthlyCharges`\n",
    ">- `TotalCharges`\n",
    "\n",
    "3. Demographic info about customers:\n",
    ">- `gender`\n",
    ">- `SeniorCitizen`\n",
    ">- `Partner`\n",
    ">- `Dependents` (are they financially dependent on someone else)\n",
    "\n",
    "The to be predicted target: `Churn` - customers who left within the last month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yy1QqS99ESeZ"
   },
   "source": [
    "\n",
    "Let's load the dataset and conduct a preliminary data assessment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4CCedJCex0J"
   },
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(\"telco_customer_churn_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I33R2vmEdQI"
   },
   "source": [
    "See whether the size of the dataframe is as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "h1t6oba-EeXY",
    "outputId": "dd1a2238-0b2e-4553-dce3-d30e5f6256d6"
   },
   "outputs": [],
   "source": [
    "original_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeY6p4cvFCcR"
   },
   "source": [
    "Take a look at the different features (columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "88lERuqtFGdH",
    "outputId": "de9a5047-6ba6-4dd9-bded-13229c83f620"
   },
   "outputs": [],
   "source": [
    "original_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QC63PscFOPD"
   },
   "source": [
    "Print and inspect the top 10 feature rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "iRbHBXCSHrgH",
    "outputId": "54473cc0-bb79-4998-f35e-2667bd88747e"
   },
   "outputs": [],
   "source": [
    "original_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "645VXBY8HGbt"
   },
   "source": [
    "As can be seen from the dataframe, some features express their values in categorical terms, rather than in nominal terms. \n",
    "\n",
    "For instance, the `gender` column takes values `Female`/`Male`. Others take `Yes`/`No`, or even more specific values: the `PaymentMethod` column has `Mailed check`/`Bank transfer (automatic)`/`Credit card (automatic)`/`Electonic check` as values.\n",
    "\n",
    "In general, the **Logistic Regression** technique model is designed to process numerical information only. We, therefore, have to pre-process the data to learn a model from it and ultimately make predictions.\n",
    "\n",
    "Let's first verify what are **numerical** features and what are **non-numerical** features in the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "m80Cpu4SIJWh",
    "outputId": "6df3789c-c97f-4647-83cc-5aa4fb830763"
   },
   "outputs": [],
   "source": [
    "# to obtain all categorically valued features, take all columns with 'object' type\n",
    "cats = list(original_data.select_dtypes(include=['object']).columns)\n",
    "\n",
    "# print both lists of feature types\n",
    "print('Categorical features in the dataset: {}'.format(cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain all numerically valued features, exclude all columns with 'object' type\n",
    "nums = list(original_data.select_dtypes(exclude=['object']).columns)\n",
    "\n",
    "# print both lists of feature types\n",
    "print('Numerical features in the dataset: {}'.format(nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAnwDlljMFj_"
   },
   "source": [
    "Surprisingly the `TotalCharges` feature seems not to be contained in the list of numerical features. However, when inspecting the `dataframe` of the original data, it seems that the feature should be numerical. To investigate this further let's verify how many unique feature values each categorical feature corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "HBG7cz-UPCX_",
    "outputId": "c4fde393-8b57-4adb-8d58-5ad23fc6efd9"
   },
   "outputs": [],
   "source": [
    "# determine number of unique categorical feature values\n",
    "original_data[cats].nunique(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPVESA55PcPL"
   },
   "source": [
    "Ok, there are 7043 unique values for the `customerID` feature, which makes sense since this column refers to a unique identifier of the distinct customers. Interestingly, we observe **6531** different feature values for the `TotalCharges` feature. \n",
    "\n",
    "It seems that there are a lot of instances where the `TotalCharges` feature exhibits a different value for different customers. Let's, therefore, verify if potentially missing values might cause this, e.g. instances were the feature might be filled with a whitespace only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "RX6tuiDKQN0X",
    "outputId": "6779a825-ae36-4718-fd89-3ab78d160337"
   },
   "outputs": [],
   "source": [
    "# check how many values are filled with spaces for each column\n",
    "original_data.isin([\" \"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85UYYxPEQVjQ"
   },
   "source": [
    "We found eleven instances in which the `TotalCharges` column just contained a single whitespace. As a result, the column is interpreted as non-numerical by the `Pandas` library. To resolve this challenge, we can (i) either replace the missing data with an arbitrary value or (ii) drop them. For the sake of simplicity, we will do the latter in the following data pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jL1yw3E8RHMH"
   },
   "source": [
    "Finally, let's also inspect what are the distinct feature values of all the other categorical features, except the `customerID` identifier and `TotalCharges` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "HUhTQkpge0IF",
    "outputId": "06486aa8-9fe0-4c68-b883-19f54c2fb1d9"
   },
   "outputs": [],
   "source": [
    "# creating a list with all categorically valued columns except the customerID and TotalCharges\n",
    "cats = [cat for cat in cats if cat not in[\"customerID\", \"TotalCharges\"]]\n",
    "\n",
    "# iterate over all categorical features\n",
    "for col in cats:\n",
    "    \n",
    "    # print all feature values of current categorical feature\n",
    "    print('{} : {}'.format(col, original_data[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6oGbkKvRX_t"
   },
   "source": [
    "Now we know what feature type each column in the original dataset corresponds to, and we discovered some whitespace (generally referred to as **null**) values in the `TotalCharges` column. To gain an even more in-depth intuition of the data, let's visualize the distinct feature value distributions in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViY645pEb6Vz"
   },
   "source": [
    "#### 3.1.1 Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWSsTpoQieKh"
   },
   "source": [
    "To visualize the distinct feature value distributions, we will make use of the 'cats' list, which we initially created. Remember the list contains the names of all categorically features except the `TotalCharges` and `customerID` feature. \n",
    "\n",
    "We will also add the `SeniorCitizen` feature to the list. Having a closer look at the `SeniorCitizen` reveals that feature can also be interpreted as a categorical feature since it is flagging a customer is a **senior** (indicated by **\"1\"**) or **not senior** (indicated by **\"0\"**). Therefore, we will insert the `SeniorCitizen` feature into the list of categorical features at **index 15** just before the `Churn` feature (since we want to keep the `Churn` feature last in our visualizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LY7eTZxrflHm"
   },
   "outputs": [],
   "source": [
    "# case: the \"SeniorCitizen\" feature is not contained in the \"cats\" list of categorical features\n",
    "if 'SeniorCitizen' not in cats:\n",
    "    \n",
    "    # insert \"SeniorCitizen\" feature into the \"cats\" list at index 15\n",
    "    cats.insert(15,'SeniorCitizen')\n",
    "\n",
    "# case: the \"SeniorCitizen\" feature is contained in the \"cats\" list of categorical features\n",
    "else:\n",
    "    \n",
    "    # do nothing\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjVcGWO6jXVc"
   },
   "source": [
    "Now let's visualize the value distributions of the distinct categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xlqbLWrrdaTE",
    "outputId": "a171091c-b178-4aa7-d1a7-2ccb0a60dbbb"
   },
   "outputs": [],
   "source": [
    "# define a new figure and its size\n",
    "fig = plt.figure(1, (18, 24))\n",
    "\n",
    "# iterate over the list of categorical feature names\n",
    "for i,cat in enumerate(cats):\n",
    "    \n",
    "    # create a subplot for each categorical feature\n",
    "    ax = plt.subplot(9, 2, i+1)\n",
    "    \n",
    "    # count the distribution in values for each categorical feature\n",
    "    feature_value_count = original_data[cat].value_counts(normalize=True)\n",
    "    \n",
    "    # rename the feature value count axes labels and reset the index\n",
    "    feature_value_count = feature_value_count.rename(\"percentage\").reset_index()\n",
    "    \n",
    "    # re-scale the percentages of feature value counts\n",
    "    feature_value_count[\"percentage\"] = feature_value_count[\"percentage\"] * 100.0\n",
    "    \n",
    "    # create a histogram (barplot) of the distinct feature value counts\n",
    "    sns.barplot(x=\"index\", y=\"percentage\", data=feature_value_count)\n",
    "    \n",
    "    # set the label for the subplot's x-axis to None\n",
    "    ax.set_xlabel(None)\n",
    "    \n",
    "    # add a title to the subplot\n",
    "    ax.set_title('{}. Feature Value Distribution of the \\\"{}\\\" Feature'.format(i+1, cat), fontsize=14)\n",
    "    \n",
    "    # adjust the space (padding) between the subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "# show the final figure containing all subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "267Yhv96cE6b"
   },
   "source": [
    "Next, let's also visually examine how each feature is distributed with regards to its influence on the to be predicted target, namely the `Churn` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-gO7k26ho9sS",
    "outputId": "41b9898c-143c-4c80-c711-dcf4b1cdba35"
   },
   "outputs": [],
   "source": [
    "# Define a new figure and its size\n",
    "fig = plt.figure(1, (18, 24))\n",
    "\n",
    "# iterate over the list of categorical feature names\n",
    "for i,cat in enumerate(cats):\n",
    "    \n",
    "    # case: not the \"Churn\" column\n",
    "    if not cat == \"Churn\":\n",
    "        \n",
    "        # create a subplot for each categorical feature\n",
    "        ax = plt.subplot(9, 2, i+1)\n",
    "        \n",
    "        # count the distribution in values for each categorical feature in relation to the Churn column (->.groupby())\n",
    "        feature_value_count = original_data[cat].groupby(original_data[\"Churn\"]).value_counts(normalize=True)\n",
    "        \n",
    "        # rename the feature value count axes labels and reset the index\n",
    "        feature_value_count = feature_value_count.rename(\"percentage\").reset_index()\n",
    "        \n",
    "        # Use integers for the percentage\n",
    "        feature_value_count[\"percentage\"] = feature_value_count[\"percentage\"] * 100.0\n",
    "        \n",
    "        # create a histogram (barplot) of the distinct feature value counts\n",
    "        sns.barplot(x=cat, y=\"percentage\",hue=\"Churn\", data=feature_value_count)\n",
    "        \n",
    "        # set the label for the subplot's x-axis to None\n",
    "        ax.set_xlabel(None)\n",
    "\n",
    "        # add a title to the subplot\n",
    "        ax.set_title('{}. Feature Value Distribution of the \\\"{}\\\" Feature'.format(i+1, cat), fontsize=14)\n",
    "\n",
    "        # adjust the space (padding) between the subplots\n",
    "        plt.tight_layout()\n",
    "\n",
    "# show the figure which contains all subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQLvrlcvYU-x"
   },
   "source": [
    "When inspecting the created visualizations it can be observed that, for instance, the `gender` feature (shown in plot no 1.)  exhibits a significant lower influence on customer churning compared to the `Contract` feature (shown in plot no 13.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP8l3f0HY9xr"
   },
   "source": [
    "### 3.2 Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i383BHMhZGB6"
   },
   "source": [
    "#### 3.2.1 Value conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXJ4eBejJSeI"
   },
   "source": [
    "As studied previously, a lot of columns have categorical variables (yes/no, female/male etc..). We must convert these values to a numerical format. Moreover, we must drop the null values from the `TotalCharges` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMK8aDFbAYvn"
   },
   "outputs": [],
   "source": [
    "# Renaming the dataset so as to not create any interference when re-running the code\n",
    "intermediate_data = original_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxZ4KPvjoXzE"
   },
   "source": [
    "First, to be able to convert the categorically expressed features to numerically expressed features, some columns take `Yes`, `No` and `No internet service` as values. We can treat `No internet service` as if it were `No` to simplify things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOJsutmKLjpD"
   },
   "outputs": [],
   "source": [
    "# These columns contains 'Yes', 'No' and 'No internet service'\n",
    "cols = ['OnlineBackup', 'StreamingMovies','DeviceProtection', 'TechSupport','OnlineSecurity','StreamingTV']\n",
    "\n",
    "# Convert 'No internet service' to 'No' for the above mentioned columns\n",
    "for i in cols : \n",
    "    intermediate_data[i] = intermediate_data[i].replace({'No internet service' : 'No'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ek9XWKY9pGoD"
   },
   "source": [
    "Second, let's deal with the missing values from the `TotalCharges` column. As they are not technically null values (we can assume they consist of a space, \" \"), we will convert them to NaN values. NaN stands for Not a Number - more info at https://en.wikipedia.org/wiki/NaN.\n",
    "\n",
    "After this conversion, we can safely drop these NaN values and re-index the column. Lastly, we will make sure all values in the column are floating points (floats) for coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpd0a7q_LwzM"
   },
   "outputs": [],
   "source": [
    "# Replace all the spaces with null values\n",
    "intermediate_data['TotalCharges'] = intermediate_data[\"TotalCharges\"].replace(\" \",np.nan)\n",
    "\n",
    "# Drop null values of 'Total Charges' feature\n",
    "intermediate_data = intermediate_data[intermediate_data[\"TotalCharges\"].notnull()]\n",
    "intermediate_data = intermediate_data.reset_index()[intermediate_data.columns]\n",
    "\n",
    "# Convert 'Total Charges' column values to float data type\n",
    "intermediate_data[\"TotalCharges\"] = intermediate_data[\"TotalCharges\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekuOxO7znmYl"
   },
   "source": [
    "We have just dropped (deleted) the lines in the dataset in which there was a missing value in the `TotalCharges` column. There 11 of these instances, which means that if everything worked out, we now have 11 less lines in the dataset. We had 7043 to start with, and we now should have 7043 - 11 = 7032. Let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HR_woJTQoHxU",
    "outputId": "3e069e98-11f0-472f-d31e-054614f30381"
   },
   "outputs": [],
   "source": [
    "intermediate_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wksB7lwqq4Jm"
   },
   "source": [
    "Nearly there! Third, we will use a method to transform categorical values into 0s and 1s. Unfortunately, regression models are, in general, not designed to be trained directly on categorical data and require the attributes to be trained on to be numeric. One simple way to meet this requirement is by applying a technique referred to as **\"one-hot\" encoding**. Using this encoding technique, we will derive a numerical representation of each of the categorical attribute values. One-hot encoding creates new binary columns for each categorical attribute value present in the original data.\n",
    "\n",
    "Let's have a look at the example shown in the figure below. The categorical attribute “Student” below contains the names \"Tim\", \"Richard\" and \"David\". We \"one-hot\" encode the names by creating a separate binary column for each possible name-value observable in the \"Student\" column. Now, we encode for each transaction that contains the value \"Tim\" in the \"Student\" column this observation with 1.0 in the newly created \"Tim\" column and 0.0 in all other generated name columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtYJvzAdjfko"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"encoding.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPH6LLoJjlAX"
   },
   "source": [
    "This can be achieved using the get_dummies() function available in the Pandas data science library. More info on that method at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wgIzSeAOf4r"
   },
   "outputs": [],
   "source": [
    "# Perform One Hot Encoding using get_dummies method\n",
    "df = pd.get_dummies(intermediate_data, columns = cats, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeHa0E4ZrXOn"
   },
   "source": [
    "Note that we renamed our dataframe as 'df' to follow convention :)\n",
    "\n",
    "Let's now look at the top of our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "WPt3aYTHO1Ri",
    "outputId": "0d3e6def-1364-4557-df60-cc009fff5f13"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RdN-uhisCGY"
   },
   "source": [
    "We can observe that the `get_dummies` method renamed our columns. For instance, `gender` became `gender_Male`. That means that where the customer's value for that column was 'Male', it is now 1 - and where it was 'Female', it is now 0. The transformation also added 5 columns - this is to deal with the fact that some columns contained more than 2 categorical variables. So our dataset should now have 7032 lines and 21 + 5 = 26 columns (features). Let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "5w9z7EHpuoau",
    "outputId": "651e8183-0a10-4643-c9ea-4883ef6ffd81"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-kWqOMIZTBL"
   },
   "source": [
    "#### 3.2.2 Feature Re-Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqcWignYswpw"
   },
   "source": [
    "As can be seen in the data, all values in the columns are contained in $[0,1]$, except three columns. This is very dangerous because it will give huge and undeserved weights to the values that are not contained in that range when we feed the data to our classifier. We thus need to rescale these values so that they fall into the $[0,1]$ range.\n",
    "\n",
    "One widley used method of feature re-scaling is referred to as **Min-Max Normalization** and is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLDN7Aklt-F9"
   },
   "source": [
    "$$x'={\\frac  {x-{\\text{min}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nRLgP3IuCPe"
   },
   "source": [
    "Let's re-scale the distinct feature values of the out-of-range columns of our dataframe using **Min-Max Normalization**. We will do so by using the `MinMaxScaler` class of the `sklearn` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDLLEm7Q9_Fh"
   },
   "outputs": [],
   "source": [
    "# Designate the columns that need normalizing\n",
    "need_scaling = ['tenure','MonthlyCharges','TotalCharges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7B5LJYXVYmR"
   },
   "source": [
    "It is worth noting here that we do not explicitly have to designate which features need scaling. If the scaler were applied to all features, those with numbers already between 0 and 1 would not be changed. We simply choose to designate the features that need scaling to make sure we get the intuition behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQdCejyk9xwv"
   },
   "outputs": [],
   "source": [
    "# Choose the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply the feature scaling operation on dataset using fit_transform() method\n",
    "df[need_scaling] = scaler.fit_transform(df[need_scaling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "J8gQeO6R_BcL",
    "outputId": "bc0c8ebb-4def-4ab8-c6ce-d5b7f691a154"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWtjTXGpZXO8"
   },
   "source": [
    "#### 3.2.3 Extraction of Training- and Evaluation-Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmDLTC8Su5Ue"
   },
   "source": [
    "We can now create our target variable *y*, which tells us whether the customer has left the company (this is what our model will attempt to predict on the test data), and our feature variable *X* which are all features that need taken into account to predict *y*. We can therefore drop `customerID` and `Churn_Yes` from our dataframe to determine *X*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-YQtw2XYteW"
   },
   "outputs": [],
   "source": [
    "#Create Feature variable X and Target variable y\n",
    "y = df['Churn_Yes']\n",
    "X = df.drop(['Churn_Yes','customerID'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1-vDLYBWCjn"
   },
   "source": [
    "Notice that we also dropped `customerID` from our input data $X$. This is because the ID is specific to each customer and has no relationship whatsoever with whether they will churn or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms31lkXrwB6B"
   },
   "source": [
    "The final step of our pre-processing is to make our train/test split. To understand and evaluate the performance of any trained **supervised machine learning** model, it is good practice to divide the dataset into a **training set** (the fraction of data records solely used for training purposes) and a **evaluation set** (the fraction of data records solely used for evaluation purposes). Please note, the **evaluation set** (also called test set) will never be shown to the model as part of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agXhqF6_wt2i"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"train_eval_dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjVn69ngw19h"
   },
   "source": [
    "We set the fraction of testing records to 30% of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMFxqIP414rE"
   },
   "outputs": [],
   "source": [
    "eval_fraction = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2171baHw6VQ"
   },
   "source": [
    "Randomly split the dataset into training set and evaluation set using sklearn's train_test_split function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wa4nkyarY5O_"
   },
   "outputs": [],
   "source": [
    "#Split the data into training set (70%) and test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = eval_fraction, random_state = random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PueuBTpxBvb"
   },
   "source": [
    "Evaluate the training set dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "vm-p-XOWxCgK",
    "outputId": "94de3514-1e72-4557-819e-15cc22463014"
   },
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDLfDRGRxKfw"
   },
   "source": [
    "Evaluate the evaluation set dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ezq2wD-MxLBQ",
    "outputId": "8ad87072-acec-41f0-cded-9f71ad447b3a"
   },
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Mrrwo-5ZeMR"
   },
   "source": [
    "### 3.3 Logistic Regression Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbQBui4eaBkS"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 700px; height: auto\" src=\"architecture_pipeline.png\">\n",
    "\n",
    "Basic Logistic Regression architecture - from a dataset with features and a target variable, compute a decision boundary based on training data, which will be used to predict the target value of yet unseen (test) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIvn2yGaffp9"
   },
   "source": [
    "#### 3.3.1 Theoretical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p-z6XtmQVgQ"
   },
   "source": [
    "Logistic Regression builds on the concepts of Linear Regression, where the model produces a linear equation relating the input features $X$ - the **independent** variables - to the target - **dependent** - variable $Y$. In our case, $X$ are all the features in the dataset except `Churn_Yes` and `customerID`, while $Y$, the target variable, is `Churn_Yes`. Remember, we aim at building a model that understands the relationships between these variables to be able to predict which of our future customers will leave.\n",
    "\n",
    "In Logistic Regression, the target variable is a discrete value (0 or 1), unlike a continuous value as in the case of Linear Regression. To get that discrete variable (e.g. 0 or 1), one must use an **activation function** (see below). The equation built by the model focuses of separating the various discrete values of target — trying to identify a line such that all 1’s fall on one side of the line and all 0’s on the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3B37rfISrJQ"
   },
   "source": [
    "The main **equation** behind Logistic Regression is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMaOdmQpTJ8n"
   },
   "source": [
    "$$s = w_{1}x_{1} + w_{2}x_{2} + ... + w_{d}x_{d} + b$$\n",
    "\n",
    "$$ŷ = H(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykrQi3LSb1fW"
   },
   "source": [
    "Where \n",
    "- $s$ is a score. It is a value that can range from $- \\infty$ to $\\infty$ and is the result of classic regression.\n",
    "- $ŷ$ is a continuous value between $[0,1]$. It is the output and can be regarded as a probability.\n",
    "- $w_{x}$ are weights given to the different feature\n",
    "- $b$ is a bias value\n",
    "- $H$ is an activation function\n",
    "- $d$ is the number of features (columns) in our input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwyvKgMkW8ZD"
   },
   "source": [
    "This means that for each instance of the data (each customer), we compute $s$, a continuous and unbounded value, which corresponds to the prediction we would compute in Linear Regression. To calculate $s$, we assign weights to the features (gender, payment method etc..) and a bias. This is as simple as a linear equation $y = wx + b$, where we have a slope and an intercept. The only difference is that we call them weights (given to each feature) and the bias.\n",
    "\n",
    "It is quite intuitive that we must give weights to the different features because as we saw, some features have less impact on a customer churning than others. For instance, it is a reasonable assumption that the `gender` feature is given less weight than the `Contract` feature (as seen in *3.1.1 Data Visualization*).\n",
    "\n",
    "To get $ŷ$, our discrete variable which lets us know if the customer has churned, we must feed $s$ to an activation function $H()$. The purpose of the activation function is to convert the score $s$ into a probability. To do so, the score $s$ is transformed into a vlue between $[0,1]$, which can be considered a probability. Then, the actual decision is made according to a threshold, generally *0.5*. For instance, if $ŷ = 0.7$, we will see that *$ŷ$ > threshold* and we will consider it equal to *1* as a result (meaning the customer has left, in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDeTe2nkRGb4"
   },
   "source": [
    "We can distinguish several **steps** in the Logistic Regression algorithm. Here's an overview:\n",
    "\n",
    "1. Assign random values to the weights $w$ and the bias $b$.\n",
    "2. Calculate $s$ and $ŷ$ given these weights and the bias.\n",
    "3. Calculate the loss (how far off are these results from the actual values).\n",
    "4. Compute the gradients for the weights and the bias to know how to adjust the values that we initially set at random.\n",
    "5. Update the weights and the bias until we get satisfactory results, while avoiding overfitting.\n",
    "\n",
    "Steps 3 to 5 are done repeatedly until the algorithm gets the desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A8g3kIgs8am"
   },
   "source": [
    "#### 3.3.2 Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_xccCFZeEAB"
   },
   "source": [
    "The **activation function** allows us to obtain continuous values bounded between $[0,1]$ from the original score $s$. It is then what makes it possible to use regression for classification! **Linear Regression** does not work for classification because it outputs continuous values that are not bounded between $[0,1]$ - that is why you can predict e.g. stock prices (which are continuous) but not classes (which are discrete, e.g. binary) with Linear Regression.\n",
    "\n",
    "The activation function, which is simply a **sigmoidal** (*S*-shaped) function, translates $s$ into a value in $[0,1]$ that we call ŷ, which in turn allows the those values to be compared with a threshold (0.5). If the value of $ŷ$ is higher than the threshold, we will give it the discrete value *1*, and if it is below the threshold, we will assign it a value of *0*. Here is the equation for the *logistic function* (one function we can use as our sigmoidal activation function):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPN9kwlgmMKY"
   },
   "source": [
    "$$H(s) = {\\frac  {1}{1+e^{-s}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1F14Lt8UoKa8"
   },
   "source": [
    "Below is a graph of an example of a sigmoid function - the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3TkrmHKtiPk"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"sigmoid.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8n9fB3IkisD"
   },
   "source": [
    "As can be seen, the sigmoid function intersects with the y-axis at $y = 0.5$, i.e. the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9_2452Kk3AP"
   },
   "source": [
    "A very useful property of the activation function is that it is **differentiable**. We will use this property when using **gradient descent** when we want to update our initially random weights.\n",
    "\n",
    "Another great feature of the sigmoidal activation function is that not only it helps us decide which class to give to the input, it also tells us how good our prediction was. A prediction of 0.9 for a customer that is leaving (Churn = 1) is better than a prediction of 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlLUH8rmpxJ6"
   },
   "source": [
    "#### 3.3.3 Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUCJ9Hywp-t6"
   },
   "source": [
    "After getting our first result for $ŷ$, we can start adjusting our weights so that $ŷ$ gets closer to its real value $y$. To do that, we must first learn how badly we performed. This can be done with a loss function, where we compare our prediction with the actual value. We want a function that outputs a large loss when our assumptions provide a value close to 0 while the actual is 1, and vice-versa.\n",
    "\n",
    "One function that we can use for the purpose is called the Log Loss function (also known as the Binary Cross-Entropy). It is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdATZMqls3H_"
   },
   "source": [
    "$$L = -\\frac {1}{N}\\sum_{i=1}^\\infty y_{i}*log(ŷ_{i}) + (1-y_{i})*log(1-ŷ_{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkfn-nWzmG1d"
   },
   "source": [
    "Where $N$ is the number of instances in the data. If we are only applying the loss function to one instance of the data, we can simplify as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnZ1NXuukTwZ"
   },
   "source": [
    "$$L = -y*log(ŷ) - (1-y)*log(1-ŷ)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33jTw2zbcmjZ"
   },
   "source": [
    "Let's visualize this function to understand it a bit better. The goal of Log Loss is to punish predictions that were way off, so that we know we need to diminish the loss by updating our parameters. The function looks different for predictions where the real value, $y$, is $y = 1$ or $y = 0$. Let's start by $y = 1$. The function looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hUK48bVdUHg"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"logloss_y1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvyOPtw8deb_"
   },
   "source": [
    "Because $y = 1$, only the first term of our equation matters, as the second one is multiplied by 0. $L = -y*log(ŷ) - (1-y)*log(1-ŷ)$ so $L_{y=1} = -1*log(ŷ) - (1-1)*log(1-ŷ)$ so $L_{y=1} = -1*log(ŷ)$. That means that as the predicted value approaches 0, the loss increases exponentially (because we want our predicted value to approach 1). If the predicted value is close to 1, the loss is very small.\n",
    "\n",
    "By the same logic, the loss function for $y = 0$ is different:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPNz09Fre2y6"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"logloss_y0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1dwvBj8fAs8"
   },
   "source": [
    "Because $y = 0$, our loss function is reduced to $L_{y=0} = -1*log(1-ŷ)$, as the first term is multiplied by 0. Accordingly, the loss function gives greater loss for predictions approaching 1, and very little loss for predictions close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9LPrMm3eLz8"
   },
   "source": [
    "When training the model, we want to find parameters $w∗$, $b∗$ that minimize the total loss across all training examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoASheh_fPkv"
   },
   "source": [
    "$$w*, b* = argmin_{\\mathbf{w, b}} L(w,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U4XzhTffQII"
   },
   "source": [
    "#### 3.3.4 Example with steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OfmPlX0gIos"
   },
   "source": [
    "For our simple example, let us imagine a scenario where our inputs are how comfortable students are in *Maths* and in *Business Innovation*, and how that affects their success in a *Machine Learning* class.\n",
    "\n",
    "5 HSG students grade their level of comfort in maths and in business innovation on a scale from 1 to 10, and we compare that with whether they passed the *Machine Learning* exam or not (1 for a pass, 0 for a fail).\n",
    "\n",
    "$X_{1}$ represents the level of comfort in maths, $X_{2}$ the level of comfort in business innovation and $Y$ whether the student has passed the ML exam.\n",
    "\n",
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"student_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rWOFO_8lGpD"
   },
   "source": [
    "Note: we will conduct this example as if we had mini-batches of size 1 (one row at a time) for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tZTGqvue-t9"
   },
   "source": [
    "**Step 1 - Assign random values to the weights and the bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUbvYZo6qQ4W"
   },
   "source": [
    "We can randomly assign a weight $w_{1}$ to $X_{1}$ and a weight $w_{2}$ to $X_{2}$. Let's also give a value to the bias $b$.\n",
    "\n",
    "Random values: \n",
    "- $w_{1} = 0.5$\n",
    "- $w_{2} = -0.5$\n",
    "- $b = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HymSoVVMrjY2"
   },
   "source": [
    "**Step 2 - Calculate $s$ and $ŷ$ given these weights and the bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HW7thiqZruMw"
   },
   "source": [
    "Ok, let's take our first student to see what happens with the first row of our example data.\n",
    "\n",
    "$s = 0.5 * 8 + (-0.5)*6 + 0 = 1$\n",
    "\n",
    "$ŷ = H(s) = {\\frac  {1}{1+e^{-1}}} = 0.7311$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp0R4rHPncsJ"
   },
   "source": [
    "**Step 3 - Calculate the loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asMflqAunlZk"
   },
   "source": [
    "$loss = -1*log(0.7311) - (1-1)*log(1-0.7311) = 0.136$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4jXRF2yo5_x"
   },
   "source": [
    "**Step 4 - Calculate the gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFhbvZ88o9Ex"
   },
   "source": [
    "We must calculate the gradient of each of our parameters (weights and bias) with respect to the loss. These will indicate how much we must change our parameters to reduce the loss and thus get our predicted values closer to the actual ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvxhQz7wqSae"
   },
   "source": [
    "$\\frac {∂loss}{∂w_{1}}= (ŷ-y)*X_{1} = (0.7311-1)*8=-2.1515$\n",
    "\n",
    "$\\frac {∂loss}{∂w_{2}}= (ŷ-y)*X_{2} = (0.7311-1)*6=-1.6136$\n",
    "\n",
    "$\\frac {∂loss}{∂b}= (ŷ-y)*1 = (0.7311-1)*1=-0.2689$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifQVyg5cvToF"
   },
   "source": [
    "In our case, the gradients turn out to be negative, so we will have to **increase** our parameters' values. The gradient of $w_{1}$ is larger than that of $b$, which means that the adjustment needed for $w_{1}$ is more important. The gradients give us the direction and scope of the change needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f7EFXhWtAmh"
   },
   "source": [
    "**Step 5 - Update the weights and the bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeRwunzYtJBq"
   },
   "source": [
    "The model trains on one instance (student) at a time. Because that student's data is different from another's, we must limit the impact the loss on this individual instance has on our parameters (which are shared for all students). To that end,  we adopt a *learning rate η* to scale the gradients to e.g. a tenth of their value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHDZMPeCuS_D"
   },
   "source": [
    "$w_{1} = w_{1} - η*\\frac {∂loss}{∂w_{1}}=0.5-0.1*(-2.1515)=0.7151$\n",
    "\n",
    "$w_{2} = w_{2} - η*\\frac {∂loss}{∂w_{2}}=-0.5-0.1*(-1.6136)=-0.339$\n",
    "\n",
    "$b = b - η*\\frac {∂loss}{∂b}=0-0.1*(-0.2689)=0.0269$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KYYLkIKwNqt"
   },
   "source": [
    "Updating the weights and the bias is an iterative process which will be repeated for either a fixed number of times or until the loss converges. Some instances in the data may pull the weights and the bias in opposite directions. A decision boundary between the classes is therefore built by these forces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXINDpuLQhU0"
   },
   "source": [
    "**2nd iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke0lx0xtQku0"
   },
   "source": [
    "If we continue the same process with the next line in our small dataset, we will again update our parameters a little:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ncm-0IUdQwNz"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"student_example2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezN9SJ-qQtSR"
   },
   "source": [
    "$s = 0.7151*9 + (-0.339)*2 + 0.0269 = 5.7848$\n",
    "\n",
    "$ŷ = H(s) = {\\frac  {1}{1+e^{-5.7848}}} = 0.9969$\n",
    "\n",
    "$loss = -1*log(0.9969) - (1-1)*log(1-0.9969) = -0.00135$\n",
    "\n",
    "$\\frac {∂loss}{∂w_{1}}= (ŷ-y)*X_{1} = (0.9969-1)*9=-0.0279$\n",
    "$\\frac {∂loss}{∂w_{2}}= (ŷ-y)*X_{2} = (0.9969-1)*2=-0.0062$\n",
    "$\\frac {∂loss}{∂b}= (ŷ-y)*1 = (0.9969-1)*1=-0.0031$\n",
    "\n",
    "$w_{1} = w_{1} - η*\\frac {∂loss}{∂w_{1}}=0.7151-0.1*(-0.0279)=0.71789$\n",
    "$w_{2} = w_{2} - η*\\frac {∂loss}{∂w_{2}}=-0.339-0.1*(-0.0062)=-0.3384$\n",
    "$b = b - η*\\frac {∂loss}{∂b}=0.0269-0.1*(-0.0031)=0.0272$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_GwQaSdUtG9"
   },
   "source": [
    "Our loss was even less important here and we only had to adjust our parameters a little. Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY9rUhG4VFU_"
   },
   "source": [
    "Now let's **test** these parameters on the third row of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZIVmWNyVL3S"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"student_example3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPAmVEyeVNJo"
   },
   "source": [
    "$s = 0.71789*7 + (-0.3384)*3 + 0.0272 = 4.0372$\n",
    "\n",
    "$ŷ = H(s) = {\\frac  {1}{1+e^{-4.0372}}} = 0.9826$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7Zp9WJ9WBTm"
   },
   "source": [
    "That is really close to the actual value *1*!\n",
    "Now let's do the same for our 4th student, who failed the *ML* exam:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNSmlo3bXT-f"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"student_example4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_e-QxcyWPz0"
   },
   "source": [
    "$s = 0.71789*2 + (-0.3384)*8 + 0.0272 = -1.2442$\n",
    "\n",
    "$ŷ = H(s) = {\\frac  {1}{1+e^{1.2442}}} = 0.2237$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8fb6zNbWped"
   },
   "source": [
    "Again it accurately predicted the class, as 0.2237 is lower than the threshold 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9Ii3c9aP5wb"
   },
   "source": [
    "This small example is largely based on https://towardsdatascience.com/under-the-hood-logistic-regression-407c0276c0b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqmOMOpjs99L"
   },
   "source": [
    "#### 3.3.5 Classification with 2 feature input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzsBfLp1tNn-"
   },
   "source": [
    "We are now going to use only a subset of our data to make customer churn predictions. We will only use two columns of the data (two features). The goal is to be able to visualize the decision boundary that Logistic Regression creates to separate the two classes - namely churn or no churn. Plotting the decision boundary is with only two features is best for understanding what is happening through visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNHWzXa5RIRI"
   },
   "source": [
    "In order to be able to see the data points around the decision boundary, let's choose two columns that have continuous values instead of discrete ones. For instance, let's select `tenure` and `MonthlyCharges` as input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GTfT9A1uTb6"
   },
   "outputs": [],
   "source": [
    "X_small = df[['tenure', 'MonthlyCharges']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouHySnXQRkGY"
   },
   "source": [
    "This small dataframe should have 7'032 lines and only 2 columns. Let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3OiXD6cjRi4j",
    "outputId": "f497f444-7801-4254-ef6b-a8a383d4d897"
   },
   "outputs": [],
   "source": [
    "X_small.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z6d_PaPR1h3"
   },
   "source": [
    "We can now split our small example of data between a training set and an evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiiEtV4nv2P2"
   },
   "outputs": [],
   "source": [
    "#Split the data into training set (70%) and test set (30%)\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(X_small, y, test_size = eval_fraction, random_state = random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwtpgsfzSEPW"
   },
   "source": [
    "Let's quickly instantiate our Logistic Regression model and fit - or 'train' it on - the training data. More details on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "j8xLYvfowDgK",
    "outputId": "1db41686-2d9a-402d-9e21-9fb9bc41f93e"
   },
   "outputs": [],
   "source": [
    "# Choose our classifier, namely Logistic Regression\n",
    "clf_small = LogisticRegression(random_state=random_seed)\n",
    "\n",
    "# Fit the logistic Regression Model\n",
    "clf_small.fit(X_train_small,y_train_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAyxg0jDSpIX"
   },
   "source": [
    "Now that we have trained the model on our input data, we can create a grid on which we visualize our input data over the decision boundary. This is helpful when trying to understand how the model separates the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUFZTm8Px6T0"
   },
   "outputs": [],
   "source": [
    "# Create a grid of numbers that cover all potential number combinations between -0.2\n",
    "# and 1.2 (could have chosen 0 and 1 but it's nicer on the plot) at 0.1 intervals\n",
    "xx, yy = np.mgrid[-0.2:1.2:.01, -0.2:1.2:.01]\n",
    "\n",
    "# Making a grid out of those numbers, which are created as 2D arrays\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUlOXap0T-jy"
   },
   "source": [
    "We now calculate the probability that these combinations would lead to `Churn`, given the parameters our model has found from when we trained it. To this end, we use the `predict_proba()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AID6faL2UOU6"
   },
   "outputs": [],
   "source": [
    "probs = clf_small.predict_proba(grid)[:, 1].reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um_9tTGaUR1D"
   },
   "source": [
    "We now transform our input data (which are currently dataframes) into numpy arrays, which will enable us to scatter them on a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaGWHdlZFg_m"
   },
   "outputs": [],
   "source": [
    "X_train_array = X_train_small.to_numpy()\n",
    "y_train_array = y_train_small.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lNqObtzUkp1"
   },
   "source": [
    "Finally, we create a plot that shows the decision boundary. The clients are scattered around it according to their feature values. We only look at 200 clients, because the plot would get very messy if we tried to see all 7'032! You can also see the degree of confidence our classifier had with the nuances of colour. For this, look at the probability column on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "V-_13n-20NNP",
    "outputId": "cb641812-54d2-45b9-a29e-b6e08f349e0d"
   },
   "outputs": [],
   "source": [
    "# Define figure and figure size\n",
    "figure, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Define the contour frame - made with all values and the churn probabilities\n",
    "contour = ax.contourf(xx, yy, probs, 16, cmap=\"PiYG_r\",\n",
    "                      vmin=0, vmax=1)\n",
    "\n",
    "# Add the color bar on the right\n",
    "ax_c = figure.colorbar(contour)\n",
    "# Name the color bar's label\n",
    "ax_c.set_label(\"$P(y = 1)$\")\n",
    "# Define the color bar's ticks\n",
    "ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "# Scatter our training data on the graph\n",
    "ax.scatter(X_train_array[:200,0], X_train_array[:200, 1], c=y_train_array[:200], s=50,\n",
    "           cmap=\"PiYG_r\",\n",
    "           edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "# Determine and plot decision boundary\n",
    "Z = clf_small.decision_function(grid).reshape(xx.shape)\n",
    "ax.contour(xx, yy, Z, colors='k', levels=[0], linestyles=['-'])\n",
    "\n",
    "# Limits of the axes and labels\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(-0.2, 1.2), ylim=(-0.2, 1.2),\n",
    "       xlabel=\"$X_1$\", ylabel=\"$X_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AykS1AIRW_5H"
   },
   "source": [
    "As can be seen around the decision boundary, the classifier does okay to recognize clients who do not churn, but has more trouble recognizing those who do. The decision boudary is set at the 0.5 threshold (standard practice) - this is done automatically by the `sklearn` library we are using. If we were to change that threshold, the boundary would shift, e.g. to the right if the threshold were decreased. This would change the balance between precision and recall - the desirability of which is case-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4PW_qO_Pyrz"
   },
   "source": [
    "Out of curiosity, let's see how well the classifier performed, in terms of accuracy, on this reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCt061t6RC1Z"
   },
   "outputs": [],
   "source": [
    "#Predict the value for new, unseen data\n",
    "pred_small = clf_small.predict(X_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "E46AmbxyQ_Nn",
    "outputId": "90d91a20-9352-4eff-c24a-dedb79665735"
   },
   "outputs": [],
   "source": [
    "# Find Accuracy using accuracy_score method\n",
    "accuracy = round(metrics.accuracy_score(y_test_small, pred_small) * 100, 2)\n",
    "print('Small classifier accuracy on test data:', accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKfVeRTlfo7e"
   },
   "source": [
    "### 3.4 Classification on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzPN4S2M7aZQ"
   },
   "source": [
    "Luckily, the `Scikit-Learn` (https://scikit-learn.org) machine learning library provides a variety of machine learning algorithms that can be easily interfaced using the Python programming language. Among others the library also contains a variety of supervised classification algorithms such as the **Logistic Regression** classifier. The Logistic Regression classifier can be trained \"off-the-shelf\" to solve the mathematical task formulated above. Let's instantiate a Logistic Regression classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVCpMbo8ZqlC"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "clf = LogisticRegression(random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Oswmnce73ZR"
   },
   "source": [
    "Train or fit the Logistoc Regression classifier using the training dataset features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "1GFD6v9U7_rT",
    "outputId": "0e6085d4-940f-4132-ef15-3159a0bd4ed4"
   },
   "outputs": [],
   "source": [
    "# Fit the logistic Regression Model\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_WRJJ1G8wAJ"
   },
   "source": [
    "Out of interest, we can have a look at the weights $w$ the algorithm found for the different features, as well as the bias $b$ (intercept):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "aMp-Vwtw887b",
    "outputId": "1b1fc2be-ec2a-429a-9d65-709f8ab52cfc"
   },
   "outputs": [],
   "source": [
    "# Weights given to the different features\n",
    "print(clf.coef_)\n",
    "\n",
    "# Bias (intercept)\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8jAqD1bc8Nj"
   },
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7tIlu0QLEvP"
   },
   "source": [
    "### 4.1 Metrics and Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2Mxvd-a8NBz"
   },
   "source": [
    "After fitting the training data, the optimal decision boundary learned by the Logistic Regression model can then be used to predict the corresponding class labels (churn vs no churn) of so far unknown observations. We will utilize the trained model to predict the class labels of the remaining observations contained in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LgV2UQj8NpN"
   },
   "outputs": [],
   "source": [
    "# Predict the value for new, unseen data\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcQ2z2_ZkqOi"
   },
   "source": [
    "Let's have a look at the predicted values (where 1 = churn and 0 = no churn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "wODAQNdAkpK0",
    "outputId": "a5c370a1-db90-426e-ae11-c29a8add43b7"
   },
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCSQL_DBk3f9"
   },
   "source": [
    "As well as the true values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "q9tq7zgzk7pm",
    "outputId": "60a40269-c8ba-4efd-d431-9944121525c2"
   },
   "outputs": [],
   "source": [
    "# Convert y_test to a list\n",
    "true = y_test.values.tolist()\n",
    "\n",
    "print(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBSqcWXZlwod"
   },
   "source": [
    "Determine **prediction accuracy** of the trained model on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "UdVs6XDnlydk",
    "outputId": "245ef9e9-fca7-457f-95e5-f89ce4385dd4"
   },
   "outputs": [],
   "source": [
    "# Find Accuracy using accuracy_score method\n",
    "accuracy = round(metrics.accuracy_score(y_test, pred) * 100, 2)\n",
    "print('Classifier accuracy on test data:', accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pARe_nRRmOHL"
   },
   "source": [
    "Determine and plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "TwcnUWA_mnY8",
    "outputId": "356758f2-16ee-4df6-8d0f-d986bcc5e4c9"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "mat = confusion_matrix(y_test, pred)\n",
    "\n",
    "# Init the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Plot confusion matrix heatmap\n",
    "ax = sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='BuGn_r')\n",
    "\n",
    "# Choose where to display our ticks\n",
    "plt.tick_params(axis='both', which='major', labelsize=10, labelbottom = False, bottom=False, top = False, left = False, labeltop=True)\n",
    "\n",
    "# Add plot axis labels\n",
    "plt.xlabel('[true churns $y_{i}$]')\n",
    "plt.ylabel('[predictions $y_{i}\\'$]')\n",
    "\n",
    "# Add plot title\n",
    "plt.title('Logistic Regression Predictions - Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj9BWqmZe011"
   },
   "source": [
    "The confusion matrix shows that most of the success of our classifier comes from the correct no churn predictions. Let's further analyze this using the classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSlGa7e2fuWV"
   },
   "source": [
    "Remember that as part of the class you learned about several measures to evaluate the quality of a retrieval system, namely **Precision**, **Recall** and **F1-Score**. Let's briefly revisit their definition and subsequently calculate those measures based on the confusion matrix above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0xoyCU5fxru"
   },
   "source": [
    ">- The **Precision**, denoted by Precision $=\\frac{TP}{TP + FP}$, is the probability that a retrieved document is relevant.\n",
    ">- The **Recall**, denoted by Recall $=\\frac{TP}{TP + FN}$, is the probability that a relevant document is retrieved.\n",
    ">- The **F1-Score**, denoted by F1-Score $= 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$, combines precision and recall is the harmonic mean of both measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "_jN_e22ufNBO",
    "outputId": "2551581f-b01b-494e-9daf-5a5c32f09cfa"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQJjNfmTgb5A"
   },
   "source": [
    "**Prescision** corresponds to the proportion of positive predictions that are correct. As shown by the confusion matrix and highlighted by the classification report, precision is much higher for customers who stayed than customers who churned. **Recall**, which represents the proportion of positives that are correctly predicted, shows an even larger gap in our classifier's performance between the customers who actually churn and those who do not. One reason our model has higher success on the customers who stay may simply be the higher amount of customers who stay than customers who leave in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVEluBocLME2"
   },
   "source": [
    "### 4.2 Different threshold values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivuCxV3Gkjss"
   },
   "source": [
    "The Logistic Regression model outputs probabilities that the different customers have churned and classifies those who have a probability above the 0.5 (50%) threshold as churning. The `sklearn` library from which we borrowed the Logistic Regression classifier automatically uses 0.5 as threshold value (this is common practice). What happens if we change the threshold? Let's see how different thresholds change the prediction decisions. We shall choose threshold values of value of 20%, 50% and 80%, or in other terms: $t = [0.2, 0.5, 0.8]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "q4oBOplAeauI",
    "outputId": "5dfcabc4-1f84-4ee5-b172-f1ba5a50c504"
   },
   "outputs": [],
   "source": [
    "# Predicted probabilities\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "\n",
    "# Converting y_pred to a dataframe which is an array\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "\n",
    "# Converting to column dataframe\n",
    "y_pred_1 = y_pred_df.iloc[:,[1]]\n",
    "\n",
    "# Converting y_test to dataframe\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "\n",
    "# Removing index for both dataframes to append them side by side \n",
    "y_pred_1.reset_index(drop=True, inplace=True)\n",
    "y_test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Appending y_test_df and y_pred_1\n",
    "y_pred_final = pd.concat([y_test_df,y_pred_1],axis=1)\n",
    "\n",
    "# Renaming the column \n",
    "y_pred_final= y_pred_final.rename(columns={ 1 : 'Churn_Prob'})\n",
    "\n",
    "# Creating new column 'predicted' with 1 if Churn_Prob>0.2 else 0\n",
    "y_pred_final['Predictions for t = 0.2'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.2 else 0)\n",
    "\n",
    "# Creating new column 'predicted' with 1 if Churn_Prob>0.5 else 0\n",
    "y_pred_final['Predictions for t = 0.5'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Creating new column 'predicted' with 1 if Churn_Prob>0.8 else 0\n",
    "y_pred_final['Predictions for t = 0.8'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.8 else 0)\n",
    "\n",
    "# Let's see the head of y_pred_final\n",
    "y_pred_final.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKMTr5IcMJnq"
   },
   "source": [
    "The 80% threshold obviously predicts less churns, since it has higher criteria. Let's see how the different thresholds affect the model's accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "9REGKyNEJTbe",
    "outputId": "4abf4ed8-9f7d-4516-e3cc-9ceaae94746f"
   },
   "outputs": [],
   "source": [
    "# Create a list with the threshold values we used in the dataframe\n",
    "threshold_values = [0.2, 0.5, 0.8]\n",
    "\n",
    "# Loop through threshold_values list\n",
    "for i in threshold_values:\n",
    "  # Convert predictions for that threshold to a list\n",
    "  pred = y_pred_final['Predictions for t = {}'.format(i)].tolist()\n",
    "  # Find Accuracy using accuracy_score method\n",
    "  accuracy = round(metrics.accuracy_score(y_test, pred) * 100, 2)\n",
    "  print('Classifier accuracy on test data for threshold = {}: '.format(i), accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeHdOrswr43i"
   },
   "source": [
    "We can also examine the **Receiver Operating Characteristic (ROC) Curve** from our Logistic Regression classification. This curve compares the True-Positive rate **TPR** with the False-Positive rate **FPR**. Let's plot the ROC Curve for each threshold to see the effect they have on the decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "id": "IoMTBcKTrQZq",
    "outputId": "ec327d28-fc73-428d-ef44-64c70a2e57fe"
   },
   "outputs": [],
   "source": [
    "# Loop through threshold_values list\n",
    "for i in threshold_values:\n",
    "\n",
    "    # For each theshold column, get the fpr, tpr and threshold from sklearn metrics\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_pred_final.Churn_Yes, y_pred_final['Predictions for t = {}'.format(i)],\n",
    "                                              drop_intermediate = False)\n",
    "    \n",
    "    # Area Under the Curve score\n",
    "    auc_score = metrics.roc_auc_score(y_pred_final.Churn_Yes, y_pred_final['Predictions for t = {}'.format(i)])\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    \n",
    "    # Draw the curve and define the legend\n",
    "    plt.plot(fpr, tpr, label='ROC curve (AUC = {})'.format(auc_score.round(2)), c='green')\n",
    "    # Draw the diagonal\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    # Set the limits of the axes\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    # Label titles\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # Title\n",
    "    plt.title('ROC Curve, Threshold = {}'.format(i))\n",
    "    # Place legend at lower right of figure\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Throw an empty line between each figure for clarity\n",
    "    print()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efUmWaxSj2VL"
   },
   "source": [
    "A couple things might be worth noting here. First, the Area Under the Curve (AUC) is higher for $t = 0.2$ than for the other $t$ values, despite highest accuracy belonging to $t = 0.5$. This might seem counter-intuitive at first, but both values are not directly comparable and do not work in the same way. For instance, if the huge majority (say 90%) of the real labels are 0s (no churn), and our classifier naively classifies all samples as 0s, it will get 90% accuracy. However, we might get a very low AUC score (maybe close to 0.5.). The AUC score tells us how well the classifier is able to distinguish between the different classes. This example is similar to what happens when $t = 0.8$. When $t = 0.8$, accuracy is decent because predictions are mostly 0s, and the majority of samples are actually 0s. But the classifier is actually bad at distinguishing the two classes, so the AUC is low. The AUC is a very relevant measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVYanFxVlbuV"
   },
   "source": [
    "Second, what do the different shapes of the ROC curves imply? The point at which the green line changes direction is the point at which we get the distribution of True Positives **TP** and False Positives **FP**. We can see that for $t = 0.2$, we have a higher **TP** rate but also a higher **FP** rate than for $t = 0.5$. This is to be expected, as the lower theshold value simply classifies more predictions as positive.\n",
    "\n",
    "We might then want to think about that - should we aim to have a high **TP** rate, at the risk of having a high **FP** rate? Or should we instead aim at minimizing the **FP** rate? This will depend on the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiTu6L0Z4Wnm"
   },
   "source": [
    "## 5. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRbA-M03Lx5n"
   },
   "source": [
    "We recommend you to try the following exercises as part of the lab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqUP_D3UNx1j"
   },
   "source": [
    "**1. Try different evaluation fractions on the dataset, and have a look at the differences in accuracy results.**\n",
    "\n",
    "Write a Python loop in which you fit and predict the data with the Logistic Regression classifier for different values of the `eval_fraction`. For each iteration, print the accuracy result. You can for instance try to set the evaluation fraction to 0.1, 0.2, 0.3, 0.4, 0.5, 0.6 and 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLg_nb-dO6UM"
   },
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzLOYsbqQ5XW"
   },
   "source": [
    "**2. Try different scalers to normalize the data**\n",
    "\n",
    "We used the sklearn's `MinMaxScaler()` to normalize our data. However, sklearn offers several scalers (see https://scikit-learn.org/stable/modules/classes.html?highlight=preprocessing#module-sklearn.preprocessing). Use another scaler, such as the `StandardScaler` and visualize the dataframe. Notice anything different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfEYTCiWLzsT"
   },
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPDa1E4GldCc"
   },
   "source": [
    "**3. Think about whether either of recall and precision matters more for this prediction task**\n",
    "\n",
    "The task at hand might change whether the predictor cares more about precision or recall. For instance, when detecting cancer, recall is more important because you want to avoid False Negatives. For YouTube recommendations, the inverse might hold true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUv4pxTKp-5V"
   },
   "source": [
    "## 6. Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-biLT8BeqFOl"
   },
   "source": [
    "In this lab, a step by step introduction into **Logistic Regression** classification is presented. The code and exercises presented in this lab may serves as a starting point for more complex and tailored programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWXbX5-_qMBe"
   },
   "source": [
    "You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Pls. note that to convert the notebook, you need to install Python's **nbconvert** library and its extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FtQ66WLzqOuf",
    "outputId": "f42694d1-b422-4f21-fa9b-7497bee8d8ad"
   },
   "outputs": [],
   "source": [
    "# installing the nbconvert library\n",
    "!pip install nbconvert\n",
    "!pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG8W9RW6qTkd"
   },
   "source": [
    "Let's now convert the Jupyter notebook into a plain Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_KWvZ_hYqUc2",
    "outputId": "4c0f74fc-4e23-467c-dbeb-90a7ba2baf52"
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script aiml_lab_07.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sSojLr8LXHz"
   },
   "source": [
    "## 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6Gl2QEOLayX"
   },
   "source": [
    "- Main source (basis of the code):\n",
    "\n",
    ">- https://github.com/nitinkaushik01/Deep_and_Machine_Learning_Projects/blob/master/Churn_Prediction_of_Customers/Customer_Churn_Prediction.ipynb and corresponding video: https://www.youtube.com/watch?v=j5WJDimjgLg\n",
    "\n",
    "- Other sources:\n",
    "\n",
    ">- Theory: https://towardsdatascience.com/under-the-hood-logistic-regression-407c0276c0b4\n",
    "\n",
    ">- Pre-processing: https://github.com/sercandogan/churn-telco/blob/master/Churn%20Analysis.ipynb\n",
    "\n",
    ">- ROC Curve: https://www.nucleusbox.com/building-a-logistic-regression-model-in-python/\n",
    "\n",
    ">- Decision boundary: https://stackoverflow.com/questions/28256058/plotting-decision-boundary-of-logistic-regression\n",
    "\n",
    ">- Mathematical notation: Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2020). Dive into Deep Learning. Available at https://d2l.ai/index.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "aiml_LogReg",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "290px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
